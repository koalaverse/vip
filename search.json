[{"path":"https://koalaverse.github.io/vip/articles/vip.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Variable Importance Plots—An Introduction to the vip Package","text":"often machine learning (ML) models summarized using single metric (e.g., cross-validated accuracy) put production. Although often care predictions models, becoming routine (good practice) also better understand predictions! Understanding ML model makes predictions helps build trust model fundamental idea emerging field interpretable machine learning (IML).1 -depth discussion IML, see Molnar (2019b). paper, focus global methods quantifying importance2 features ML model; , methods help us understand global contribution feature model’s predictions. Computing variable importance (VI) communicating variable importance plots (VIPs) fundamental component IML main topic paper. many procedures discussed paper apply model makes predictions, noted methods heavily depend accuracy importance fitted model; hence, unimportant features may appear relatively important (albeit predictive) comparison included features. reason, stress usefulness understanding scale VI scores calculated take account assessing importance feature communicating results others. Also, point work focuses mostly post-hoc interpretability trained model given goal understand features driving model’s predictions. Consequently, work focuses functional understanding model contrast lower-level mechanistic understanding (Montavon, Samek, Müller 2018). , seek explain relationship model’s prediction behavior features without explaining full internal representation model.3 VI scores VIPs can constructed general ML models using number available packages. iml package (Molnar 2019a) provides FeatureImp() function computes feature importance general prediction models using permutation approach (discussed later). written R6 (Chang 2019) allows user specify generic loss function select one pre-defined list (e.g., mean squared error). also allows user specify whether importance measured difference ratio original model error model error permutation. user can also specify number repetitions used permuting feature help stabilize variability procedure. function can also run parallel using parallel backend supported foreach package (Revolution Analytics Weston, n.d.). ingredients package (Biecek, Baniecki, Izdebski 2019) also provides permutation-based VI scores feature_importance() function. (Note function recently replaced now deprecated DALEX function variable_importance() (Biecek 2019).) Similar iml::FeatureImp(), function allows user specify loss function importance scores computed (e.g., using difference ratio). also provides option sample training data shuffling data compute importance (default use n_sample = 1000), can help speed computation. mmpf package (Jones 2018) also provides permutation-based VI scores via mmpf::permutationImportance() function. Similar iml ingredients implementation, function flexible enough applied class ML models R. varImp package (Probst 2019) extends permutation-based method RFs package party (Hothorn et al. 2019) arbitrary measures measures package (Probst 2018). Additionally, functions varImp include option using conditional approach described Strobl et al. (2008) reliable presence correlated features. number RF-specific VI packages exist CRAN, including, limited , vita (Celik 2015), rfVarImpOOB (Loecher 2019), randomForestExplainer (Paluszynska, Biecek, Jiang 2019), tree.interpreter (Sun 2019).4. caret package (Kuhn 2020) includes general varImp() function computing model-specific filter-based VI scores. Filter-based approaches, described Kuhn Johnson (2013), make use fitted model measure VI. also take account predictors model. regression problems, popular filter-based approach measuring VI numeric predictor \\(x\\) first fit flexible nonparametric model \\(x\\) target \\(Y\\); example, locally-weighted polynomial regression (LOWESS) method developed Cleveland (1979). fit, pseudo-\\(R^2\\) measure can obtained resulting residuals used measure VI. categorical predictors, different method based standard statistical tests (e.g., \\(t\\)-tests ANOVAs) can employed; see Kuhn Johnson (2013) details. classification problems, area ROC curve (AUC) statistic can used quantify predictor importance. AUC statistic computed using predictor \\(x\\) input ROC curve. \\(x\\) can reasonably separate classes \\(Y\\), clear indicator \\(x\\) important predictor (terms class separation) captured corresponding AUC statistic. problems two classes, extensions ROC curve one-vs-approach can used. use mlr interface fitting ML models (Bischl et al. 2020), can use getFeatureImportance() function extract model-specific VI scores various tree-based models (e.g., RFs GBMs). Unlike caret, model needs fit via mlr interface; instance, use getFeatureImportance() ranger (Wright, Wager, Probst 2020) model unless fit using mlr. iml DALEX packages provide model-agnostic approaches computing VI, caret, extent, mlr, provide model-specific approaches (e.g., using absolute value \\(t\\)-statistic linear models) well less accurate filter-based approaches. Furthermore, package completely different interface (e.g., iml written R6). vip package (B. Greenwell, Boehmke, Gray 2019) strives provide consistent interface model-specific model-agnostic approaches feature importance simple use. three important functions exported vip described : vi() computes VI scores using model-specific model-agnostic approaches (results always returned tibble [Müller Wickham (2019)}); vip() constructs VIPs using model-specific model-agnostic approaches ggplot2-style graphics (Wickham et al. 2019); Note vi() actually wrapper around four workhorse functions, {vi_model(), vi_firm(), vi_permute(), vi_shap(), compute various types VI scores. first computes model-specific VI scores, latter three produce model-agnostic ones. workhorse function actually gets called controlled method argument vi(); default method = \"model\" corresponds model-specific VI (see ?vip::vi details links documentation).","code":""},{"path":"https://koalaverse.github.io/vip/articles/vip.html","id":"constructing-vips-in-r","dir":"Articles","previous_headings":"Introduction","what":"Constructing VIPs in R","title":"Variable Importance Plots—An Introduction to the vip Package","text":"’ll illustrate major concepts using Friedman 1 benchmark problem described Friedman (1991) Breiman (1996): \\[\\begin{equation}   Y_i = 10 \\sin\\left(\\pi X_{1i} X_{2i}\\right) + 20 \\left(X_{3i} - 0.5\\right) ^ 2 + 10 X_{4i} + 5 X_{5i} + \\epsilon_i, \\quad = 1, 2, \\dots, n, \\tag{1} \\end{equation}\\] \\(\\epsilon_i \\stackrel{iid}{\\sim} N\\left(0, \\sigma^2\\right)\\). Data model can generated using vip::gen_friedman(). default, features consist 10 independent variables uniformly distributed interval \\(\\left[0,1\\right]\\); however, 5 10 actually used true model. code chunk simulates 500 observations model Equation (1) \\(\\sigma = 1\\); see ?vip::gen_friedman details. Equation (1), clear features \\(X_1\\)–\\(X_5\\) important! (others don’t influence \\(Y\\) .) Also, based form model, ’d expect \\(X_4\\) important feature, probably followed \\(X_1\\) \\(X_2\\) (comparably important), \\(X_5\\) probably less important. influence \\(X_3\\) harder determine due quadratic nature, seems likely nonlinearity suppress variable’s influence observed range (.e., 0–1).","code":"trn <- vip::gen_friedman(500, sigma = 1, seed = 101)  # simulate training data tibble::as_tibble(trn)  # inspect output ## # A tibble: 500 × 11 ##        y     x1    x2    x3    x4     x5      x6    x7    x8    x9   x10 ##    <dbl>  <dbl> <dbl> <dbl> <dbl>  <dbl>   <dbl> <dbl> <dbl> <dbl> <dbl> ##  1 14.9  0.372  0.406 0.102 0.322 0.693  0.758   0.518 0.530 0.878 0.763 ##  2 15.3  0.0438 0.602 0.602 0.999 0.776  0.533   0.509 0.487 0.118 0.176 ##  3 15.1  0.710  0.362 0.254 0.548 0.0180 0.765   0.715 0.844 0.334 0.118 ##  4 10.7  0.658  0.291 0.542 0.327 0.230  0.301   0.177 0.346 0.474 0.283 ##  5 17.6  0.250  0.794 0.383 0.947 0.462  0.00487 0.270 0.114 0.489 0.311 ##  6 18.3  0.300  0.701 0.992 0.386 0.666  0.198   0.924 0.775 0.736 0.974 ##  7 14.6  0.585  0.365 0.283 0.488 0.845  0.466   0.715 0.202 0.905 0.640 ##  8 17.0  0.333  0.552 0.858 0.509 0.697  0.388   0.260 0.355 0.517 0.165 ##  9  8.54 0.622  0.118 0.490 0.390 0.468  0.360   0.572 0.891 0.682 0.717 ## 10 15.0  0.546  0.150 0.476 0.706 0.829  0.373   0.192 0.873 0.456 0.694 ## # ℹ 490 more rows"},{"path":"https://koalaverse.github.io/vip/articles/vip.html","id":"model-specific-vi","dir":"Articles","previous_headings":"","what":"Model-specific VI","title":"Variable Importance Plots—An Introduction to the vip Package","text":"machine learning algorithms way quantifying importance feature, refer model-specific VI. describe subsections follow. One particular issue model-specific VI scores necessarily comparable across different types models. example, directly comparing impurity-based VI scores tree-based models absolute value \\(t\\)-statistic linear models.","code":""},{"path":"https://koalaverse.github.io/vip/articles/vip.html","id":"decision-trees-and-tree-ensembles","dir":"Articles","previous_headings":"Model-specific VI","what":"Decision trees and tree ensembles","title":"Variable Importance Plots—An Introduction to the vip Package","text":"Decision trees probably offer natural model-specific approach quantifying importance feature. binary decision tree, node \\(t\\), single predictor used partition data two homogeneous groups. chosen predictor one maximizes measure improvement \\(^t\\). relative importance predictor \\(X\\) sum squared improvements internal nodes tree \\(X\\) chosen partitioning variable; see Breiman, Friedman, Charles J. Stone (1984) details. idea also extends ensembles decision trees, RFs GBMs. ensembles, improvement score predictor averaged across trees ensemble. Fortunately, due stabilizing effect averaging, improvement-based VI metric often reliable large ensembles; see Hastie, Tibshirani, Friedman (2009, 368). RFs offer additional method computing VI scores. idea use leftover --bag (OOB) data construct validation-set errors tree. , predictor randomly shuffled OOB data error computed . idea variable \\(X\\) important, validation error go \\(X\\) perturbed OOB data. difference two errors recorded OOB data averaged across trees forest. Note methods constructing VI scores can unreliable certain situations; example, predictor variables vary scale measurement number categories [Strobl et al. (2007), predictors highly correlated (Strobl et al. 2008). varImp package discussed earlier provides methods address concerns random forests package party, similar functionality also built partykit package (Hothorn Zeileis 2019). vip package also supports conditional importance described (Strobl et al. 2008) party- partykit-based RFs; see ?vip::vi_model details. Later , ’ll discuss general permutation method can applied supervised learning model. illustrate, fit CART-like regression tree, RF, GBM simulated training data. (Note: number different packages available fitting types models, just picked popular implementations illustration.) packages include ability compute VI scores features model; however, implementation rather package-specific, shown code chunk . results displayed Figure ?? (code reproduce plots omitted can made available upon request).  expect, three methods rank variables x1–x5 important others. good news, unfortunate remember different functions ways extracting plotting VI scores various model fitting functions. one place vip can help…one function rule ! vip loaded, can use vi() extract tibble VI scores.5 Notice vi() function always returns tibble6 two columns: Variable Importance (exceptions coefficient-based models also include Sign column giving sign corresponding coefficient, permutation importance involving multiple Monte Carlo simulations, later). Also, default, vi() always orders VI scores highest lowest; , among options, can controlled user (see ?vip::vi details). Plotting VI scores vip() just straightforward. example, following code can used reproduce Figure ??. Notice vip() function always returns \"ggplot\" object (default, bar plot). large models many features, Cleveland dot plot effective (fact, number useful plotting options can fiddled ). call vip() change useful options (resulting plot displayed Figure ??. Note can also call vip() directly \"vi\" object ’s already constructed.","code":"# Load required packages library(rpart)          # for fitting CART-like decision trees library(randomForest)   # for fitting RFs library(xgboost)        # for fitting GBMs  # Fit a single regression tree tree <- rpart(y ~ ., data = trn)  # Fit an RF set.seed(101)  # for reproducibility rfo <- randomForest(y ~ ., data = trn, importance = TRUE)  # Fit a GBM set.seed(102)  # for reproducibility bst <- xgboost(   data = data.matrix(subset(trn, select = -y)),   label = trn$y,   objective = \"reg:squarederror\",   nrounds = 100,   max_depth = 5,   eta = 0.3,   verbose = 0  # suppress printing ) # Extract VI scores from each model vi_tree <- tree$variable.importance vi_rfo <- rfo$variable.importance  # or use `randomForest::importance(rfo)` vi_bst <- xgb.importance(model = bst) # Load required packages library(vip)  # Compute model-specific VI scores vi(tree)  # CART-like decision tree ## # A tibble: 10 × 2 ##    Variable Importance ##    <chr>         <dbl> ##  1 x4            4234. ##  2 x2            2513. ##  3 x1            2461. ##  4 x5            1230. ##  5 x3             688. ##  6 x6             533. ##  7 x7             357. ##  8 x9             331. ##  9 x8             276. ## 10 x10            275. vi(rfo)   # RF ## # A tibble: 10 × 2 ##    Variable Importance ##    <chr>         <dbl> ##  1 x4           72.9   ##  2 x2           61.4   ##  3 x1           55.6   ##  4 x5           37.0   ##  5 x3           22.0   ##  6 x8            1.84  ##  7 x6            1.12  ##  8 x9            0.720 ##  9 x7           -1.39  ## 10 x10          -2.61 vi(bst)   # GBM ## # A tibble: 10 × 2 ##    Variable Importance ##    <chr>         <dbl> ##  1 x4          0.403   ##  2 x2          0.225   ##  3 x1          0.189   ##  4 x5          0.0894  ##  5 x3          0.0682  ##  6 x9          0.00802 ##  7 x6          0.00746 ##  8 x7          0.00400 ##  9 x10         0.00377 ## 10 x8          0.00262 library(patchwork)  # for easily arranging multiple ggplot2 plots  p1 <- vip(tree) + ggtitle(\"Single tree\") p2 <- vip(rfo) + ggtitle(\"Random forest\") p3 <- vip(bst) + ggtitle(\"Gradient boosting\")  # Display plots in a grid (Figure 1) p1 + p2 + p3 # Construct VIP (Figure 2) library(ggplot2)  # for theme_light() function vip(bst, num_features = 5, geom = \"point\", horizontal = FALSE,     aesthetics = list(color = \"red\", shape = 17, size = 5)) +   theme_light()"},{"path":"https://koalaverse.github.io/vip/articles/vip.html","id":"linear-models","dir":"Articles","previous_headings":"Model-specific VI","what":"Linear models","title":"Variable Importance Plots—An Introduction to the vip Package","text":"multiple linear regression, linear models (LMs), absolute value \\(t\\)-statistic (scaled variant estimated coefficients) commonly used measure VI.7. Motivation use assoicated \\(t\\)-statistic given Bring (1994). idea also extends generalized linear models (GLMs). code chunk , fit LM simulated Friedman data (trn) allowing main effects two-way interactions, use step() function perform backward elimination. resulting VIP displayed Figure ??.  major limitation approach VI score assigned term model, rather individual feature! can solve problem using one model-agnostic approaches discussed later. Multivariate adaptive regression splines (MARS), introduced Friedman (1991), automatic regression technique can seen generalization LMs GLMs. MARS algorithm, contribution (VI score) predictor determined using generalized cross-validation (GCV) statistic (though, statistics can also used; see details). example using earth package (Milborrow 2019) given (results plotted Figure ??):  access VI scores directly earth, can use earth::evimp() function.","code":"# Fit a LM linmod <- lm(y ~ .^2, data = trn) backward <- step(linmod, direction = \"backward\", trace = 0)  # Extract VI scores (vi_backward <- vi(backward)) ## # A tibble: 21 × 3 ##    Variable Importance Sign  ##    <chr>         <dbl> <chr> ##  1 x4            14.2  POS   ##  2 x2             7.31 POS   ##  3 x1             5.63 POS   ##  4 x5             5.21 POS   ##  5 x3:x5          2.46 POS   ##  6 x1:x10         2.41 NEG   ##  7 x2:x6          2.41 NEG   ##  8 x1:x5          2.37 NEG   ##  9 x10            2.21 POS   ## 10 x3:x4          2.01 NEG   ## # ℹ 11 more rows # Plot VI scores; by default, `vip()` displays the top ten features pal <- palette.colors(2, palette = \"Okabe-Ito\")  # colorblind friendly palette vip(vi_backward, num_features = length(coef(backward)),  # Figure 3     geom = \"point\", horizontal = FALSE, mapping = aes(color = Sign)) +   scale_color_manual(values = unname(pal)) +   theme_light() +   theme(axis.text.x = element_text(angle = 45, hjust = 1)) # Load required packages library(earth)  # Fit a MARS model mars <- earth(y ~ ., data = trn, degree = 2, pmethod = \"exhaustive\")  # Extract VI scores vi(mars, type = \"gcv\") ## # A tibble: 10 × 2 ##    Variable Importance ##    <chr>         <dbl> ##  1 x4            100   ##  2 x1             83.2 ##  3 x2             83.2 ##  4 x5             59.3 ##  5 x3             43.5 ##  6 x6              0   ##  7 x7              0   ##  8 x8              0   ##  9 x9              0   ## 10 x10             0 # Plot VI scores (Figure 4) vip(mars)"},{"path":"https://koalaverse.github.io/vip/articles/vip.html","id":"neural-networks","dir":"Articles","previous_headings":"Model-specific VI","what":"Neural networks","title":"Variable Importance Plots—An Introduction to the vip Package","text":"neural networks (NNs), two popular methods constructing VI scores Garson algorithm (Garson 1991), later modified Goh (1995), Olden algorithm (Olden, Joy, Death 2004). algorithms, basis VI scores network’s connection weights. Garson algorithm determines VI identifying weighted connections nodes interest. Olden’s algorithm, hand, uses products raw connection weights input output neuron sums products across hidden neurons. shown outperform Garson method various simulations. DNNs, similar method due Gedeon (1997) considers weights connecting input features first two hidden layers (simplicity speed); method can slow large networks. illustrate two methods using vip() nnet package (Ripley 2016) (see results Figure ??).","code":"# Load required packages library(nnet)  # Fit a neural network set.seed(0803)  # for reproducibility nn <- nnet(y ~ ., data = trn, size = 7, decay = 0.1,            linout = TRUE, trace = FALSE)  # Construct VIPs p1 <- vip(nn, type = \"garson\") p2 <- vip(nn, type = \"olden\")  # Display plots in a grid (Figure 5) p1 + p2"},{"path":"https://koalaverse.github.io/vip/articles/vip.html","id":"model-agnostic-vi","dir":"Articles","previous_headings":"","what":"Model-agnostic VI","title":"Variable Importance Plots—An Introduction to the vip Package","text":"Model-agnostic interpretability separates interpretation model. Compared model-specific approaches, model-agnostic VI methods flexible can applied supervised learning algorithm. section, discuss model-agnostic methods quantifying global feature importance using three different approaches: simple variance-based approach; permutation-based feature importance; Shapley-based feature importance.","code":""},{"path":"https://koalaverse.github.io/vip/articles/vip.html","id":"variance-based-methods","dir":"Articles","previous_headings":"Model-agnostic VI","what":"Variance-based methods","title":"Variable Importance Plots—An Introduction to the vip Package","text":"first model-agnostic method based simple feature importance ranking measure (FIRM); details, see B. M. Greenwell, Boehmke, McCarthy (2018), Zien et al. (2009), Scholbeck et al. (2019). specific approach used based quantifying “flatness” effects feature.8 Feature effects can assessed using partial dependence plots (PDPs) individual conditional expectation (ICE) curves (Goldstein et al. 2015). PDPs ICE curves help visualize effect low cardinality subsets feature space estimated prediction surface (e.g., main effects two/three-way interaction effects.). also model-agnostic can constructed way supervised learning algorithm. , fit projection pursuit regression (PPR) model (see ?stats::ppr details references) construct PDPs feature using pdp package B. M. Greenwell (2017). results displayed Figure ??. Notice PDPs uninformative features relatively flat compared PDPs features x1–x5!  Next, compute PDP-based VI scores fitted PPR NN models. PDP method constructs VI scores quantify relative “flatness” PDP (default, defined computing standard deviation \\(y\\)-axis values PDP). use PDP method, specify method = \"firm\" call vi() vip() (just use vi_firm() directly):  Figure ?? display PDP-based feature importance previously obtained PPR NN models. VI scores essentially capture variability partial dependence values main effect. ICE curve method similar PDP method, except measure “flatness” individual ICE curve aggregate results (e.g., averaging). (substantial) interaction effects, using ICE curves produce results similar using PDPs (just averaged ICE curves). However, strong interaction effects present, can obfuscate main effects render PDP-based approach less useful (since PDPs important features can relatively flat certain interactions present; see Goldstein et al. (2015) details). fact, probably safest always use ICE curves employing FIRM method. , display ICE curves feature fitted PPR model using \\(y\\)-axis scale; see Figure ??. , clear difference ICE curves features x1–x5 x6–x10; later relatively flat comparison. Also, notice ICE curves within feature relatively parallel (ICE curves within feature perfectly parallel, standard deviation curve results identical PDP method). example, interaction term x1 x2 obfuscate PDPs main effects results much different.  Obtaining ICE-based feature importance scores also straightforward, just specify ice = TRUE using FIRM approach. illustrated code chunk results, displayed Figure ??, similar obtained using PDP method.  using method = \"firm\", feature effect values stored attribute called \"effects\". convenience feature effect plots (e.g., PDPs ICE curves) can easily reconstructed compared VI scores, demonstrated example (see Figure ??):","code":"# Fit a PPR model (nterms was chosen using the caret package with 5 repeats of # 5-fold cross-validation) pp <- ppr(y ~ ., data = trn, nterms = 11)  # Construct VIPs p1 <- vip(pp, method = \"firm\", train = trn) + ggtitle(\"PPR\") p2 <- vip(nn, method = \"firm\", train = trn) + ggtitle(\"NN\")  # Display plots in a grid (Figure 7) p1 + p2 # Construct VIPs p1 <- vip(pp, method = \"firm\", ice = TRUE, train = trn) + ggtitle(\"PPR\") p2 <- vip(nn, method = \"firm\", ice = TRUE, train = trn) + ggtitle(\"NN\")  # Display plots in a grid (Figure 9) p1 + p2 # Construct PDP-based VI scores (vis <- vi(pp, method = \"firm\", train = trn)) ## # A tibble: 10 × 2 ##    Variable Importance ##    <chr>         <dbl> ##  1 x4           2.96   ##  2 x2           2.21   ##  3 x1           2.14   ##  4 x5           1.53   ##  5 x3           1.46   ##  6 x6           0.128  ##  7 x9           0.114  ##  8 x8           0.0621 ##  9 x10          0.0374 ## 10 x7           0.0170 # Reconstruct PDPs for all 10 features (Figure 10) par(mfrow = c(2, 5)) for (name in paste0(\"x\", 1:10)) {   plot(attr(vis, which = \"effects\")[[name]], type = \"l\", ylim = c(9, 19), las = 1) }"},{"path":"https://koalaverse.github.io/vip/articles/vip.html","id":"permutation-method","dir":"Articles","previous_headings":"Model-agnostic VI","what":"Permutation method","title":"Variable Importance Plots—An Introduction to the vip Package","text":"permutation method exists various forms made popular Breiman (2001) RFs, generalized extended Fisher, Rudin, Dominici (2018). permutation approach used vip quite simple outlined Algorithm 1 . idea randomly permute values important feature training data, training performance degrade (since permuting values feature effectively destroys relationship feature target variable). course assumes model properly tuned (e.g., using cross-validation) fitting. permutation approach uses difference baseline performance measure (e.g., training \\(R^2\\), AUC, RMSE) performance measure obtained permuting values particular feature training data (Note: model refit training data randomly permuting values feature). also important note method may appropriate , example, highly correlated features (since permuting one feature time may lead unlikely data instances). Let \\(x_1, x_2, \\dots, x_j\\) features interest let \\(M_{orig}\\) baseline performance metric trained model; brevity, ’ll assume smaller better (e.g., classification error RMSE). permutation-based importance scores can computed follows: \\(= 1, 2, \\dots, j\\): Permute values feature \\(x_i\\) training data. Recompute performance metric permuted data \\(M_{perm}\\). Record difference baseline using \\(VI\\left(x_i\\right) = M_{perm} - M_{orig}\\). Return VI scores \\(VI\\left(x_1\\right), VI\\left(x_2\\right), \\dots, VI\\left(x_j\\right)\\). Algorithm 1: simple algorithm constructing permutation-based VI scores. Algorithm 1 can improved modified number ways. instance, process can repeated several times results averaged together. helps provide stable VI scores, also opportunity measure variability. Rather taking difference step (c), Molnar (2019b, sec. 5.5.4) argues using ratio \\(M_{perm} / M_{orig}\\) makes importance scores comparable across different problems. ’s also possible assign importance scores groups features (e.g., permuting one feature time); useful features can categorized mutually exclusive groups, instance, categorical features *one-hot-encoded. use permutation approach vip, specify method = \"permute\" call vi() vip() (can use vi_permute() directly). Note using method = \"permute\" requires specifying additional arguments (e.g., training data, target name vector target values, prediction function, etc.); see ?vi_permute details. use vi_permute() first define prediction wrapper tells function generate write predictions chosen metric. example given previously fitted PPR NN models. use \\(R^2\\) (metric = \"rsq\") evaluation metric. results, displayed Figure ??, agree obtained using PDP- ICE-based methods.  permutation approach introduces randomness procedure therefore run computationally feasible. upside performing multiple runs Algorithm 1 allows us compute standard errors (among metrics) estimated VI scores, illustrated example ; specify nsim = 30 request feature permuted 30 times results averaged together. (Additionally, nsim > 1, can set call vip() construct boxplots raw permutation-based VI scores. useful want visualize variability VI estimates; see Figure ?? example.)  available performance metrics regression classification can listed using list_metrics() function, example: permutation method vip supports vector performance functions available yardstick (Kuhn, Vaughan, Hvitfeldt 2023). can also use custom metric (.e., loss function). Suppose example want measure importance using mean absolute error (MAE): \\[\\begin{equation}   MAE = \\frac{1}{n}\\sum_{= 1}^n\\left|y_i - \\hat{f}\\left(\\boldsymbol{x}_i\\right)\\right|, \\end{equation}\\] \\(\\hat{f}\\left(\\boldsymbol{x}_i\\right)\\) predicted value \\(y_i\\). simple function implementing metric given (consistent yardstick functions, user-supplied metric functions require two arguments: truth estimate). use computing permutation-based VI scores just pass via metric argument (warned, however, metric used computing permutation importance metric used train tune model). Also, since custom metric, need specify whether smaller value indicates better performance setting smaller_is_better = TRUE. results, displayed Figure ??, similar Figure ??, albeit different scale.  Although permutation importance naturally computed training data, may also useful shuffling measure performance new data! discussed depth Molnar (2019b, sec. 5.2). users interested computing permutation importance using new data, just supply train argument call vi(), vip(), vi_permute(). instance, suppose wanted use fraction original training data carry computations. case, simply pass sampled data train argument follows:  using permutation method nsim > 1, default keep permutation scores attribute called \"raw_scores\"; can turn behavior setting keep = FALSE call vi_permute(), vi(), vip(). keep = TRUE nsim > 1, can request permutation scores plotted setting all_permutations = TRUE call vip(), demonstrated code chunk (see Figure ??). also let’s visually inspect variability permutation scores within feature.","code":"# Prediction wrapper pfun_ppr <- function(object, newdata) {  # needs to return a numeric vector   stats::predict(object, newdata = newdata) } pfun_nnet <- function(object, newdata) {  # needs to return a numeric vector   stats::predict(object, newdata = newdata)[, 1L, drop = TRUE] }  # Plot VI scores set.seed(2021)  # for reproducibility p1 <- vip(pp, method = \"permute\", train = trn, target = \"y\", metric = \"rsq\",           pred_wrapper = pfun_ppr) + ggtitle(\"PPR\") p2 <- vip(nn, method = \"permute\", train = trn, target = \"y\", metric = \"rsq\",           pred_wrapper = pfun_nnet) + ggtitle(\"NN\")  # Display plots in a grid (Figure 11) p1 + p2 # Use 10 Monte Carlo reps set.seed(403)  # for reproducibility vis <- vi(pp, method = \"permute\", train = trn, target = \"y\", metric = \"rsq\",           pred_wrapper = pfun_ppr, nsim = 30) vip(vis, geom = \"boxplot\")  # Figure 12 list_metrics() ##          metric                               description ## 1      accuracy                   Classification accuracy ## 2  bal_accuracy          Balanced classification accuracy ## 3        youden Youden;'s index (or Youden's J statistic) ## 4       roc_auc                      Area under ROC curve ## 5        pr_auc    Area under precision-recall (PR) curve ## 6       logloss                                  Log loss ## 7         brier                               Brier score ## 8           mae                       Mean absolute error ## 9          mape            Mean absolute percentage error ## 10         rmse                   Root mean squared error ## 11          rsq                   R-squared (correlation) ## 12     rsq_trad                   R-squared (traditional) ##                                task smaller_is_better yardstick_function ## 1  Binary/multiclass classification             FALSE       accuracy_vec ## 2  Binary/multiclass classification             FALSE   bal_accuracy_vec ## 3  Binary/multiclass classification             FALSE            j_index ## 4             Binary classification             FALSE        roc_auc_vec ## 5             Binary classification             FALSE         pr_auc_vec ## 6  Binary/multiclass classification              TRUE    mn_log_loss_vec ## 7  Binary/multiclass classification              TRUE    brier_class_vec ## 8                        Regression              TRUE            mae_vec ## 9                        Regression              TRUE           mape_vec ## 10                       Regression              TRUE           rmse_vec ## 11                       Regression             FALSE            rsq_vec ## 12                       Regression             FALSE       rsq_trad_vec mae <- function(truth, estimate) {   mean(abs(truth - estimate)) } # Construct VIP (Figure 13) set.seed(2321)  # for reproducibility p1 <- vip(nn, method = \"permute\", train = trn, target = \"y\", metric = mae,           smaller_is_better = TRUE, pred_wrapper = pfun_nnet) +   ggtitle(\"Custom loss function: MAE\") set.seed(2321)  # for reproducibility p2 <- vip(nn, method = \"permute\", train = trn, target = \"y\",           metric = yardstick::mae_vec, smaller_is_better = TRUE,           pred_wrapper = pfun_nnet) +   ggtitle(\"Using `yardstick`'s MAE function\") p1 + p2 # Construct VIP (Figure 14) set.seed(2327)  # for reproducibility vip(nn, method = \"permute\", pred_wrapper = pfun_nnet, target = \"y\",     metric = \"rmse\",     train = trn[sample(nrow(trn), size = 400), ]) +  # sample 400 observations   ggtitle(\"Using a random subset of training data\") # Construct VIP (Figure 15) set.seed(8264)  # for reproducibility vip(nn, method = \"permute\", pred_wrapper = pfun_nnet, train = trn,     target = \"y\", metric = \"mae\", nsim = 10, geom = \"point\",     all_permutations = TRUE, jitter = TRUE) +   ggtitle(\"Plotting all permutation scores\")"},{"path":"https://koalaverse.github.io/vip/articles/vip.html","id":"a-classification-example","dir":"Articles","previous_headings":"Model-agnostic VI > Permutation method","what":"A classification example","title":"Variable Importance Plots—An Introduction to the vip Package","text":"example, ’ll illustrate use permutation importance classification problem. start, ’ll use randomForest package (Liaw Wiener 2002) build (default) random forest predict survivability passengers ill-fated Titanic. source data (also available vip::titanic) contains 263 missing values (.e., NA’s) age column. titanic_mice version, ’ll use vignette, contains imputed values age column using multivariate imputation chained equations via mice package. Consequently, titanic_mice list containing 11 imputed versions original data; see ?vip::titanic_mice details. now, ’ll just use one 11 imputed versions: Next, ’ll build default random forest predict survivability: comparison, ’s plot OOB-based permutation importance scores available random forest (note setting include_type = TRUE results \\(x\\)-axis label including method importance computed):  categorical outcomes, random forests can provide predicted class labels (.e., classification) predicted class probabilities (.e., prediction), shown . performance metric choose permutation importance determine whether prediction wrapper return class label (factor) numeric vector class probabilities. ’ll start classification accuracy (metric used random forest’s build-OOB-based permutation VI scores). basic call vi() (, similarly, vi_permute()) look something like: Note standard deviation VI score also computed returned whenever nsim > 1. results comparable fitted random forest computed internally setting importance = TRUE nPerm = 30; difference random forest uses OOB data computing drop accuracy shuffling variable. Next, ’ll compute permutation VI scores using metric requires predicted probabilities. , ’ll use Brier score, measures accuracy individual probabilities (smaller better). However, instead using built-metric = \"brier\" option, ’ll pass corresponding yardstick function directly. Note modify prediction wrapper return predicted probabilities, single vector probabilities case binary outcome (case, care event survived = \"yes\"): Finally, illustrate use event_level argument, ’ll compute permutation-based VI scores using area ROC curve (AUROC metric = \"roc_auc\"). results negative? issue metrics like AUROC (similar area PR curve) treat one class outcomes “event” interest. case, using predicted probability event survived = \"yes\", default event level (yardstick therefore vip) always first class label alphabetical order (survived = \"\", case): Consequently, using metrics like AUROC, good idea set event_level parameter call vi() vi_permute(). fix previous issue, just set event level second class label using even_level = \"second\": Much better (just negative previous results, expected)! similar example using multiclass outcome, see discussion issue.","code":"head(t1 <- vip::titanic_mice[[1L]]) ##   survived pclass   age    sex sibsp parch ## 1      yes      1 29.00 female     0     0 ## 2      yes      1  0.92   male     1     2 ## 3       no      1  2.00 female     1     2 ## 4       no      1 30.00   male     1     2 ## 5       no      1 25.00 female     1     2 ## 6      yes      1 48.00   male     0     0 t1$pclass <- as.ordered(t1$pclass)  # makes more sense as an ordered factor library(randomForest)  set.seed(2053)  # for reproducibility (rfo <- randomForest(survived ~ ., data = t1, importance = TRUE, nPerm = 30)) ##  ## Call: ##  randomForest(formula = survived ~ ., data = t1, importance = TRUE,      nPerm = 30)  ##                Type of random forest: classification ##                      Number of trees: 500 ## No. of variables tried at each split: 2 ##  ##         OOB estimate of  error rate: 18.79% ## Confusion matrix: ##      no yes class.error ## no  727  82   0.1013597 ## yes 164 336   0.3280000 vip(rfo, include_type = TRUE) head(predict(rfo, newdata = t1, type = \"response\"))  # predicted class labels ##   1   2   3   4   5   6  ## yes yes yes  no yes  no  ## Levels: no yes head(predict(rfo, newdata = t1, type = \"prob\"))  # predicted class probabilities ##      no   yes ## 1 0.014 0.986 ## 2 0.114 0.886 ## 3 0.472 0.528 ## 4 0.716 0.284 ## 5 0.392 0.608 ## 6 0.894 0.106 pfun_class <- function(object, newdata) {  # prediction wrapper   predict(object, newdata = newdata, type = \"response\") }  # Compute mean decrease in accuracy set.seed(1359)  # for reproducibility vi(rfo,    method = \"permute\",    train = t1,    target = \"survived\",    metric = \"accuracy\",  # or pass in `yardstick::accuracy_vec` directly    # smaller_is_better = FALSE,  # no need to set for built-in metrics    pred_wrapper = pfun_class,    nsim = 30  # use 30 repetitions ) ## # A tibble: 5 × 3 ##   Variable Importance   StDev ##   <chr>         <dbl>   <dbl> ## 1 sex          0.226  0.0111  ## 2 pclass       0.0801 0.00488 ## 3 age          0.0738 0.00595 ## 4 sibsp        0.0346 0.00459 ## 5 parch        0.0166 0.00247 sort(rfo$importance[, \"MeanDecreaseAccuracy\"], decreasing = TRUE) ##        sex     pclass        age      parch      sibsp  ## 0.17102147 0.05877827 0.04408406 0.01895065 0.01583429 pfun_prob <- function(object, newdata) {  # prediction wrapper   predict(object, newdata = newdata, type = \"prob\")[, \"yes\"] }  # Compute mean increase in Brier score set.seed(1411)  # for reproducibility vi(rfo,    method = \"permute\",    train = t1,    target = \"survived\",    metric = yardstick::brier_class_vec,  # or pass in `\"brier\"` directly    smaller_is_better = FALSE,  # need to set when supplying a function    pred_wrapper = pfun_prob,    nsim = 30  # use 30 repetitions ) ## # A tibble: 5 × 3 ##   Variable Importance   StDev ##   <chr>         <dbl>   <dbl> ## 1 sex          0.209  0.00866 ## 2 pclass       0.0977 0.00479 ## 3 age          0.0947 0.00460 ## 4 parch        0.0542 0.00271 ## 5 sibsp        0.0414 0.00186 set.seed(1413)  # for reproducibility vi(rfo,    method = \"permute\",    train = t1,    target = \"survived\",    metric = \"roc_auc\",    pred_wrapper = pfun_prob,    nsim = 30  # use 30 repetitions ) ## # A tibble: 5 × 3 ##   Variable Importance   StDev ##   <chr>         <dbl>   <dbl> ## 1 parch       -0.0251 0.00351 ## 2 sibsp       -0.0283 0.00211 ## 3 age         -0.0850 0.00477 ## 4 pclass      -0.0920 0.00533 ## 5 sex         -0.229  0.0137 levels(titanic$survived) ## [1] \"no\"  \"yes\" set.seed(1413)  # for reproducibility vi(rfo,    method = \"permute\",    train = t1,    target = \"survived\",    metric = \"roc_auc\",    event_level = \"second\",  # use \"yes\" as class label/\"event\" of interest    pred_wrapper = pfun_prob,    nsim = 30  # use 30 repetitions ) ## # A tibble: 5 × 3 ##   Variable Importance   StDev ##   <chr>         <dbl>   <dbl> ## 1 sex          0.229  0.0137  ## 2 pclass       0.0920 0.00533 ## 3 age          0.0850 0.00477 ## 4 sibsp        0.0283 0.00211 ## 5 parch        0.0251 0.00351"},{"path":"https://koalaverse.github.io/vip/articles/vip.html","id":"benchmarks","dir":"Articles","previous_headings":"Model-agnostic VI > Permutation method","what":"Benchmarks","title":"Variable Importance Plots—An Introduction to the vip Package","text":"section, compare performance four implementations permutation-based VI scores: iml::FeatureImp() (version 0.11.1), ingredients::feature_importance() (version 2.3.0), mmpf::permutationImportance (version 0.0.5), vip::vi() (version 0.4.1). simulated 10,000 training observations Friedman 1 benchmark problem trained random forest using ranger package. implementation, computed permutation-based VI scores 100 times using microbenchmark package (Mersmann 2019). benchmark use parallel processing capability available iml vip implementations. results microbenchmark displayed Figure (fig:benchmark) summarized output . case, vip package (version 0.4.1) fastest, followed closely ingredients mmpf. noted, however, implementations vip iml can parallelized. best knowledge, case ingredients mmpf (although difficult write simple parallel wrapper either). code used generate benchmarks can found https://github.com/koalaverse/vip/blob/master/slowtests/slowtests-benchmarks.R.","code":""},{"path":"https://koalaverse.github.io/vip/articles/vip.html","id":"shapley-method","dir":"Articles","previous_headings":"Model-agnostic VI","what":"Shapley method","title":"Variable Importance Plots—An Introduction to the vip Package","text":"Although vip focuses global VI methods, becoming increasing popular asses global importance aggregating local VI measures; particular, Shapley explanations (Štrumbelj Kononenko 2014). Using Shapley values (method coalitional game theory), prediction single instance \\(x^\\star\\) can explained assuming feature value \\(x^\\star\\) “player” game payout equal corresponding prediction \\(\\hat{f}\\left(x^\\star\\right)\\). Shapley values tell us fairly distribute “payout” (.e., prediction) among features. Shapley values become popular due attractive fairness properties posses (Lundberg Lee 2017). popular implementation available Python shap package (Lundberg Lee 2017); although number implementations now available R; example, iml, iBreakDown (Biecek et al. 2019), fastshap (B. Greenwell 2019). Obtaining global VI score Shapley values requires aggregating Shapley values feature across entire training set (least reasonable sample thereof). particular, use mean absolute value individual Shapley values feature. Unfortunately, Shapley values can computationally expensive, therefore approach may feasible large training sets (say, >3000 observations). fastshap package provides relief exploiting computational tricks, including option perform computations parallel (see details). Also, fast exact algorithms can exploited certain classes models. Starting vip version 0.4.1 can now use method = \"shap\" call vi() (use vi_shap() directly) compute global Shapley-based VI scores using method described (provided fastshap package installed)—see ?vip::vi_shap details. illustrate, compute Shapley-based VI scores xgboost model [R-xgboost] using Friedman data earlier; results displayed Figure (fig:vi-shap).9 ({Note: specifying include_type = TRUE call vip() causes type VI computed displayed part axis label.)  Passing exact = TRUE fastshap::explain() via ... argument call vip() (vi() vi_shap()) works lightgbm, xgboost, additive (generalized) linear models fit using R’s internal stats package. cases, prediction wrapper must supplied via ... argument. illustrate, let’s use previous random forest fit Titanic data set. Note Shapley explanation support classification, ’ll use probability-based prediction wrapper defined :","code":"# Load required packages library(xgboost)  # Feature matrix X <- data.matrix(subset(trn, select = -y))  # matrix of feature values  # Fit an XGBoost model; hyperparameters were tuned using 5-fold CV set.seed(859)  # for reproducibility bst <- xgboost(X, label = trn$y, nrounds = 338, max_depth = 3, eta = 0.1,                verbose = 0)  # Construct VIP (Figure 17) vip(bst, method = \"shap\", train = X, exact = TRUE, include_type = TRUE,     geom = \"point\", horizontal = FALSE,     aesthetics = list(color = \"forestgreen\", shape = 17, size = 5)) +   theme_light() pfun_prob <- function(object, newdata) {  # prediction wrapper   # For Shapley explanations, this should ALWAYS return a numeric vector   predict(object, newdata = newdata, type = \"prob\")[, \"yes\"] }  # Compute Shapley-based VI scores set.seed(853)  # for reproducibility vi_shap(rfo, train = subset(t1, select = -survived), pred_wrapper = pfun_prob,         nsim = 30) ## # A tibble: 5 × 2 ##   Variable Importance ##   <chr>         <dbl> ## 1 pclass       0.104  ## 2 age          0.0649 ## 3 sex          0.272  ## 4 sibsp        0.0260 ## 5 parch        0.0291"},{"path":"https://koalaverse.github.io/vip/articles/vip.html","id":"drawbacks-of-existing-methods","dir":"Articles","previous_headings":"Model-agnostic VI","what":"Drawbacks of existing methods","title":"Variable Importance Plots—An Introduction to the vip Package","text":"discussed Hooker Mentch (2019), permute--predict methods—like PDPs, ICE curves, permutation importance—can produce results highly misleading.10 example, standard approach computing permutation-based VI scores involves independently permuting individual features. implicitly makes assumption observed features statistically independent. practice, however, features often independent can lead nonsensical VI scores. One way mitigate issue use conditional approach described Strobl et al. (2008); Hooker Mentch (2019) provides additional alternatives, permute--relearn importance. Unfortunately, best knowledge, approach yet available general purpose. similar modification can applied PDPs (Parr Wilson 2019)11 seems reasonable use FIRM approach strong dependencies among features present (though, given much thought consideration). already mentioned PDPs can misleading presence strong interaction effects. drawback, course, equally applies FIRM approach using PDPs computing VI scores. discussed earlier, can mitigated using ICE curves instead. Another alternative use accumulated local effect (ALE) plots (D. W. Apley Zhu 2016) (though haven’t really tested idea). Compared PDPs, ALE plots advantage faster compute less affected strong dependencies among features. downside, however, ALE plots complicated implement (hence, currently available using method = \"firm\"). ALE plots available ALEPlot (D. Apley 2018) iml packages. Hooker (2007) also argues feature importance (concern main effects) can misleading high dimensional settings, especially strong dependencies interaction effects among features, suggests approach based generalized functional ANOVA decomposition—though, knowledge, approach widely implemented open source.","code":""},{"path":"https://koalaverse.github.io/vip/articles/vip.html","id":"summary","dir":"Articles","previous_headings":"","what":"Summary","title":"Variable Importance Plots—An Introduction to the vip Package","text":"VIPs help visualize strength relationship feature predicted response, accounting features model. ’ve discussed two types VI: model-specific model-agnostic, well strengths weaknesses. paper, showed construct VIPs various types “black box” models R using vip package. also briefly discussed related approaches available number R packages. Suggestions avoid high execution times discussed demonstrated via examples. paper based vip version 0.4.1. terms future development, vip can expanded number ways. example, plan incorporate option compute group-based conditional permutation scores. Although discussed paper, vip also includes promising statistic (similar variance-based VI scores previously discussed) measuring relative strength interaction features. Although VIPs can help understand features driving model’s predictions, ML practitioners cognizant fact none methods discussed paper uniformly best across situations; require accurate model properly tuned, checked consistency human domain knowledge.","code":""},{"path":"https://koalaverse.github.io/vip/articles/vip.html","id":"acknowledgments","dir":"Articles","previous_headings":"","what":"Acknowledgments","title":"Variable Importance Plots—An Introduction to the vip Package","text":"authors like thank anonymous reviewers Editor helpful comments suggestions. also like thank members 84.51\\(^{\\circ}\\) Interpretable Machine Learning Special Interest Group thoughtful discussions topics discussed herein.","code":""},{"path":[]},{"path":"https://koalaverse.github.io/vip/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Brandon M. Greenwell. Author, maintainer. Brad Boehmke. Author.","code":""},{"path":"https://koalaverse.github.io/vip/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Brandon M. Greenwell Bradley C. Boehmke (2020). Variable Importance Plots—Introduction vip Package. R Journal, 12(1), 343--366. URL https://doi.org/10.32614/RJ-2020-013.","code":"@Article{,   title = {Variable Importance Plots---An Introduction to the vip Package},   author = {Brandon M. Greenwell and Bradley C. Boehmke},   journal = {The R Journal},   year = {2020},   volume = {12},   number = {1},   pages = {343--366},   url = {https://doi.org/10.32614/RJ-2020-013}, }"},{"path":[]},{"path":"https://koalaverse.github.io/vip/index.html","id":"overview","dir":"","previous_headings":"","what":"Overview","title":"Variable Importance Plots","text":"vip R package constructing variable importance plots (VIPs). VIPs part larger framework referred interpretable machine learning (IML), includes (limited ): partial dependence plots (PDPs) individual conditional expectation (ICE) curves. PDPs ICE curves (available R package pdp) help visualize feature effects, VIPs help visualize feature impact (either locally globally). -progress, comprehensive, overview IML can found : https://github.com/christophM/interpretable-ml-book. Many supervised learning algorithms can naturally emit measure importance features used model, approaches embedded many different packages. downside, however, package uses different function interface can challenging (distracting) remember one (e.g., remembering use xgb.importance() xgboost models gbm.summary() gbm models). vip get one consistent interface computing variable importance many types supervised learning models across number packages. Additionally, vip offers number model-agnostic procedures computing feature importance (see next section) well experimental function quantifying strength potential interaction effects. details example usage, visit vip package website.","code":""},{"path":"https://koalaverse.github.io/vip/index.html","id":"features","dir":"","previous_headings":"","what":"Features","title":"Variable Importance Plots","text":"Model-based variable importance - Compute variable importance specific particular model (like random forest, gradient boosted decision trees, multivariate adaptive regression splines) wide range R packages (e.g., randomForest, ranger, xgboost, many ). Also supports caret parsnip (starting version 0.0.4) packages. Permutation-based variable importance - efficient implementation permutation feature importance algorithm discussed chapter Christoph Molnar’s Interpretable Machine Learning book. Shapley-based variable importance - efficient implementation feature importance based popular Shapley values via fastshap package. Variance-based variable importance - Compute variable importance using simple feature importance ranking measure (FIRM) approach. details, see see Greenwell et al. (2018) Scholbeck et al. (2019).","code":""},{"path":"https://koalaverse.github.io/vip/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"Variable Importance Plots","text":"","code":"# The easiest way to get vip is to install it from CRAN: install.packages(\"vip\")  # Alternatively, you can install the development version from GitHub: if (!requireNamespace(\"remotes\")) {   install.packages(\"remotes\") } remotes::install_github(\"koalaverse/vip\")"},{"path":"https://koalaverse.github.io/vip/reference/bin.html","id":null,"dir":"Reference","previous_headings":"","what":"Bin a numeric vector — bin","title":"Bin a numeric vector — bin","text":"Function bin numeric vector","code":""},{"path":"https://koalaverse.github.io/vip/reference/bin.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Bin a numeric vector — bin","text":"","code":"bin(x, n_bins)"},{"path":"https://koalaverse.github.io/vip/reference/bin.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Bin a numeric vector — bin","text":"x numeric vector. Integer specifying number bins split x .","code":""},{"path":"https://koalaverse.github.io/vip/reference/gen_friedman.html","id":null,"dir":"Reference","previous_headings":"","what":"Friedman benchmark data — gen_friedman","title":"Friedman benchmark data — gen_friedman","text":"Simulate data Friedman 1 benchmark problem. data originally described Friedman (1991) Breiman (1996). details, see sklearn.datasets.make_friedman1.","code":""},{"path":"https://koalaverse.github.io/vip/reference/gen_friedman.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Friedman benchmark data — gen_friedman","text":"","code":"gen_friedman(   n_samples = 100,   n_features = 10,   n_bins = NULL,   sigma = 0.1,   seed = NULL )"},{"path":"https://koalaverse.github.io/vip/reference/gen_friedman.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Friedman benchmark data — gen_friedman","text":"n_samples Integer specifying number samples (.e., rows) generate. Default 100. n_features Integer specifying number features generate. Default 10. n_bins Integer specifying number (roughly) equal sized bins split response . Default NULL binning. Setting positive integer > 1 effectively turns classification problem n_bins gives number classes. sigma Numeric specifying standard deviation noise. seed Integer specifying random seed. NULL (default) results different time function run.","code":""},{"path":"https://koalaverse.github.io/vip/reference/gen_friedman.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Friedman benchmark data — gen_friedman","text":"Breiman, Leo (1996) Bagging predictors. Machine Learning 24, pages 123-140. Friedman, Jerome H. (1991) Multivariate adaptive regression splines. Annals Statistics 19 (1), pages 1-67.","code":""},{"path":"https://koalaverse.github.io/vip/reference/gen_friedman.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Friedman benchmark data — gen_friedman","text":"","code":"gen_friedman() #>             y          x1          x2          x3          x4          x5 #> 1   16.976501 0.631675954 0.871071756 0.452977981 0.413325371 0.567986187 #> 2   16.500167 0.858684332 0.412470120 0.390449678 0.309109204 0.829878627 #> 3   15.601748 0.217330578 0.421157449 0.033231578 0.433165770 0.837966824 #> 4   17.533231 0.680877398 0.414154296 0.134868340 0.564376870 0.316282573 #> 5    9.293548 0.983244197 0.093391256 0.196743613 0.141799615 0.635765992 #> 6    8.301749 0.030674560 0.497356236 0.947646512 0.014908865 0.737200767 #> 7   16.935036 0.804780768 0.731981588 0.718363146 0.161310408 0.964792164 #> 8   22.427294 0.715037743 0.613705293 0.893818433 0.933948228 0.011436522 #> 9   18.144511 0.907574862 0.223045166 0.212348065 0.989781432 0.134624908 #> 10  21.059047 0.938068524 0.686288204 0.814325405 0.527960867 0.968026098 #> 11  15.889141 0.151185663 0.831157442 0.902057439 0.583458502 0.609258221 #> 12  16.839157 0.152444293 0.691116907 0.256232996 0.864358238 0.766976811 #> 13  13.314227 0.255576581 0.975898861 0.137206498 0.101652046 0.526893531 #> 14  16.237616 0.283447837 0.704952119 0.277733215 0.481040020 0.876323058 #> 15  17.663455 0.748991321 0.947099699 0.492040504 0.964333974 0.014011946 #> 16  13.764638 0.766947272 0.181066280 0.078335340 0.501717729 0.230233608 #> 17  17.146282 0.670559488 0.736486087 0.144985345 0.165321273 0.597705806 #> 18  10.346533 0.879458235 0.945342788 0.472540217 0.298373228 0.458441418 #> 19  11.136658 0.340776464 0.904966528 0.257049969 0.011713040 0.295522292 #> 20  11.455153 0.728911526 0.381506856 0.179182523 0.180261683 0.016562122 #> 21   7.747713 0.199192752 0.182997522 0.382926679 0.289993959 0.656604981 #> 22  20.128720 0.673630706 0.905124306 0.850867569 0.534144673 0.557108087 #> 23  18.208937 0.298157707 0.530918019 0.667573967 0.977064086 0.608102854 #> 24  17.187613 0.512791863 0.350287767 0.082814153 0.739822852 0.198038888 #> 25  15.624657 0.950181397 0.178702033 0.674235711 0.544856545 0.881971820 #> 26  17.589743 0.982253800 0.275918534 0.846229583 0.578956767 0.390852148 #> 27  20.378074 0.568654864 0.569132434 0.350533324 0.981898185 0.334659056 #> 28  10.643517 0.333497354 0.675749324 0.361356799 0.212695696 0.319730877 #> 29  15.485922 0.322848451 0.331371719 0.602118141 0.788985869 0.836390581 #> 30   8.277770 0.007553466 0.750428062 0.040020139 0.178497395 0.411190092 #> 31  20.113125 0.384506829 0.592706370 0.834609390 0.825227508 0.604128249 #> 32  13.808075 0.338630203 0.856547253 0.418496298 0.451200804 0.274619463 #> 33   9.538215 0.314667469 0.061906121 0.195764970 0.668047984 0.064784759 #> 34   7.250367 0.656836455 0.115496995 0.390102384 0.297654604 0.347705300 #> 35  24.310882 0.757793591 0.352848354 0.156595781 0.993813467 0.929786792 #> 36  13.682749 0.327556049 0.875483254 0.414102377 0.165837998 0.819050186 #> 37  19.780169 0.530115247 0.875364512 0.762798940 0.705115503 0.286503908 #> 38  13.119034 0.074512150 0.941768235 0.876485183 0.673669763 0.261336010 #> 39  18.287978 0.817752601 0.376784203 0.036657023 0.222991786 0.718754775 #> 40  16.625020 0.518982120 0.847237778 0.334363921 0.488637134 0.257681591 #> 41  13.262126 0.233352149 0.509520798 0.905732569 0.527135487 0.210310126 #> 42  10.918240 0.834256413 0.112036514 0.133976916 0.350815791 0.350432169 #> 43  14.987739 0.380075325 0.729417264 0.781516820 0.132279771 0.902333182 #> 44  15.504113 0.930803411 0.467032204 0.785141123 0.256642200 0.309700339 #> 45   8.853122 0.849447894 0.120294221 0.859646334 0.154152506 0.305555001 #> 46   8.742356 0.414229673 0.342575222 0.729831379 0.115233117 0.423189257 #> 47   8.929244 0.057264397 0.599118719 0.205685571 0.140003619 0.929742931 #> 48   8.154509 0.059754730 0.784022792 0.582958510 0.202737621 0.914917475 #> 49  11.454988 0.811985419 0.257417285 0.856735481 0.008718245 0.561127441 #> 50   7.721815 0.260581260 0.827362668 0.418924719 0.089823189 0.088866886 #> 51  15.143780 0.378367144 0.688762853 0.131348478 0.432240363 0.212315380 #> 52   8.595650 0.013552682 0.926721674 0.581191823 0.336882074 0.935887425 #> 53  17.246627 0.783056194 0.421886894 0.345190956 0.503531415 0.613621308 #> 54  13.979214 0.742456208 0.687150940 0.588961160 0.339627716 0.074835765 #> 55  15.473307 0.979803348 0.141029086 0.664016492 0.762551563 0.656373372 #> 56  16.926664 0.866342029 0.916405894 0.675540888 0.801727324 0.457001157 #> 57  10.165943 0.409667948 0.215428264 0.753078074 0.387576644 0.484638488 #> 58   9.037749 0.558474664 0.083192638 0.464889758 0.350203032 0.817837341 #> 59  19.175045 0.422208135 0.875313197 0.058783069 0.567895770 0.054295819 #> 60  15.997342 0.076287633 0.930295785 0.170241066 0.869748494 0.586870526 #> 61  13.654272 0.289538463 0.586552708 0.738607484 0.321028014 0.816269561 #> 62  15.820964 0.542793660 0.709899094 0.389952763 0.375379004 0.502505699 #> 63  19.220668 0.372325180 0.582539384 0.923390516 0.759172606 0.369191800 #> 64  14.837755 0.154248002 0.864648741 0.619746975 0.651784434 0.782610946 #> 65   8.479238 0.225768549 0.628813348 0.249175259 0.126718372 0.328422425 #> 66  21.571506 0.651690922 0.546482912 0.494515233 0.990661174 0.523642704 #> 67   6.998764 0.022850175 0.584626310 0.873836140 0.179837878 0.366222899 #> 68  12.559482 0.642870787 0.208829887 0.903041041 0.072073469 0.852402804 #> 69  18.251242 0.296406037 0.563380719 0.368478296 0.932403311 0.699552755 #> 70  17.530375 0.867287372 0.216257367 0.178094242 0.579822404 0.848219468 #> 71   9.668344 0.676768060 0.461091608 0.343458120 0.079248077 0.020703070 #> 72  15.475823 0.632379413 0.694762170 0.661137730 0.413864980 0.202702707 #> 73   7.629884 0.330350673 0.106541197 0.916503269 0.289945686 0.011651113 #> 74  12.827223 0.318142316 0.667146289 0.696450128 0.529771638 0.143643437 #> 75  15.957321 0.548335272 0.050607361 0.994953345 0.556043397 0.939561758 #> 76  21.122006 0.541488892 0.953741503 0.138677525 0.695784054 0.320812795 #> 77   8.797551 0.721150326 0.006840569 0.924137221 0.319116057 0.362254124 #> 78  14.150114 0.810855828 0.417220214 0.835206240 0.210277536 0.221620744 #> 79  23.862369 0.703562062 0.796775954 0.192128665 0.738352766 0.960866016 #> 80  10.739785 0.764227478 0.315939393 0.460157309 0.265635962 0.246862028 #> 81  15.575486 0.125109596 0.770906801 0.951810396 0.491825133 0.711263651 #> 82  21.742240 0.689516194 0.829238424 0.706124878 0.664201712 0.912496795 #> 83  20.385541 0.393511047 0.666325775 0.633540899 0.820764035 0.874064176 #> 84  12.175272 0.215407399 0.420132505 0.813795059 0.652919258 0.163809486 #> 85  16.142396 0.137166417 0.588639467 0.652264694 0.874402949 0.889067861 #> 86  12.396822 0.870763201 0.773689958 0.646244966 0.093679758 0.503140346 #> 87  15.543601 0.621258410 0.829953831 0.319102112 0.482637174 0.038597407 #> 88   6.800806 0.162131972 0.597335299 0.270391211 0.014733215 0.527000012 #> 89  13.728389 0.287192781 0.508753046 0.923526762 0.491288682 0.156567883 #> 90  16.630244 0.751478297 0.953530994 0.693353237 0.649227770 0.301724473 #> 91  15.070963 0.830836474 0.282007494 0.083005948 0.307249689 0.361677159 #> 92  15.603323 0.973174233 0.622810306 0.527416503 0.180558371 0.865405990 #> 93  10.809361 0.358959578 0.390367604 0.266123049 0.393414928 0.282301308 #> 94  18.259126 0.216115972 0.539020203 0.001644427 0.976964086 0.005082714 #> 95  15.724433 0.360427620 0.409314780 0.294683304 0.610500163 0.856790501 #> 96  16.354863 0.404407139 0.798976548 0.862926559 0.381593765 0.271634173 #> 97   6.896775 0.079218751 0.918491453 0.940549014 0.014105887 0.145797194 #> 98  12.131737 0.339745495 0.233693007 0.428051462 0.903655064 0.102115871 #> 99   7.563889 0.750417240 0.063670425 0.768474457 0.205951801 0.533231065 #> 100 13.202971 0.686906547 0.454122943 0.400372445 0.281985777 0.364768770 #>              x6          x7          x8         x9        x10 #> 1   0.161107766 0.207619793 0.321073955 0.93953274 0.11684485 #> 2   0.076696777 0.221408810 0.362764165 0.79414422 0.34944642 #> 3   0.578651103 0.606859197 0.176077551 0.79948664 0.01188524 #> 4   0.419518960 0.584720683 0.916456710 0.08746218 0.12155954 #> 5   0.613769640 0.918332098 0.894107062 0.56514734 0.75178880 #> 6   0.265238645 0.881183226 0.573668035 0.70270337 0.96560767 #> 7   0.440914714 0.556675691 0.457651958 0.87297605 0.39224583 #> 8   0.227210416 0.462672091 0.024690456 0.66794293 0.68724588 #> 9   0.558987414 0.721355593 0.715030433 0.70011243 0.09316872 #> 10  0.272494360 0.257448323 0.401939621 0.38127742 0.83136247 #> 11  0.327549207 0.324307371 0.455893323 0.31024099 0.19730209 #> 12  0.854709671 0.912844302 0.503065157 0.28701246 0.91480437 #> 13  0.319968575 0.245129285 0.085890463 0.38376872 0.57612077 #> 14  0.539391186 0.712612598 0.866045458 0.90616914 0.66910398 #> 15  0.545354954 0.420213789 0.838262249 0.14103002 0.21374972 #> 16  0.372133990 0.432053106 0.664676750 0.58666016 0.40908570 #> 17  0.572657129 0.586230555 0.669527009 0.01787209 0.69883866 #> 18  0.119764886 0.842323753 0.099777168 0.86167619 0.83038949 #> 19  0.488243777 0.816039367 0.609593143 0.11124351 0.73567816 #> 20  0.349650876 0.819434965 0.254921244 0.76276629 0.42189910 #> 21  0.795744601 0.018551452 0.730956452 0.60364026 0.23025507 #> 22  0.876236638 0.049971209 0.155718811 0.08012965 0.36423207 #> 23  0.166279150 0.735469294 0.738155313 0.50147699 0.35660879 #> 24  0.858372532 0.584397572 0.489948607 0.91804126 0.45992293 #> 25  0.847948894 0.325493190 0.637033261 0.42646382 0.43455210 #> 26  0.668455952 0.312400682 0.250640373 0.38358221 0.92680333 #> 27  0.700480191 0.944765502 0.508218075 0.59167270 0.15433466 #> 28  0.335669094 0.825937907 0.517497334 0.77160028 0.19964521 #> 29  0.038844270 0.348708753 0.682955612 0.02217775 0.94651502 #> 30  0.245959383 0.062952528 0.403085410 0.85178182 0.86897015 #> 31  0.761252699 0.353579039 0.393752639 0.90701903 0.26450129 #> 32  0.353009412 0.074011315 0.437797458 0.87312466 0.13938073 #> 33  0.001460670 0.092309304 0.531889361 0.46911221 0.29723975 #> 34  0.775693191 0.006452727 0.336770533 0.79542085 0.63525833 #> 35  0.378261218 0.059072361 0.441153008 0.20111579 0.25159051 #> 36  0.759531138 0.297545365 0.884616688 0.94581096 0.19642292 #> 37  0.685180195 0.131467561 0.824858066 0.09443136 0.31385977 #> 38  0.681939323 0.789758257 0.932677008 0.24645788 0.54341242 #> 39  0.062953055 0.927160287 0.259854890 0.95402948 0.55335344 #> 40  0.373360688 0.029847582 0.800718169 0.89330293 0.53256034 #> 41  0.996519804 0.511316691 0.942573381 0.41904545 0.50923538 #> 42  0.083126765 0.574793985 0.538756164 0.28729313 0.84850562 #> 43  0.735013397 0.848474107 0.365843052 0.36884323 0.37555758 #> 44  0.730195088 0.821220627 0.363562454 0.38786375 0.96209655 #> 45  0.020147586 0.155607012 0.433470801 0.18218565 0.80706843 #> 46  0.349610718 0.572511432 0.108151437 0.33624012 0.30081576 #> 47  0.152629047 0.818633312 0.437166714 0.75967797 0.33444318 #> 48  0.605226003 0.679017249 0.515897860 0.28439333 0.53642806 #> 49  0.539751902 0.164685660 0.171859787 0.89255698 0.31363215 #> 50  0.397091195 0.670569756 0.585377050 0.19779494 0.71154964 #> 51  0.614781228 0.248419169 0.352279752 0.13256540 0.47476122 #> 52  0.857612227 0.675538447 0.945126421 0.86094123 0.77470260 #> 53  0.471249504 0.235065642 0.188045381 0.84508093 0.32580347 #> 54  0.947907970 0.645181842 0.575135570 0.40028734 0.18098013 #> 55  0.679192248 0.669623616 0.512769716 0.90264532 0.32794292 #> 56  0.637725082 0.016759021 0.842446565 0.73950951 0.81023964 #> 57  0.600657429 0.796105374 0.582868275 0.37016346 0.33796791 #> 58  0.713750661 0.900793898 0.608004504 0.64909395 0.32961305 #> 59  0.713987009 0.772946005 0.898918698 0.07608635 0.30523902 #> 60  0.122523598 0.826322920 0.458950059 0.44117554 0.76829471 #> 61  0.398549309 0.309410169 0.065268878 0.35359261 0.36794106 #> 62  0.145626288 0.608956100 0.627678412 0.86841208 0.32949566 #> 63  0.649903436 0.687768716 0.675620860 0.91761154 0.94183182 #> 64  0.218477824 0.773477252 0.809933146 0.40923497 0.58506563 #> 65  0.719168668 0.105931549 0.766353937 0.73852222 0.06533017 #> 66  0.024382079 0.421726623 0.717581614 0.33234915 0.26227351 #> 67  0.052096129 0.082307472 0.992746690 0.73052220 0.02540879 #> 68  0.422759339 0.951485484 0.661980039 0.58804618 0.96098424 #> 69  0.154971749 0.950662453 0.805575543 0.60094614 0.98733969 #> 70  0.485582996 0.916782238 0.938234730 0.36655052 0.29930503 #> 71  0.562938660 0.598956770 0.326024584 0.70583623 0.56763074 #> 72  0.500579153 0.478949799 0.659606672 0.81160390 0.83253549 #> 73  0.699928174 0.255624072 0.706133065 0.23268532 0.60652998 #> 74  0.176226656 0.665307347 0.577219328 0.44985302 0.27784112 #> 75  0.901931266 0.868160985 0.546723743 0.32682807 0.91055433 #> 76  0.473941420 0.019240447 0.002589624 0.16352290 0.52874481 #> 77  0.312441811 0.457007555 0.898472774 0.57034617 0.24437921 #> 78  0.616185558 0.087150355 0.624914761 0.89071674 0.38374880 #> 79  0.009891189 0.401458647 0.231384394 0.90559741 0.47999151 #> 80  0.132968071 0.668871341 0.380589359 0.99620026 0.19071918 #> 81  0.309769478 0.465531803 0.863866784 0.12184399 0.07896498 #> 82  0.851714578 0.297438596 0.256893829 0.56172501 0.40135647 #> 83  0.954040427 0.864228091 0.172863220 0.45632057 0.59932214 #> 84  0.341569389 0.822225519 0.672400513 0.01668140 0.48029403 #> 85  0.335157020 0.068215986 0.850826806 0.70217682 0.68324932 #> 86  0.120605124 0.740097270 0.165290598 0.85731817 0.61093398 #> 87  0.869803404 0.274550612 0.165004834 0.21150869 0.62391872 #> 88  0.548132747 0.027441496 0.893007161 0.28038652 0.73824212 #> 89  0.920355980 0.855052414 0.556374932 0.54995071 0.44804424 #> 90  0.824874862 0.922223949 0.692704028 0.83684463 0.76759089 #> 91  0.888955155 0.262072365 0.964542825 0.11575496 0.62441113 #> 92  0.150484454 0.840030419 0.307424065 0.66900638 0.39469965 #> 93  0.495364265 0.398318323 0.924772107 0.65580030 0.97717260 #> 94  0.350593768 0.790977478 0.956345581 0.90622495 0.01614503 #> 95  0.042435116 0.139606323 0.100889059 0.96109988 0.27091998 #> 96  0.741039617 0.780601638 0.614621850 0.67526345 0.53697986 #> 97  0.357732598 0.202104322 0.796255789 0.22763737 0.87989101 #> 98  0.757641173 0.865707470 0.200830390 0.92245054 0.17206503 #> 99  0.110809144 0.838882552 0.565201156 0.97762240 0.61076208 #> 100 0.552645630 0.390805527 0.704040335 0.21619488 0.70113720"},{"path":"https://koalaverse.github.io/vip/reference/get_feature_names.html","id":null,"dir":"Reference","previous_headings":"","what":"Extract feature names — get_feature_names","title":"Extract feature names — get_feature_names","text":"Extract predictor names fitted model.","code":""},{"path":"https://koalaverse.github.io/vip/reference/get_feature_names.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Extract feature names — get_feature_names","text":"","code":"get_feature_names(object, ...)"},{"path":"https://koalaverse.github.io/vip/reference/get_feature_names.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Extract feature names — get_feature_names","text":"object appropriate fitted model object. ... Additional optional arguments.","code":""},{"path":"https://koalaverse.github.io/vip/reference/list_metrics.html","id":null,"dir":"Reference","previous_headings":"","what":"List metrics — list_metrics","title":"List metrics — list_metrics","text":"List available performance metrics.","code":""},{"path":"https://koalaverse.github.io/vip/reference/list_metrics.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"List metrics — list_metrics","text":"","code":"list_metrics()"},{"path":"https://koalaverse.github.io/vip/reference/list_metrics.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"List metrics — list_metrics","text":"data frame following columns: metric - optimization tuning metric; description - brief description metric; task - whether metric suitable regression classification; smaller_is_better - logical indicating whether smaller value metric considered better. yardstick_function - name corresponding function yardstick package.","code":""},{"path":"https://koalaverse.github.io/vip/reference/list_metrics.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"List metrics — list_metrics","text":"","code":"(metrics <- list_metrics()) #>          metric                               description #> 1      accuracy                   Classification accuracy #> 2  bal_accuracy          Balanced classification accuracy #> 3        youden Youden;'s index (or Youden's J statistic) #> 4       roc_auc                      Area under ROC curve #> 5        pr_auc    Area under precision-recall (PR) curve #> 6       logloss                                  Log loss #> 7         brier                               Brier score #> 8           mae                       Mean absolute error #> 9          mape            Mean absolute percentage error #> 10         rmse                   Root mean squared error #> 11          rsq                   R-squared (correlation) #> 12     rsq_trad                   R-squared (traditional) #>                                task smaller_is_better yardstick_function #> 1  Binary/multiclass classification             FALSE       accuracy_vec #> 2  Binary/multiclass classification             FALSE   bal_accuracy_vec #> 3  Binary/multiclass classification             FALSE            j_index #> 4             Binary classification             FALSE        roc_auc_vec #> 5             Binary classification             FALSE         pr_auc_vec #> 6  Binary/multiclass classification              TRUE    mn_log_loss_vec #> 7  Binary/multiclass classification              TRUE    brier_class_vec #> 8                        Regression              TRUE            mae_vec #> 9                        Regression              TRUE           mape_vec #> 10                       Regression              TRUE           rmse_vec #> 11                       Regression             FALSE            rsq_vec #> 12                       Regression             FALSE       rsq_trad_vec metrics[metrics$task == \"Multiclass classification\", ] #> [1] metric             description        task               smaller_is_better  #> [5] yardstick_function #> <0 rows> (or 0-length row.names)"},{"path":"https://koalaverse.github.io/vip/reference/titanic.html","id":null,"dir":"Reference","previous_headings":"","what":"Survival of Titanic passengers — titanic","title":"Survival of Titanic passengers — titanic","text":"data set containing survival outcome, passenger class, age, sex, number family members large number passengers aboard ill-fated Titanic.","code":""},{"path":"https://koalaverse.github.io/vip/reference/titanic.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Survival of Titanic passengers — titanic","text":"","code":"titanic"},{"path":"https://koalaverse.github.io/vip/reference/titanic.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Survival of Titanic passengers — titanic","text":"data frame 1309 observations following 6 variables: survived - binary levels \"yes\" survived \"\" otherwise; pclass - integer giving corresponding passenger (.e., ticket) class values 1--3; age - age years corresponding passenger (263 missing values); age - factor giving sex passenger levels \"male\" \"female\"; sibsp - integer giving number siblings/spouses aboard passenger (ranges 0--8); parch - integer giving number parents/children aboard passenger (ranges 0--9).","code":""},{"path":"https://koalaverse.github.io/vip/reference/titanic.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"Survival of Titanic passengers — titanic","text":"https://hbiostat.org/data/.","code":""},{"path":"https://koalaverse.github.io/vip/reference/titanic.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Survival of Titanic passengers — titanic","text":"mentioned column description, age contains 263 NAs (missing values). complete version (versions) data set, see titanic_mice.","code":""},{"path":"https://koalaverse.github.io/vip/reference/titanic_mice.html","id":null,"dir":"Reference","previous_headings":"","what":"Survival of Titanic passengers — titanic_mice","title":"Survival of Titanic passengers — titanic_mice","text":"titanic data set contains 263 missing values (.e., NA's) age column. version data contains imputed values age column using multivariate imputation chained equations via mice package. Consequently, list containing 11 imputed versions observations containd titanic data frame; completed data sets dimension column structure titanic.","code":""},{"path":"https://koalaverse.github.io/vip/reference/titanic_mice.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Survival of Titanic passengers — titanic_mice","text":"","code":"titanic_mice"},{"path":"https://koalaverse.github.io/vip/reference/titanic_mice.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Survival of Titanic passengers — titanic_mice","text":"object class mild (inherits list) length 21.","code":""},{"path":"https://koalaverse.github.io/vip/reference/titanic_mice.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"Survival of Titanic passengers — titanic_mice","text":"Greenwell, Brandon M. (2022). Tree-Based Methods Statistical Learning R. CRC Press.","code":""},{"path":"https://koalaverse.github.io/vip/reference/vi.html","id":null,"dir":"Reference","previous_headings":"","what":"Variable importance — vi","title":"Variable importance — vi","text":"Compute variable importance scores predictors model.","code":""},{"path":"https://koalaverse.github.io/vip/reference/vi.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Variable importance — vi","text":"","code":"vi(object, ...)  # S3 method for default vi(   object,   method = c(\"model\", \"firm\", \"permute\", \"shap\"),   feature_names = NULL,   abbreviate_feature_names = NULL,   sort = TRUE,   decreasing = TRUE,   scale = FALSE,   rank = FALSE,   ... )"},{"path":"https://koalaverse.github.io/vip/reference/vi.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Variable importance — vi","text":"object fitted model object (e.g., randomForest object) object inherits class \"vi\". ... Additional optional arguments passed vi_model, vi_firm, vi_permute, vi_shap; see respective help pages details. method Character string specifying type variable importance (VI) compute. Current options : \"model\" (default), model-specific VI scores (see vi_model details). \"firm\", variance-based VI scores (see vi_firm details). \"permute\", permutation-based VI scores (see vi_permute details). \"shap\", Shapley-based VI scores (see vi_shap details). feature_names Character string giving names predictor variables (.e., features) interest. abbreviate_feature_names Integer specifying length abbreviate feature names. Default NULL results abbreviation (.e., full name feature printed). sort Logical indicating whether order sort variable importance scores. Default TRUE. decreasing Logical indicating whether variable importance scores sorted descending (TRUE) ascending (FALSE) order importance. Default TRUE. scale Logical indicating whether scale variable importance scores largest 100. Default FALSE. rank Logical indicating whether rank variable importance scores (.e., convert integer ranks). Default FALSE. Potentially useful comparing variable importance scores across different models using different methods.","code":""},{"path":"https://koalaverse.github.io/vip/reference/vi.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Variable importance — vi","text":"tidy data frame (.e., tibble object) two columns: Variable - corresponding feature name; Importance - associated importance, computed average change performance random permutation (permutations, nsim > 1) feature question. lm/glm-like objects, whenever method = \"model\", sign (.e., POS/NEG) original coefficient also included column called Sign. method = \"permute\" nsim > 1, additional column (StDev) containing standard deviation individual permutation scores feature also returned; helps assess stability/variation individual permutation importance feature.","code":""},{"path":"https://koalaverse.github.io/vip/reference/vi.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Variable importance — vi","text":"","code":"# # A projection pursuit regression example #  # Load the sample data data(mtcars)  # Fit a projection pursuit regression model mtcars.ppr <- ppr(mpg ~ ., data = mtcars, nterms = 1)  # Prediction wrapper that tells vi() how to obtain new predictions from your # fitted model pfun <- function(object, newdata) predict(object, newdata = newdata)  # Compute permutation-based variable importance scores set.seed(1434)  # for reproducibility (vis <- vi(mtcars.ppr, method = \"permute\", target = \"mpg\", nsim = 10,            metric = \"rmse\", pred_wrapper = pfun, train = mtcars)) #> # A tibble: 10 × 3 #>    Variable Importance   StDev #>    <chr>         <dbl>   <dbl> #>  1 wt         3.17     0.374   #>  2 hp         2.18     0.462   #>  3 gear       0.755    0.367   #>  4 qsec       0.674    0.240   #>  5 cyl        0.462    0.158   #>  6 am         0.173    0.144   #>  7 vs         0.0999   0.0605  #>  8 drat       0.0265   0.0564  #>  9 carb       0.00898  0.00885 #> 10 disp      -0.000824 0.00744  # Plot variable importance scores vip(vis, include_type = TRUE, all_permutations = TRUE,     geom = \"point\", aesthetics = list(color = \"forestgreen\", size = 3))   # # A binary classification example # if (FALSE) { library(rpart)  # for classification and regression trees  # Load Wisconsin breast cancer data; see ?mlbench::BreastCancer for details data(BreastCancer, package = \"mlbench\") bc <- subset(BreastCancer, select = -Id)  # for brevity  # Fit a standard classification tree set.seed(1032)  # for reproducibility tree <- rpart(Class ~ ., data = bc, cp = 0)  # Prune using 1-SE rule (e.g., use `plotcp(tree)` for guidance) cp <- tree$cptable cp <- cp[cp[, \"nsplit\"] == 2L, \"CP\"] tree2 <- prune(tree, cp = cp)  # tree with three splits  # Default tree-based VIP vip(tree2)  # Computing permutation importance requires a prediction wrapper. For # classification, the return value depends on the chosen metric; see # `?vip::vi_permute` for details. pfun <- function(object, newdata) {   # Need vector of predicted class probabilities when using  log-loss metric   predict(object, newdata = newdata, type = \"prob\")[, \"malignant\"] }  # Permutation-based importance (note that only the predictors that show up # in the final tree have non-zero importance) set.seed(1046)  # for reproducibility vi(tree2, method = \"permute\", nsim = 10, target = \"Class\", train = bc,    metric = \"logloss\", pred_wrapper = pfun, reference_class = \"malignant\")  # Equivalent (but not sorted) set.seed(1046)  # for reproducibility vi_permute(tree2, nsim = 10, target = \"Class\", metric = \"logloss\",            pred_wrapper = pfun, reference_class = \"malignant\") }"},{"path":"https://koalaverse.github.io/vip/reference/vi_firm.html","id":null,"dir":"Reference","previous_headings":"","what":"Variance-based variable importance — vi_firm","title":"Variance-based variable importance — vi_firm","text":"Compute variance-based variable importance (VI) scores using simple feature importance ranking measure (FIRM) approach; details, see Greenwell et al. (2018) Scholbeck et al. (2019).","code":""},{"path":"https://koalaverse.github.io/vip/reference/vi_firm.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Variance-based variable importance — vi_firm","text":"","code":"vi_firm(object, ...)  # S3 method for default vi_firm(   object,   feature_names = NULL,   train = NULL,   var_fun = NULL,   var_continuous = stats::sd,   var_categorical = function(x) diff(range(x))/4,   ... )"},{"path":"https://koalaverse.github.io/vip/reference/vi_firm.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Variance-based variable importance — vi_firm","text":"object fitted model object (e.g., randomForest object). ... Additional arguments passed pdp::partial() function (e.g., ice = TRUE, prob = TRUE, prediction wrapper via pred.fun argument); see ?pdp::partial details useful arguments. feature_names Character string giving names predictor variables (.e., features) interest. NULL (default) internal get_feature_names() function called try extract automatically. good practice always specify argument. train matrix-like R object (e.g., data frame matrix) containing training data. NULL (default) internal get_training_data() function called try extract automatically. good practice always specify argument. var_fun Deprecated; use var_continuous var_categorical instead. var_continuous Function used quantify variability effects continuous features. Defaults using sample standard deviation (.e., stats::sd()). var_categorical Function used quantify variability effects categorical features. Defaults using range divided four; , function(x) diff(range(x)) / 4.","code":""},{"path":"https://koalaverse.github.io/vip/reference/vi_firm.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Variance-based variable importance — vi_firm","text":"tidy data frame (.e., tibble object) two columns: Variable - corresponding feature name; Importance - associated importance, computed described Greenwell et al. (2018).","code":""},{"path":"https://koalaverse.github.io/vip/reference/vi_firm.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Variance-based variable importance — vi_firm","text":"approach based quantifying relative \"flatness\" effect feature assumes user familiarity pdp::partial() function.  Feature effects can assessed using partial dependence (PD) plots (Friedman, 2001) individual conditional expectation (ICE) plots (Goldstein et al., 2014). methods model-agnostic can applied supervised learning algorithm. default, relative \"flatness\" defined computing standard deviation y-axis values feature effect plot numeric features; categorical features, default use range divided 4. can changed via var_continuous var_categorical arguments. See Greenwell et al. (2018) details additional examples.","code":""},{"path":"https://koalaverse.github.io/vip/reference/vi_firm.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Variance-based variable importance — vi_firm","text":"approach can provide misleading results presence interaction effects (akin interpreting main effect coefficients linear higher level interaction effects).","code":""},{"path":"https://koalaverse.github.io/vip/reference/vi_firm.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Variance-based variable importance — vi_firm","text":"J. H. Friedman. Greedy function approximation: gradient boosting machine. Annals Statistics, 29: 1189-1232, 2001. Goldstein, ., Kapelner, ., Bleich, J., Pitkin, E., Peeking Inside Black Box: Visualizing Statistical Learning Plots Individual Conditional Expectation. (2014) Journal Computational Graphical Statistics, 24(1): 44-65, 2015. Greenwell, B. M., Boehmke, B. C., McCarthy, . J. Simple Effective Model-Based Variable Importance Measure. arXiv preprint arXiv:1805.04755 (2018). Scholbeck, C. . Scholbeck, Molnar, C.,  Heumann C., Bischl, B., Casalicchio, G. Sampling, Intervention, Prediction, Aggregation: Generalized Framework Model-Agnostic Interpretations. arXiv preprint arXiv:1904.03959 (2019).","code":""},{"path":"https://koalaverse.github.io/vip/reference/vi_firm.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Variance-based variable importance — vi_firm","text":"","code":"if (FALSE) { # # A projection pursuit regression example #  # Load the sample data data(mtcars)  # Fit a projection pursuit regression model mtcars.ppr <- ppr(mpg ~ ., data = mtcars, nterms = 1)  # Compute variable importance scores using the FIRM method; note that the pdp # package knows how to work with a \"ppr\" object, so there's no need to pass # the training data or a prediction wrapper, but it's good practice. vi_firm(mtcars.ppr, train = mtcars)  # For unsopported models, need to define a prediction wrapper; this approach # will work for ANY model (supported or unsupported, so better to just always # define it pass it) pfun <- function(object, newdata) {   # To use partial dependence, this function needs to return the AVERAGE   # prediction (for ICE, simply omit the averaging step)   mean(predict(object, newdata = newdata)) }  # Equivalent to the previous results (but would work if this type of model # was not explicitly supported) vi_firm(mtcars.ppr, pred.fun = pfun, train = mtcars)  # Equivalent VI scores, but the output is sorted by default vi(mtcars.ppr, method = \"firm\")  # Use MAD to estimate variability of the partial dependence values vi_firm(mtcars.ppr, var_continuous = stats::mad)  # Plot VI scores vip(mtcars.ppr, method = \"firm\", train = mtcars, pred.fun = pfun) }"},{"path":"https://koalaverse.github.io/vip/reference/vi_model.html","id":null,"dir":"Reference","previous_headings":"","what":"Model-specific variable importance — vi_model","title":"Model-specific variable importance — vi_model","text":"Compute model-specific variable importance scores predictors fitted model.","code":""},{"path":"https://koalaverse.github.io/vip/reference/vi_model.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Model-specific variable importance — vi_model","text":"","code":"vi_model(object, ...)  # S3 method for default vi_model(object, ...)  # S3 method for C5.0 vi_model(object, type = c(\"usage\", \"splits\"), ...)  # S3 method for train vi_model(object, ...)  # S3 method for cubist vi_model(object, ...)  # S3 method for earth vi_model(object, type = c(\"nsubsets\", \"rss\", \"gcv\"), ...)  # S3 method for gbm vi_model(object, type = c(\"relative.influence\", \"permutation\"), ...)  # S3 method for glmnet vi_model(object, lambda = NULL, ...)  # S3 method for cv.glmnet vi_model(object, lambda = NULL, ...)  # S3 method for H2OBinomialModel vi_model(object, ...)  # S3 method for H2OMultinomialModel vi_model(object, ...)  # S3 method for H2ORegressionModel vi_model(object, ...)  # S3 method for lgb.Booster vi_model(object, type = c(\"gain\", \"cover\", \"frequency\"), ...)  # S3 method for mixo_pls vi_model(object, ncomp = NULL, ...)  # S3 method for mixo_spls vi_model(object, ncomp = NULL, ...)  # S3 method for WrappedModel vi_model(object, ...)  # S3 method for Learner vi_model(object, ...)  # S3 method for nn vi_model(object, type = c(\"olden\", \"garson\"), ...)  # S3 method for nnet vi_model(object, type = c(\"olden\", \"garson\"), ...)  # S3 method for RandomForest vi_model(object, type = c(\"accuracy\", \"auc\"), ...)  # S3 method for constparty vi_model(object, ...)  # S3 method for cforest vi_model(object, ...)  # S3 method for mvr vi_model(object, ...)  # S3 method for mixo_pls vi_model(object, ncomp = NULL, ...)  # S3 method for mixo_spls vi_model(object, ncomp = NULL, ...)  # S3 method for WrappedModel vi_model(object, ...)  # S3 method for Learner vi_model(object, ...)  # S3 method for randomForest vi_model(object, ...)  # S3 method for ranger vi_model(object, ...)  # S3 method for rpart vi_model(object, ...)  # S3 method for mlp vi_model(object, type = c(\"olden\", \"garson\"), ...)  # S3 method for ml_model_decision_tree_regression vi_model(object, ...)  # S3 method for ml_model_decision_tree_classification vi_model(object, ...)  # S3 method for ml_model_gbt_regression vi_model(object, ...)  # S3 method for ml_model_gbt_classification vi_model(object, ...)  # S3 method for ml_model_generalized_linear_regression vi_model(object, ...)  # S3 method for ml_model_linear_regression vi_model(object, ...)  # S3 method for ml_model_random_forest_regression vi_model(object, ...)  # S3 method for ml_model_random_forest_classification vi_model(object, ...)  # S3 method for lm vi_model(object, type = c(\"stat\", \"raw\"), ...)  # S3 method for model_fit vi_model(object, ...)  # S3 method for workflow vi_model(object, ...)  # S3 method for xgb.Booster vi_model(object, type = c(\"gain\", \"cover\", \"frequency\"), ...)"},{"path":"https://koalaverse.github.io/vip/reference/vi_model.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"Model-specific variable importance — vi_model","text":"Johan Bring (1994) Standardize Regression Coefficients, American Statistician, 48:3, 209-213, DOI: 10.1080/00031305.1994.10476059.","code":""},{"path":"https://koalaverse.github.io/vip/reference/vi_model.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Model-specific variable importance — vi_model","text":"object fitted model object (e.g., randomForest object). See details section see variable importance computed supported model types. ... Additional optional arguments passed methods. See details section arguments can passed specific object types. type Character string specifying type variable importance return (used models). See details section methods argument applies . lambda Numeric value penalty parameter glmnet model (equivalent s argument coef.glmnet). See section glmnet details . ncomp integer number partial least squares components used importance calculations. components requested used model, model's components used.","code":""},{"path":"https://koalaverse.github.io/vip/reference/vi_model.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Model-specific variable importance — vi_model","text":"tidy data frame (.e., tibble object) two columns: Variable - corresponding feature name; Importance - associated importance, computed average change performance random permutation (permutations, nsim > 1) feature question. lm/glm-like objects, sign (.e., POS/NEG) original coefficient also included column called Sign.","code":""},{"path":"https://koalaverse.github.io/vip/reference/vi_model.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Model-specific variable importance — vi_model","text":"Computes model-specific variable importance scores depending class object: C5.0 - Variable importance measured determining percentage training set samples fall terminal nodes split. example, predictor first split automatically importance measurement 100 percent since samples affected split. predictors may used frequently splits, terminal nodes cover handful training set samples, importance scores may close zero. strategy applied rule-based models boosted versions model. underlying function can also return number times predictor involved split using option metric = \"usage\". See C5imp details. cubist - Cubist output contains variable usage statistics. gives percentage times variable used condition /linear model. Note output probably inconsistent rules shown output summary.cubist. split tree, Cubist saves linear model (feature selection) allowed terms variable used current split split . Quinlan (1992) discusses smoothing algorithm model prediction linear combination parent child model along tree. , final prediction function linear models initial node terminal node. percentages shown Cubist output reflects models involved prediction (opposed terminal models shown output). variable importance used linear combination usage rule conditions model. See summary.cubist varImp details. glmnet - Similar (generalized) linear models, absolute value coefficients returned specific model. important features  (hence, estimated coefficients) standardized prior fitting model. can specify coefficients return passing specific value penalty parameter via lambda argument (equivalent s argument coef.glmnet). default, lambda = NULL coefficients corresponding final penalty value sequence returned; words, ALWAYS SPECIFY lambda! cv.glmnet objects, largest value lambda error within one standard error minimum used default. multinomial response, coefficients corresponding first class used; , first component coef.glmnet. cforest - Variable importance measured way similar computed importance. Besides standard version, conditional version available adjusts correlations predictor variables. conditional = TRUE, importance variable computed permuting within grid defined predictors associated (1 - p-value greater threshold) variable interest. resulting variable importance score conditional sense beta coefficients regression models, represents effect variable main effects interactions. See Strobl et al. (2008) details. Note, however, random forest results subject random variation. Thus, interpreting importance ranking, check whether ranking achieved different random seed - otherwise increase number trees ntree ctree_control. Note presence missings predictor variables procedure described Hapfelmeier et al. (2012) performed. See varimp details. earth - earth package uses three criteria estimating variable importance MARS model (see evimp details): nsubsets criterion (type = \"nsubsets\") counts number model subsets include feature. Variables included subsets considered important. criterion used summary.earth print variable importance. \"subsets\" mean subsets terms generated earth()'s backward pass. one subset model size (one size selected model) subset best set terms model size. (subsets specified $prune.terms component earth()'s return value.) subsets smaller equal size final model used estimating variable importance. default method used vi_model. rss criterion (type = \"rss\") first calculates decrease RSS subset relative previous subset earth()’s backward pass. (multiple response models, RSS's calculated responses.) variable sums decreases subsets include variable. Finally, ease interpretation summed decreases scaled largest summed decrease 100. Variables cause larger net decreases RSS considered important. gcv criterion (type = \"gcv\") similar rss approach, uses GCV statistic instead RSS. Note adding variable can sometimes increase GCV. (Adding variable deleterious effect model, measured terms estimated predictive power unseen data.) happens often enough, variable can negative total importance, thus appear less important unused variables. gbm - Variable importance computed using one two approaches (See summary.gbm details): standard approach (type = \"relative.influence\") described Friedman (2001). distribution = \"gaussian\" returns reduction squared error attributable variable. loss functions returns reduction attributable variable sum squared error predicting gradient iteration. describes relative influence variable reducing loss function. default method used vi_model. experimental permutation-based approach (type = \"permutation\"). method randomly permutes predictor variable time computes associated reduction predictive performance. similar variable importance measures Leo Breiman uses random forests, gbm currently computes using entire training dataset (--bag observations). H2OModel - See h2o.varimp visit https://docs.h2o.ai/h2o/latest-stable/h2o-docs/variable-importance.html details. nnet - Two popular methods constructing variable importance scores neural networks Garson algorithm (Garson 1991), later modified Goh (1995), Olden algorithm (Olden et al. 2004). algorithms, basis importance scores network’s connection weights. Garson algorithm determines variable importance identifying weighted connections nodes interest. Olden’s algorithm, hand, uses product raw connection weights input output neuron sums product across hidden neurons. shown outperform Garson method various simulations. DNNs, similar method due Gedeon (1997) considers weights connecting input features first two hidden layers (simplicity speed); method can slow large networks.. implement Olden Garson algorithms, use type = \"olden\" type = \"garson\", respectively. See garson olden details. lm/glm - (generalized) linear models, variable importance typically based absolute value corresponding t-statistics (Bring, 1994). models, sign original coefficient also returned. default, type = \"stat\" used; however, inputs appropriately standardized raw coefficients can used type = \"raw\". Note Bring (1994) provides motivation using absolute value associated t-statistics. sparklyr - Spark ML library provides standard variable importance measures tree-based methods (e.g., random forests). See ml_feature_importances details. randomForest Random forests typically provide two measures variable importance. first measure computed permuting --bag (OOB) data: tree, prediction error OOB portion data recorded (error rate classification MSE regression). done permuting predictor variable. difference two averaged trees forest, normalized standard deviation differences. standard deviation differences equal 0 variable, division done (average almost always equal 0 case). second measure total decrease node impurities splitting variable, averaged trees. classification, node impurity measured Gini index. regression, measured residual sum squares. See importance details, including additional arguments can passed via ... argument vi_model. cforest - approach described cforest (package partykit) . See varimp varimpAUC (type = \"auc\") details. ranger - Variable importance ranger objects computed usual way random forests. approach used depends importance argument provided initial call ranger. See importance details. rpart - stated one rpart vignettes. variable may appear tree many times, either primary surrogate variable. overall measure variable importance sum goodness split measures split primary variable, plus \"goodness\" * (adjusted agreement) splits surrogate. Imagine two variables essentially duplicates ; count surrogates, split importance neither showing strongly . See rpart details. caret - Various model-specific model-agnostic approaches depend learning algorithm employed original call caret. See varImp details. xgboost - linear models, variable importance absolute magnitude estimated coefficients. reason, order obtain meaningful ranking importance linear model, features need scale (also want using either L1 L2 regularization). Otherwise, approach described Friedman (2001) gbms used. See xgb.importance details. tree models, can obtain three different types variable importance: Using type = \"gain\" (default) gives fractional contribution feature model based total gain corresponding feature's splits. Using type = \"cover\" gives number observations related feature. Using type = \"frequency\" gives percentages representing relative number times feature used throughout tree ensemble. lightgbm - xgboost models, except lgb.importance (method calls internally) additional argument, percentage, defaults TRUE, resulting VI scores shown relative percentage; pass percentage = FALSE call vi_model() produce VI scores lightgbm models raw scale.","code":""},{"path":"https://koalaverse.github.io/vip/reference/vi_model.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Model-specific variable importance — vi_model","text":"Inspired caret's varImp function.","code":""},{"path":"https://koalaverse.github.io/vip/reference/vi_model.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Model-specific variable importance — vi_model","text":"","code":"if (FALSE) { # Basic example using imputed titanic data set t3 <- titanic_mice[[1L]]  # Fit a simple model set.seed(1449)  # for reproducibility bst <- lightgbm::lightgbm(   data = data.matrix(subset(t3, select = -survived)),   label = ifelse(t3$survived == \"yes\", 1, 0),   params = list(\"objective\" = \"binary\", \"force_row_wise\" = TRUE),   verbose = 0 )  # Compute VI scores vi(bst)  # defaults to `method = \"model\"` vi_model(bst)  # same as above  # Same as above (since default is `method = \"model\"`), but returns a plot vip(bst, geom = \"point\") vi_model(bst, type = \"cover\") vi_model(bst, type = \"cover\", percentage = FALSE)  # Compare to lightgbm::lgb.importance(bst) }"},{"path":"https://koalaverse.github.io/vip/reference/vi_permute.html","id":null,"dir":"Reference","previous_headings":"","what":"Permutation-based variable importance — vi_permute","title":"Permutation-based variable importance — vi_permute","text":"Compute permutation-based variable importance scores predictors model; details algorithm, see Greenwell Boehmke (2020).","code":""},{"path":"https://koalaverse.github.io/vip/reference/vi_permute.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Permutation-based variable importance — vi_permute","text":"","code":"vi_permute(object, ...)  # S3 method for default vi_permute(   object,   feature_names = NULL,   train = NULL,   target = NULL,   metric = NULL,   smaller_is_better = NULL,   type = c(\"difference\", \"ratio\"),   nsim = 1,   keep = TRUE,   sample_size = NULL,   sample_frac = NULL,   reference_class = NULL,   event_level = NULL,   pred_wrapper = NULL,   verbose = FALSE,   parallel = FALSE,   parallelize_by = c(\"features\", \"repetitions\"),   ... )"},{"path":"https://koalaverse.github.io/vip/reference/vi_permute.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Permutation-based variable importance — vi_permute","text":"object fitted model object (e.g., randomForest object). ... Additional optional arguments passed foreach (e.g., .packages .export). feature_names Character string giving names predictor variables (.e., features) interest. NULL (default) inferred train target arguments (see ). good practice always specify argument. train matrix-like R object (e.g., data frame matrix) containing training data. NULL (default) internal get_training_data() function called try extract automatically. good practice always specify argument. target Either character string giving name (position) target column train , train contains feature columns, vector containing target values used train object. metric Either function character string specifying performance metric use computing model performance (e.g., RMSE regression accuracy binary classification). metric function, requires two arguments, actual predicted, return single, numeric value. Ideally, metric used train object. See list_metrics() list built-metrics. smaller_is_better Logical indicating whether smaller value metric better. Default NULL. Must supplied metric user-supplied function. type Character string specifying compare baseline permuted performance metrics. Current options \"difference\" (default) \"ratio\". nsim Integer specifying number Monte Carlo replications perform. Default 1. nsim > 1, results replication simply averaged together (standard deviation also returned). keep Logical indicating whether keep individual permutation scores nsim repetitions. TRUE (default) individual variable importance scores stored attribute called \"raw_scores\". (used nsim > 1.) sample_size Integer specifying size random sample use Monte Carlo repetition. Default NULL (.e., use available training data). specified sample_frac. Can used reduce computation time large data sets. sample_frac Proportion specifying size random sample use Monte Carlo repetition. Default NULL (.e., use available training data). specified sample_size. Can used reduce computation time large data sets. reference_class Deprecated, use event_level instead. event_level String specifying factor level truth consider \"event\". Options \"first\" (default) \"second\". argument applicable binary classification metric one \"roc_auc\", \"pr_auc\", \"youden\". argument passed corresponding yardstick metric. pred_wrapper Prediction function requires two arguments, object newdata. output function determined metric used: Regression - numeric vector predicted outcomes. Binary classification - vector predicted class labels (e.g., using misclassification error) vector predicted class probabilities reference class (e.g., using log loss AUC). Multiclass classification - vector predicted class labels (e.g., using misclassification error) matrix/data frame predicted class probabilities class (e.g., using log loss AUC). verbose Logical indicating whether print information construction variable importance scores. Default FALSE. parallel Logical indicating whether run vi_permute() parallel (using backend provided foreach package). Default FALSE. TRUE, foreach-compatible backend must provided must provided. Note set.seed() work foreach's parellelized loops; workaround, see solution. parallelize_by Character string specifying whether parallelize across features (parallelize_by = \"features\") repetitions (parallelize_by = \"reps\"); latter useful whenever nsim > 1. Default \"features\".","code":""},{"path":"https://koalaverse.github.io/vip/reference/vi_permute.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Permutation-based variable importance — vi_permute","text":"tidy data frame (.e., tibble object) two columns: Variable - corresponding feature name; Importance - associated importance, computed average change performance random permutation (permutations, nsim > 1) feature question. nsim > 1, additional column (StDev) containing standard deviation individual permutation scores feature also returned; helps assess stability/variation individual permutation importance feature.","code":""},{"path":"https://koalaverse.github.io/vip/reference/vi_permute.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Permutation-based variable importance — vi_permute","text":"Brandon M. Greenwell Bradley C. Boehmke, R Journal (2020) 12:1, pages 343-366.","code":""},{"path":[]},{"path":"https://koalaverse.github.io/vip/reference/vi_shap.html","id":null,"dir":"Reference","previous_headings":"","what":"SHAP-based variable importance — vi_shap","title":"SHAP-based variable importance — vi_shap","text":"Compute SHAP-based VI scores predictors model. See details .","code":""},{"path":"https://koalaverse.github.io/vip/reference/vi_shap.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"SHAP-based variable importance — vi_shap","text":"","code":"vi_shap(object, ...)  # S3 method for default vi_shap(object, feature_names = NULL, train = NULL, ...)"},{"path":"https://koalaverse.github.io/vip/reference/vi_shap.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"SHAP-based variable importance — vi_shap","text":"object fitted model object (e.g., randomForest object). ... Additional arguments passed fastshap::explain() (e.g., nsim =  30, adjust = TRUE, avprediction wrapper via pred_wrapper argument); see ?fastshap::explain details useful arguments. feature_names Character string giving names predictor variables (.e., features) interest. NULL (default) inferred train target arguments (see ). good practice always specify argument. train matrix-like R object (e.g., data frame matrix) containing training data. NULL (default) internal get_training_data() function called try extract automatically. good practice always specify argument.","code":""},{"path":"https://koalaverse.github.io/vip/reference/vi_shap.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"SHAP-based variable importance — vi_shap","text":"tidy data frame (.e., tibble object) two columns: Variable - corresponding feature name; Importance - associated importance, computed mean absolute Shapley value.","code":""},{"path":"https://koalaverse.github.io/vip/reference/vi_shap.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"SHAP-based variable importance — vi_shap","text":"approach computing VI scores based mean absolute value SHAP values feature; see, example, https://github.com/shap/shap references therein. Strumbelj, E., Kononenko, . Explaining prediction models individual predictions feature contributions. Knowledge information systems 41.3 (2014): 647-665.","code":""},{"path":"https://koalaverse.github.io/vip/reference/vi_shap.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"SHAP-based variable importance — vi_shap","text":"","code":"if (FALSE) { library(ggplot2)  # for theme_light() function library(xgboost)  # Simulate training data trn <- gen_friedman(500, sigma = 1, seed = 101)  # ?vip::gen_friedman  # Feature matrix X <- data.matrix(subset(trn, select = -y))  # matrix of feature values  # Fit an XGBoost model; hyperparameters were tuned using 5-fold CV set.seed(859)  # for reproducibility bst <- xgboost(X, label = trn$y, nrounds = 338, max_depth = 3, eta = 0.1,                verbose = 0)  # Construct VIP using \"exact\" SHAP values from XGBoost's internal Tree SHAP # functionality vip(bst, method = \"shap\", train = X, exact = TRUE, include_type = TRUE,     geom = \"point\", horizontal = FALSE,     aesthetics = list(color = \"forestgreen\", shape = 17, size = 5)) +   theme_light()  # Use Monte-Carlo approach, which works for any model; requires prediction # wrapper pfun_prob <- function(object, newdata) {  # prediction wrapper   # For Shapley explanations, this should ALWAYS return a numeric vector   predict(object, newdata = newdata, type = \"prob\")[, \"yes\"] }  # Compute Shapley-based VI scores set.seed(853)  # for reproducibility vi_shap(rfo, train = subset(t1, select = -survived), pred_wrapper = pfun_prob,         nsim = 30) ## # A tibble: 5 × 2 ## Variable Importance ##   <chr>         <dbl> ## 1 pclass       0.104 ## 2 age          0.0649 ## 3 sex          0.272 ## 4 sibsp        0.0260 ## 5 parch        0.0291 }"},{"path":"https://koalaverse.github.io/vip/reference/vip.html","id":null,"dir":"Reference","previous_headings":"","what":"Variable importance plots — vip","title":"Variable importance plots — vip","text":"Plot variable importance scores predictors model.","code":""},{"path":"https://koalaverse.github.io/vip/reference/vip.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Variable importance plots — vip","text":"","code":"vip(object, ...)  # S3 method for default vip(   object,   num_features = 10L,   geom = c(\"col\", \"point\", \"boxplot\", \"violin\"),   mapping = NULL,   aesthetics = list(),   horizontal = TRUE,   all_permutations = FALSE,   jitter = FALSE,   include_type = FALSE,   ... )  # S3 method for model_fit vip(object, ...)  # S3 method for workflow vip(object, ...)  # S3 method for WrappedModel vip(object, ...)  # S3 method for Learner vip(object, ...)"},{"path":"https://koalaverse.github.io/vip/reference/vip.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Variable importance plots — vip","text":"object fitted model (e.g., class randomForest object) vi object. ... Additional optional arguments passed vi. num_features Integer specifying number variable importance scores plot. Default 10. geom Character string specifying type plot construct. currently available options described . geom = \"col\" uses geom_col construct bar chart variable importance scores. geom = \"point\" uses geom_point construct Cleveland dot plot variable importance scores. geom = \"boxplot\" uses geom_boxplot construct boxplot plot variable importance scores. option can permutation-based importance method nsim > 1 keep = TRUE; see vi_permute details. geom = \"violin\" uses geom_violin construct violin plot variable importance scores. option can permutation-based importance method nsim > 1 keep = TRUE; see vi_permute details. mapping Set aesthetic mappings created aes aes_. See example usage . aesthetics List specifying additional arguments passed layer. often aesthetics, used set aesthetic fixed value, likecolour = \"red\" size = 3. See example usage . horizontal Logical indicating whether plot importance scores x-axis (TRUE). Default TRUE. all_permutations Logical indicating whether plot permutation scores along average. Default FALSE. (used permutation scores nsim > 1.) jitter Logical indicating whether jitter raw permutation scores. Default FALSE. (used all_permutations = TRUE.) include_type Logical indicating whether include type variable importance computed axis label. Default FALSE.","code":""},{"path":"https://koalaverse.github.io/vip/reference/vip.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Variable importance plots — vip","text":"","code":"# # A projection pursuit regression example using permutation-based importance #  # Load the sample data data(mtcars)  # Fit a projection pursuit regression model model <- ppr(mpg ~ ., data = mtcars, nterms = 1)  # Construct variable importance plot (permutation importance, in this case) set.seed(825)  # for reproducibility pfun <- function(object, newdata) predict(object, newdata = newdata) vip(model, method = \"permute\", train = mtcars, target = \"mpg\", nsim = 10,     metric = \"rmse\", pred_wrapper = pfun)   # Better yet, store the variable importance scores and then plot set.seed(825)  # for reproducibility vis <- vi(model, method = \"permute\", train = mtcars, target = \"mpg\",           nsim = 10, metric = \"rmse\", pred_wrapper = pfun) vip(vis, geom = \"point\", horiz = FALSE)  vip(vis, geom = \"point\", horiz = FALSE, aesthetics = list(size = 3))   # Plot unaggregated permutation scores (boxplot colored by feature) library(ggplot2)  # for `aes_string()` function vip(vis, geom = \"boxplot\", all_permutations = TRUE, jitter = TRUE,     mapping = aes_string(fill = \"Variable\"),     aesthetics = list(color = \"grey35\", size = 0.8)) #> Warning: `aes_string()` was deprecated in ggplot2 3.0.0. #> ℹ Please use tidy evaluation idioms with `aes()`. #> ℹ See also `vignette(\"ggplot2-in-packages\")` for more information.   # # A binary classification example # if (FALSE) { library(rpart)  # for classification and regression trees  # Load Wisconsin breast cancer data; see ?mlbench::BreastCancer for details data(BreastCancer, package = \"mlbench\") bc <- subset(BreastCancer, select = -Id)  # for brevity  # Fit a standard classification tree set.seed(1032)  # for reproducibility tree <- rpart(Class ~ ., data = bc, cp = 0)  # Prune using 1-SE rule (e.g., use `plotcp(tree)` for guidance) cp <- tree$cptable cp <- cp[cp[, \"nsplit\"] == 2L, \"CP\"] tree2 <- prune(tree, cp = cp)  # tree with three splits  # Default tree-based VIP vip(tree2)  # Computing permutation importance requires a prediction wrapper. For # classification, the return value depends on the chosen metric; see # `?vip::vi_permute` for details. pfun <- function(object, newdata) {   # Need vector of predicted class probabilities when using  log-loss metric   predict(object, newdata = newdata, type = \"prob\")[, \"malignant\"] }  # Permutation-based importance (note that only the predictors that show up # in the final tree have non-zero importance) set.seed(1046)  # for reproducibility vip(tree2, method = \"permute\", nsim = 10, target = \"Class\",     metric = \"logloss\", pred_wrapper = pfun, reference_class = \"malignant\") }"},{"path":[]},{"path":"https://koalaverse.github.io/vip/news/index.html","id":"changed-0-4-1","dir":"Changelog","previous_headings":"","what":"Changed","title":"vip 0.4.1","text":"Minor tweaks URLs tests pass CRAN checks.","code":""},{"path":"https://koalaverse.github.io/vip/news/index.html","id":"vip-040","dir":"Changelog","previous_headings":"","what":"vip 0.4.0","title":"vip 0.4.0","text":"CRAN release: 2023-07-19","code":""},{"path":"https://koalaverse.github.io/vip/news/index.html","id":"changed-0-4-0","dir":"Changelog","previous_headings":"","what":"Changed","title":"vip 0.4.0","text":"NEWS file now follows Keep Changelog format. Removed lifecycle badge README file. training data explicitly passed cases using vi_permute(), vi_shap(), vi_firm(). Raised R version dependency >= 4.1.0 (introduction native piper operator |>). vi_permute function now relies yardstick package compouting performance measures (e.g., RMSE log loss); consequently, user-supplied metric functions now nned conform yardstick metric argument names. var_fun argument vi_firm() deprecated; use new var_continuous var_categorical instead. explicit ice argument vi_firm() removed; really needed since can passed via ... argument. Removed magrittr imports; ’s easy enough just laod package need use R’s newer internal pipe operator. Tweaked examples. Tests based fastshap now check make sure ’s available. Suppress loading mixOmics tests. Switched lifecycle badge “maturing”, superseded, “experimental.” Fixed H2O URL vi_model.R. Removed unnecessary LazyData: true line DESCRIPTION file. Switched using markdown syntax roxygen2 comments.","code":""},{"path":"https://koalaverse.github.io/vip/news/index.html","id":"added-0-4-0","dir":"Changelog","previous_headings":"","what":"Added","title":"vip 0.4.0","text":"vi_model() now supports lightgbm models. Thanks @nipnipj suggestion (#146). permutation importance method (.e., function vi_permute()) now integrates uses yardstick performance metrics. list_metrics() gained additional smaller_is_better column indicating whether corresponding metric minimized (smaller_is_better = TRUE) maximized (smaller_is_better = FALSE); thanks @topedo. Additionally, column names now lower case. Added support partial least squares via mixOmics package (PR #129); thanks @topedo. Added support workflows parsnip packages tidymodels ecosystem (PR #128); thanks @topedo. New pkgdown site vignette based original R Journal article.","code":""},{"path":"https://koalaverse.github.io/vip/news/index.html","id":"removed-0-4-0","dir":"Changelog","previous_headings":"","what":"Removed","title":"vip 0.4.0","text":"Function add_sparklines() seems scope removed. Function vint() also seems scope slow implement practical problems; now, function likely live moreparty package.","code":""},{"path":"https://koalaverse.github.io/vip/news/index.html","id":"fixed-0-4-0","dir":"Changelog","previous_headings":"","what":"Fixed","title":"vip 0.4.0","text":"Fix model-based VI support mlr, mlr3, parsnip, workflows model fits.","code":""},{"path":"https://koalaverse.github.io/vip/news/index.html","id":"vip-032","dir":"Changelog","previous_headings":"","what":"vip 0.3.2","title":"vip 0.3.2","text":"CRAN release: 2020-12-17","code":""},{"path":"https://koalaverse.github.io/vip/news/index.html","id":"miscellaneous-0-3-2","dir":"Changelog","previous_headings":"","what":"Miscellaneous","title":"vip 0.3.2","text":"Add tools/ .Rbuildignore.","code":""},{"path":[]},{"path":"https://koalaverse.github.io/vip/news/index.html","id":"miscellaneous-0-3-1","dir":"Changelog","previous_headings":"","what":"Miscellaneous","title":"vip 0.3.1","text":"Change http://spark.rstudio.com/mlib/ https://spark.rstudio.com/mlib/ NEWS.md. Remove unnecessary codecov.yml file.","code":""},{"path":[]},{"path":"https://koalaverse.github.io/vip/news/index.html","id":"user-visable-changes-0-3-0","dir":"Changelog","previous_headings":"","what":"User-visable changes","title":"vip 0.3.0","text":"Removed deprecated arguments vip(); particular, bar, width, alpha, color, fill, size, shape. Users instead rely mapping aesthetics arguments; see ?vip::vip details.","code":""},{"path":"https://koalaverse.github.io/vip/news/index.html","id":"bug-fixes-0-3-0","dir":"Changelog","previous_headings":"","what":"Bug fixes","title":"vip 0.3.0","text":"Fixed couple bugs occurred using vi_model() glmnet package. particular, added new lamnda parameter specifying value penalty term use extracting estimated coefficients. equivalent s argument glmnet::coef(); name lambda chosen conflict arguments vi(). Additionally, vi_model() return absolute value estimated coefficients glmnet models like advertised, now fixed version (#103).","code":""},{"path":"https://koalaverse.github.io/vip/news/index.html","id":"miscellaneous-0-3-0","dir":"Changelog","previous_headings":"","what":"Miscellaneous","title":"vip 0.3.0","text":"Switched Travis-CI GitHub Actions continuous integration. Added CITATION file PDF-based vignette based published article R Journal (#109). Switch tibble::.tibble()—deprecated tibble 2.0.0—tibble::as_tibble() function calls (#101).","code":""},{"path":"https://koalaverse.github.io/vip/news/index.html","id":"vip-022","dir":"Changelog","previous_headings":"","what":"vip 0.2.2","title":"vip 0.2.2","text":"CRAN release: 2020-04-06","code":""},{"path":"https://koalaverse.github.io/vip/news/index.html","id":"user-visible-changes-0-2-2","dir":"Changelog","previous_headings":"","what":"User-visible changes","title":"vip 0.2.2","text":"Importance column vi_model() longer contains “inner” names; accordance breaking changes tibble 3.0.0.","code":""},{"path":"https://koalaverse.github.io/vip/news/index.html","id":"vip-021","dir":"Changelog","previous_headings":"","what":"vip 0.2.1","title":"vip 0.2.1","text":"CRAN release: 2020-01-20","code":""},{"path":"https://koalaverse.github.io/vip/news/index.html","id":"enhancements-0-2-1","dir":"Changelog","previous_headings":"","what":"Enhancements","title":"vip 0.2.1","text":"Added support SHAP-based feature importance makes use recent fastshap package CRAN. use, simply call vi() vip() specify method = \"shap\", can just call vi_shap() directly (#87). Added support parsnip, mlr, mlr3 packages (#94). Added support \"mvr\" objects pls package (currently just calls caret::varImp()) (#35). \"lm\" method vi_model() gained new type argument allows users use either (1) raw coefficients features properly standardized (type = \"raw\"), (2) absolute value corresponding t- z-statistic (type = \"stat\", default) (#77). New function gen_friedman() simulating data Friedman 1 benchmark problem; see ?vip::gen_friedman details.","code":""},{"path":"https://koalaverse.github.io/vip/news/index.html","id":"user-visible-changes-0-2-1","dir":"Changelog","previous_headings":"","what":"User-visible changes","title":"vip 0.2.1","text":"vi_pdp() vi_ice() functions deprecated merged single new function called vi_firm(). Consequently, setting method = \"pdp\" method = \"ice\" also deprecated; use method = \"firm\" instead. metric pred_wrapper arguments vi_permute() longer optional. vip() function gained new argument, geom, specifying type plot construct. Current options geom = \"col\" (default), geom = \"point\", geom = \"boxplot\", geom = \"violin\" (latter two work permutation-based importance nsim > 1) (#79). Consequently, bar argument removed. vip() function gained two new arguments specifying aesthetics: mapping aesthetics (fixed aesthetics like color = \"red\"). Consequently, arguments color, fill, etc. removed (#80). example illustrating two changes given : vip() function gained new argument, include_type, defaults FALSE. TRUE, type variable importance computed included appropriate axis label. Set include_type = TRUE revert old behavior.","code":"# Load required packages library(ggplot2)  # for `aes_string()` function  # Load the sample data data(mtcars)  # Fit a linear regression model model <- lm(mpg ~ ., data = mtcars)  # Construct variable importance plots p1 <- vip(model) p2 <- vip(model, mapping = aes_string(color = \"Sign\")) p3 <- vip(model, type = \"dotplot\") p4 <- vip(model, type = \"dotplot\", mapping = aes_string(color = \"Variable\"),           aesthetics = list(size = 3)) grid.arrange(p1, p2, p3, p4, nrow = 2)"},{"path":"https://koalaverse.github.io/vip/news/index.html","id":"miscellaneous-0-2-1","dir":"Changelog","previous_headings":"","what":"Miscellaneous","title":"vip 0.2.1","text":"Removed dependency ModelMetrics built-family performance metrics (metric_*()) now documented exported. See, example, ?vip::metric_rmse (#93). Switched tinytest framework (#82). Minor documentation improvements.","code":""},{"path":"https://koalaverse.github.io/vip/news/index.html","id":"bug-fixes-0-2-1","dir":"Changelog","previous_headings":"","what":"Bug fixes","title":"vip 0.2.1","text":"internal (.e., exported) get_feature_names() function better job \"nnet\" objects containing factors. also better job extracting feature names model objects containing \"formula\" component. vi_model() now works correctly \"glm\" objects non-Gaussian families (e.g., logistic regression) (#74). Added appropriate sparklyr version dependency (#59).","code":""},{"path":"https://koalaverse.github.io/vip/news/index.html","id":"vip-013","dir":"Changelog","previous_headings":"","what":"vip 0.1.3","title":"vip 0.1.3","text":"CRAN release: 2019-07-03","code":""},{"path":"https://koalaverse.github.io/vip/news/index.html","id":"new-functions-0-1-3","dir":"Changelog","previous_headings":"","what":"New functions","title":"vip 0.1.3","text":"Removed warnings experimental functions. vi_permute() gained type argument (.e., type = \"difference\" type = \"ratio\"); argument can passed via vi() vip() well. add_sparklines() creates HTML widget display variable importance scores sparkline representation features effect (.e., partial dependence function) (#64). Added support Olden Garson algorithms neural networks fit using neuralnet, nnet, RSNNS packages (#28). Added support GLMNET models fit using glmnet package (without cross-validation).","code":""},{"path":"https://koalaverse.github.io/vip/news/index.html","id":"breaking-changes-0-1-3","dir":"Changelog","previous_headings":"","what":"Breaking changes","title":"vip 0.1.3","text":"pred_fun argument vi_permute() changed pred_wrapper. FUN argument vi(), vi_pdp(), vi_ice() changed var_fun. predicted class probabilities reference class required (numeric vector) binary classification metric = \"auc\" metric = \"logloss\".","code":""},{"path":"https://koalaverse.github.io/vip/news/index.html","id":"minor-changes-0-1-3","dir":"Changelog","previous_headings":"","what":"Minor changes","title":"vip 0.1.3","text":"vi_permute() gained new logical keep argument. TRUE (default), raw permutation scores nsim repetitions (provided nsim > 1) stored attribute called \"raw_scores\". vip() gained new logical arguments all_permutations jitter help visualize raw permutation scores nsim repetitions (provided nsim > 1). can now pass type argument vi_permute() specifying compare baseline permuted performance metrics. Current choices \"difference\" (default) \"ratio\". Improved documentation (especially vi_permute() vi_model()). Results vi_model(), vi_pdp(), vi_ice(), vi_permute() now class \"vi\", making easier plot vip().","code":""},{"path":"https://koalaverse.github.io/vip/news/index.html","id":"vip-012","dir":"Changelog","previous_headings":"","what":"vip 0.1.2","title":"vip 0.1.2","text":"CRAN release: 2018-09-30 Added nsim argument vi_permute() reducing sampling variability induced permuting predictor (#36). Added sample_size sample_frac arguments vi_permute() reducing size training sample every Monte Carlo repetition (#41). Greatly improved documentation vi_model() various objects supports. New argument rank, defaults FALSE, available vi() (#55). Added support Spark (G)LMs. vi() now generic makes adding new methods easier (e.g., support DataRobot models). Bug fixes.","code":""},{"path":"https://koalaverse.github.io/vip/news/index.html","id":"vip-011","dir":"Changelog","previous_headings":"","what":"vip 0.1.1","title":"vip 0.1.1","text":"CRAN release: 2018-09-27 Fixed bug get_feature_names.ranger() s.t. never returns NULL; either returns feature names throws error recovered model object (#43). Added pkgdown site: https://github.com/koalaverse/vip. Changed truncate_feature_names argument vi() abbreviate_feature_names abbreviates feature names, rather just truncating . Added CRAN-related badges (#32). New generic vi_permute() constructing permutation-based variable importance scores (#19). Fixed bug unnecessary error check vint() (#38). New vignette using vip unsupported models (using Keras API TensorFlow example). Added basic sparklyr support.","code":""},{"path":"https://koalaverse.github.io/vip/news/index.html","id":"vip-010","dir":"Changelog","previous_headings":"","what":"vip 0.1.0","title":"vip 0.1.0","text":"CRAN release: 2018-06-15 Added support XGBoost models (.e., objects class \"xgb.booster\"). Added support ranger models (.e., objects class \"ranger\"). Added support random forest models party package (.e., objects class \"RandomForest\"). vip() gained new argument, num_features, specifying many variable importance scores plot. default set 10. . changed _ argument names. vi() gained three new arguments: truncate_feature_names (truncating feature names returned tibble), sort (logical argument specifying whether resulting variable importance scores sorted), decreasing (logical argument specifying whether variable importance scores sorted decreasing order). vi_model.lm(), hence vi(), contains additional column called Sign contains sign original coefficients (#27). vi() gained new argument, scale, scaling variable importance scores largest 100. Default FALSE (#24). vip() gained two new arguments, size shape, controlling size shape points whenever bar = FALSE (#9). Added support \"H2OBinomialModel\", \"H2OMultinomialModel\", , \"H2ORegressionModel\" objects (#8).","code":""},{"path":"https://koalaverse.github.io/vip/news/index.html","id":"vip-001","dir":"Changelog","previous_headings":"","what":"vip 0.0.1","title":"vip 0.0.1","text":"Initial release.","code":""}]
