---
title: "Using vip with unsupported models"
subtitle: "A TensorFlow example using the Keras API"
author: "Brandon M. Greenwell and Bradley C. Boehmke"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
bibliography: vip.bib
vignette: >
  %\VignetteIndexEntry{Vignette Title}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  cache = FALSE,
  collapse = TRUE,
  comment = "#>",
  fig.align = "left",
  fig.width = 6, 
  fig.asp = 0.618,
  out.width = "70%"
)
```

It is possible to use the [vip](https://koalaverse.github.io/vip/index.html) package [@pkg-vip] with any fitted model for which new predictions can be generated. This is possible via `method = "ice"`, `method = "pdp"`, and `method = "permute"` since these methods construct variable importance (VI) scores based solely off of a model's predictions---albeit, in different ways. In this vignette, we will demonstrate the construction of permutation-based VI scores (i.e., `method = "permute"`) using a TensorFlow model trained to the Boston housing data with the [keras](https://keras.rstudio.com/) package [@pkg-keras]. This particular example is adapted from @chollet-deep-2018. We'll supplement the the variable importance plot (VIP) with feature effect plots using the [pdp](https://github.com/bgreenwell/pdp) package [@greenwell-pdp-2017]---a general R package for constructing *partial dependence plots* (PDPs) [@friedman-greedy-2001] and *individual conditional expectation* (ICE) curves [@goldstein-peeking-2015].


### Prerequisites

```{r prerequisites, message=FALSE}
# Load required packages
library(dplyr)    # for data wrangling
library(ggplot2)  # for general visualization
library(keras)    # for fitting DNNs
library(pdp)      # for partial depe
library(vip)      # for visualizing feature importance

# For reproducibility
use_session_with_seed(101)
```


## Predicting median home value

To illustrate, we'll fit a TensorFlow model to the Boston housing data [@harrison-1978-hedonic]. A corrected version of these data are available in the [pdp](https://github.com/bgreenwell/pdp) package. In the code chunk below, we load a corrected version of the original Boston housing data (see `?pdp::boston` for details) and separate the training features (`train_x`) from the training response values (`train_y`).

```{r load-data}
# Loading (corrected) Boston housing data
data(boston, package = "pdp")

# Construct matrix of training data (features only)
train_x <- boston %>%
  select(-cmedv) %>%                   # remove response
  mutate(chas = as.numeric(chas)) %>%  # convert factor to numeric
  as.matrix()                          # convert to numeric matrix

# Construct vector of training response values
train_y <- boston$cmedv
```

Since the features are measured on very different scales (e.g., longitude and per capita crime rate by town), we center and scale the columns of `train_x` using the `scale()` function.

```{r normalize-data}
train_x <- scale(train_x, center = TRUE, scale = TRUE)  # normalize data
apply(train_x, MARGIN = 2, FUN = function(x) c(mean(x), sd(x)))  # sanity check
```

Next, we define a function for fitting a Keras model composed of a linear stack of layers. Since the Boston housing data is rather small ($n =$ `r nrow(boston)`), we'll use a very small network with only two hidden layers, each with 64 units. Building small networks like this can help mitigate overfitting to smaller data sets.

```{r define-model}
build_model <- function() {                                
  model <- keras_model_sequential() %>%
    layer_dense(units = 64, activation = "relu",
                input_shape = dim(train_x)[[2]]) %>%
    layer_dense(units = 64, activation = "relu") %>%
    layer_dense(units = 1)
  model %>% compile(
    optimizer = "rmsprop",
    loss = "mse",
    metrics = c("mae")
  )
}
```

Since we don't have a lot of observations, we used $k$-fold cross-validation (CV) (with $k = 4$) to evaluate the network and choose the optimal number of *epochs*^[An epoch refers to a single iteration (both forward and backward) over the entire training data set.] ($k$-fold CV is illustrated in the **Figure 1** below). 

```{r cv-illustration, echo=FALSE, fig.cap="**Figure 1** Illustration of $k$-fold CV. The training data are split into $k$ (roughly) equal blocks at random. For each of $k$ iterations, $k-1$ blocks are used for training and the performace is evalutaed on the $k$-th block. The overall measure of performance is obtained by averaging the $k$ performance metrics together."}
knitr::include_graphics("cv.png")
```

The performance of the network was evaluated using *mean absolute error* (MAE), which is the absolute value of the difference between the predicted and observed outcomes. In this example, the cross-validated MAE stopped improving after about 125 epochs. Using this result, we train a final network using 80 epochs^[In practice, you'll want to tune various other hyperparameters of the network as well, like the size of the hidden layers.].

```{r train-final-model}
model <- build_model()
model %>% fit(train_x, train_y, epochs = 80, batch_size = 16, verbose = 0)
```


## Model interpreation

Here we'll look at two methods for model interpretation: *variable importance* and *individual conditional expectation* (ICE) curves. The methods are available in the R packages [vip](https://koalaverse.github.io/vip/index.html) and [pdp](https://github.com/bgreenwell/pdp), respectively.

While both packages support a wide range of models, it is rather straightforward to use them for any model for which new predictions can be obtained. To start, we'll have to define a prediction function wrapper which requires two arguments: `object` (the fitted model object) and `newdata`. The function needs to return a vector of predictions (one for each observation).

```{r prediction-functions}
pred_wrapper <- function(object, newdata) {
  predict(object, x = as.matrix(newdata)) %>%
    as.vector()
}
```

A simple measure of variable importance can be obtained using the permutation approach described in @random-breiman-2001 for random forests. In essence, we randomly permute the values of each feature and record the drop in training performance. This can be accomplished using the `vip()` function with `method = "permute"`. To use this method we need to supply the original training response values via the `obs` argument and specify which performance metric we are interested in (in this case, we'll use $R^2$). The results are, which are displayed in **Figure 2**, indicate that the average number of rooms per dwelling (`rm`) and the percentage of lower status of the population (`lstat`) are the most important features in predicting median home value.

```{r vip, fig.cap="**Figure 2** Permuation-based VIP for the fitted network."}
set.seed(102)  # for reproducibility
p1 <- vip(
  object = model,                     # fitted model
  method = "permute",                 # permutation-based VI scores
  num_features = ncol(train_x),       # default only plots top 10 features
  feature_names = colnames(train_x),  # feature names in training data
  pred_fun = pred_wrapper,            # user-defined prediction function
  obs = train_y,                      # response values used for training
  metric = "rsquared",                # evaluation metric
  train = as.data.frame(train_x)      # training data
  # progress = "text"                 # request a text-based progress bar
)
print(p1)  # display plot
```

Next, we'll construct ICE curves for the top two features: `rm` and `lstat`. To do this we use `pdp`'s `partial()` function. By default, `partial()` constructs *partial dependence plots* (PDPs); The PDP for a feature of interest can be constructed by averaging together the ICE curves from each observation for that feature. To suppress this averaging and construct ICE curves, set `ice = TRUE` in the call to `partial()`. Since ICE curves require a prediction for each observations, we can use the same wrapper function we defined earlier. The ICE curves for both `rm` and `lstat` in **Figure 3** display a bit of heterogeneity indicating the possible presence of interaction effects. The solid red curve in each plot represents the average of all of the ICE curves (i.e., the PDP for that feature). 

```{r ice-curves, fig.cap="**Figure 3** ICE curves (black lines) and PDPs (red lines) for the predictors `rm` (left) and `lstat` (right)."}
p2 <- partial(model, pred.var = "rm", pred.fun = pred_wrapper, 
              train = as.data.frame(train_x)) %>%
  autoplot(alpha = 0.1)
p3 <- partial(model, pred.var = "lstat", pred.fun = pred_wrapper, 
              train = as.data.frame(train_x)) %>%
  autoplot(alpha = 0.1)
grid.arrange(p2, p3, ncol = 2)  # display plots side by side
```

A couple of additional points are worth noting:

1. The default output from `partial()` is a data frame. You can set `plot = TRUE` to obtain a plot instead, but since these plots can be expensive to compute, it is better to store the results and plot them manually using, for example, `autoplot()` (for `ggplot2`-based plots) or `plotPartial()` (for `lattice`-based plots).

2. Before fitting the network we normalized the data by centering and scaling each feature. In order for these plots to be on the original scale, you would need to unscale the corresponding column(s) in the output by multiplying by the original sample standard deviation and adding back the sample mean of that feature.

3. ICE curves and PDPs can be computationally expensive. Some strategies are discussed in @greenwell-pdp-2017. The `partial()` function has many useful options to help, for example, `progress` and `parallel` (see `?pdp::partial` for details).
To obtain a PDP, we need to supply a prediction function that returns the average prediction across all observations. This can be easily accomplished by adding an extra line to the previously defined wrapper.

```{r partial-fun}
pdp_wrapper <- function(object, newdata) {
  predict(object, x = as.matrix(newdata)) %>%
    as.vector() %>%
    mean()  # aggregate ICE curves
}
```

Next, we'll construct the partial dependence of medium home value (`cmedv`) on the average number of rooms per dwelling (`rm`) and the percentage of lower status of the population (`lstat`). To restrict the predictions to the region of joint values of `rm` and `lstat` observed in the training data (i.e., to avoid extrapolating) we set `chull = TRUE` in the call to `partial()`; this also helps speed up computation time by restricting the grid over which predictions are obtained. The resulting plot displayed in **Figure 4** indicates what we would naturally expect: that census tracts with a higher average number of rooms per dwelling and a lower percentage of lower status of the population tend to have a higher median value.

```{r pdp, fig.cap="**Figure 4** Partial dependence of `cmedv` on `rm` and `lstat`."}
p4 <- partial(model, pred.var = c("rm", "lstat"), chull = TRUE, 
              pred.fun = pdp_wrapper, train = as.data.frame(train_x)) %>%
  autoplot()
print(p4)  # display plot
```

Finally, we can display all the results in a single plot.

```{r all-results}
grid.arrange(p1, p2, p3, p4, ncol = 2)  # display plots in a grid
```

## References
