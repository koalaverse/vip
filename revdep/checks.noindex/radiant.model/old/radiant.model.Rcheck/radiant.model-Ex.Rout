
R version 4.2.3 (2023-03-15) -- "Shortstop Beagle"
Copyright (C) 2023 The R Foundation for Statistical Computing
Platform: aarch64-apple-darwin20 (64-bit)

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type 'license()' or 'licence()' for distribution details.

  Natural language support but running in an English locale

R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R or R packages in publications.

Type 'demo()' for some demos, 'help()' for on-line help, or
'help.start()' for an HTML browser interface to help.
Type 'q()' to quit R.

> pkgname <- "radiant.model"
> source(file.path(R.home("share"), "R", "examples-header.R"))
> options(warn = 1)
> library('radiant.model')
Loading required package: radiant.data
Loading required package: magrittr
Loading required package: ggplot2
Loading required package: lubridate

Attaching package: ‘lubridate’

The following objects are masked from ‘package:base’:

    date, intersect, setdiff, union

Loading required package: tidyr

Attaching package: ‘tidyr’

The following object is masked from ‘package:magrittr’:

    extract

Loading required package: dplyr

Attaching package: ‘dplyr’

The following objects are masked from ‘package:stats’:

    filter, lag

The following objects are masked from ‘package:base’:

    intersect, setdiff, setequal, union


Attaching package: ‘radiant.data’

The following objects are masked from ‘package:lubridate’:

    month, wday

The following object is masked from ‘package:ggplot2’:

    diamonds

The following object is masked from ‘package:magrittr’:

    set_attr

The following object is masked from ‘package:base’:

    date

> 
> base::assign(".oldSearch", base::search(), pos = 'CheckExEnv')
> base::assign(".old_wd", base::getwd(), pos = 'CheckExEnv')
> cleanEx()
> nameEx("auc")
> ### * auc
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: auc
> ### Title: Area Under the RO Curve (AUC)
> ### Aliases: auc
> 
> ### ** Examples
> 
> auc(runif(20000), dvd$buy, "yes")
[1] 0.5024562
> auc(ifelse(dvd$buy == "yes", 1, 0), dvd$buy, "yes")
[1] 1
> 
> 
> 
> cleanEx()
> nameEx("confusion")
> ### * confusion
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: confusion
> ### Title: Confusion matrix
> ### Aliases: confusion
> 
> ### ** Examples
> 
> data.frame(buy = dvd$buy, pred1 = runif(20000), pred2 = ifelse(dvd$buy == "yes", 1, 0)) %>%
+   confusion(c("pred1", "pred2"), "buy") %>%
+   str()
List of 11
 $ dataset    : tibble [2 × 19] (S3: tbl_df/tbl/data.frame)
  ..$ Type     : chr [1:2] "All" "All"
  ..$ Predictor: chr [1:2] "pred1" "pred2"
  ..$ TP       : int [1:2] 2590 5246
  ..$ FP       : int [1:2] 7369 0
  ..$ TN       : int [1:2] 7385 14754
  ..$ FN       : int [1:2] 2656 0
  ..$ total    : int [1:2] 20000 20000
  ..$ TPR      : num [1:2] 0.494 1
  ..$ TNR      : num [1:2] 0.501 1
  ..$ precision: num [1:2] 0.26 1
  ..$ Fscore   : num [1:2] 0.341 1
  ..$ RIG      : num [1:2] -0.749 1
  ..$ accuracy : num [1:2] 0.499 1
  ..$ kappa    : num [1:2] -0.00446 1
  ..$ profit   : num [1:2] -4779 5246
  ..$ index    : num [1:2] -0.911 1
  ..$ ROME     : num [1:2] -0.48 1
  ..$ contact  : num [1:2] 0.498 0.262
  ..$ AUC      : num [1:2] 0.502 1
 $ df_name    : chr "."
 $ data_filter: chr ""
 $ arr        : chr ""
 $ rows       : NULL
 $ train      : chr "All"
 $ pred       : chr [1:2] "pred1" "pred2"
 $ rvar       : chr "buy"
 $ lev        : chr "yes"
 $ cost       : num 1
 $ margin     : num 2
 - attr(*, "class")= chr [1:2] "confusion" "list"
> 
> 
> 
> cleanEx()
> nameEx("crs")
> ### * crs
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: crs
> ### Title: Collaborative Filtering
> ### Aliases: crs
> 
> ### ** Examples
> 
> crs(ratings,
+   id = "Users", prod = "Movies", pred = c("M6", "M7", "M8", "M9", "M10"),
+   rate = "Ratings", data_filter = "training == 1"
+ ) %>% str()
List of 18
 $ recommendations:'data.frame':	5 obs. of  8 variables:
  ..$ Users   : Factor w/ 11 levels "U1","U2","U3",..: 11 11 11 11 11
  ..$ product : Factor w/ 5 levels "M6","M7","M8",..: 1 2 3 4 5
  ..$ rating  : int [1:5] NA NA NA NA NA
  ..$ average : num [1:5] 3.3 2.7 3.5 2.9 4.1
  ..$ cf      : num [1:5] 4.1 2.08 1.7 2.13 2.71
  ..$ ranking : int [1:5] NA NA NA NA NA
  ..$ avg_rank: int [1:5] 3 5 2 4 1
  ..$ cf_rank : int [1:5] 1 4 5 3 2
 $ rcf            :'data.frame':	1 obs. of  6 variables:
  ..$ Users: Factor w/ 11 levels "U1","U2","U3",..: 11
  ..$ M6   : int 1
  ..$ M7   : int 4
  ..$ M8   : int 5
  ..$ M9   : int 3
  ..$ M10  : int 2
 $ cf             :'data.frame':	1 obs. of  6 variables:
  ..$ Users: Factor w/ 11 levels "U1","U2","U3",..: 11
  ..$ M6   : num 4.1
  ..$ M7   : num 2.08
  ..$ M8   : num 1.7
  ..$ M9   : num 2.13
  ..$ M10  : num 2.71
 $ rank           : int [1, 1:5] 1 4 5 3 2
  ..- attr(*, "dimnames")=List of 2
  .. ..$ : chr "11"
  .. ..$ : NULL
 $ ract           :'data.frame':	1 obs. of  6 variables:
  ..$ Users: Factor w/ 11 levels "U1","U2","U3",..: 11
  ..$ M6   : int NA
  ..$ M7   : int NA
  ..$ M8   : int NA
  ..$ M9   : int NA
  ..$ M10  : int NA
 $ act            :'data.frame':	1 obs. of  6 variables:
  ..$ Users: Factor w/ 11 levels "U1","U2","U3",..: 11
  ..$ M6   : int NA
  ..$ M7   : int NA
  ..$ M8   : int NA
  ..$ M9   : int NA
  ..$ M10  : int NA
 $ ravg           :'data.frame':	1 obs. of  5 variables:
  ..$ M6 : int 3
  ..$ M7 : int 5
  ..$ M8 : int 2
  ..$ M9 : int 4
  ..$ M10: int 1
 $ avg            :'data.frame':	1 obs. of  5 variables:
  ..$ M6 : num 3.3
  ..$ M7 : num 2.7
  ..$ M8 : num 3.5
  ..$ M9 : num 2.9
  ..$ M10: num 4.1
 $ evar           : chr [1:9] "M2" "M3" "M4" "M5" ...
 $ df_name        : chr "ratings"
 $ vars           : chr [1:9] "M2" "M3" "M4" "M5" ...
 $ id             : chr "Users"
 $ prod           : chr "Movies"
 $ pred           : chr [1:5] "M6" "M7" "M8" "M9" ...
 $ rate           : chr "Ratings"
 $ data_filter    : chr "training == 1"
 $ arr            : chr ""
 $ rows           : NULL
 - attr(*, "class")= chr [1:2] "crs" "list"
> 
> 
> 
> cleanEx()
> nameEx("crtree")
> ### * crtree
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: crtree
> ### Title: Classification and regression trees based on the rpart package
> ### Aliases: crtree
> 
> ### ** Examples
> 
> crtree(titanic, "survived", c("pclass", "sex"), lev = "Yes") %>% summary()
Classification tree
Data                 : titanic
Response variable    : survived
Level                : Yes in survived
Explanatory variables: pclass, sex 
Complexity parameter : 0.001 
Minimum observations : 2 
Nr obs               : 1,043 

node), split, n, loss, yval, (yprob)
      * denotes terminal node

1) root 1043 425 No (0.40747843 0.59252157)  
  2) sex=female 386  96 Yes (0.75129534 0.24870466)  
    4) pclass=1st,2nd 234  16 Yes (0.93162393 0.06837607) *
    5) pclass=3rd 152  72 No (0.47368421 0.52631579) *
  3) sex=male 657 135 No (0.20547945 0.79452055) *> result <- crtree(titanic, "survived", c("pclass", "sex")) %>% summary()
Classification tree
Data                 : titanic
Response variable    : survived
Level                : Yes in survived
Explanatory variables: pclass, sex 
Complexity parameter : 0.001 
Minimum observations : 2 
Nr obs               : 1,043 

node), split, n, loss, yval, (yprob)
      * denotes terminal node

1) root 1043 425 No (0.40747843 0.59252157)  
  2) sex=female 386  96 Yes (0.75129534 0.24870466)  
    4) pclass=1st,2nd 234  16 Yes (0.93162393 0.06837607) *
    5) pclass=3rd 152  72 No (0.47368421 0.52631579) *
  3) sex=male 657 135 No (0.20547945 0.79452055) *> result <- crtree(diamonds, "price", c("carat", "clarity"), type = "regression") %>% str()
List of 31
 $ model       :List of 18
  ..$ frame              :'data.frame':	41 obs. of  8 variables:
  .. ..$ var       : chr [1:41] "carat" "carat" "carat" "<leaf>" ...
  .. ..$ n         : int [1:41] 3000 1935 1392 976 416 543 385 158 1065 715 ...
  .. ..$ wt        : num [1:41] 3000 1935 1392 976 416 ...
  .. ..$ dev       : num [1:41] 4.70e+10 2.37e+09 3.80e+08 5.37e+07 8.61e+07 ...
  .. ..$ yval      : num [1:41] 3907 1618 1050 779 1687 ...
  .. ..$ complexity: num [1:41] 0.608124 0.03408 0.005122 0.00032 0.000518 ...
  .. ..$ ncompete  : int [1:41] 1 1 1 0 0 1 0 0 1 1 ...
  .. ..$ nsurrogate: int [1:41] 1 1 0 0 0 0 0 0 0 0 ...
  ..$ where              : Named int [1:3000] 4 4 4 4 4 5 8 8 5 14 ...
  .. ..- attr(*, "names")= chr [1:3000] "1" "2" "3" "4" ...
  ..$ call               : language (function (formula, data, weights, subset, na.action = na.rpart, method,      model = FALSE, x = FALSE, y = TRUE,| __truncated__ ...
  ..$ terms              :Classes 'terms', 'formula'  language price ~ carat + clarity
  .. .. ..- attr(*, "variables")= language list(price, carat, clarity)
  .. .. ..- attr(*, "factors")= int [1:3, 1:2] 0 1 0 0 0 1
  .. .. .. ..- attr(*, "dimnames")=List of 2
  .. .. .. .. ..$ : chr [1:3] "price" "carat" "clarity"
  .. .. .. .. ..$ : chr [1:2] "carat" "clarity"
  .. .. ..- attr(*, "term.labels")= chr [1:2] "carat" "clarity"
  .. .. ..- attr(*, "order")= int [1:2] 1 1
  .. .. ..- attr(*, "intercept")= int 1
  .. .. ..- attr(*, "response")= int 1
  .. .. ..- attr(*, ".Environment")=<environment: 0x139a10570> 
  .. .. ..- attr(*, "predvars")= language list(price, carat, clarity)
  .. .. ..- attr(*, "dataClasses")= Named chr [1:3] "numeric" "numeric" "factor"
  .. .. .. ..- attr(*, "names")= chr [1:3] "price" "carat" "clarity"
  ..$ cptable            : num [1:21, 1:5] 0.6081 0.1838 0.0341 0.0299 0.0292 ...
  .. ..- attr(*, "dimnames")=List of 2
  .. .. ..$ : chr [1:21] "1" "2" "3" "4" ...
  .. .. ..$ : chr [1:5] "CP" "nsplit" "rel error" "xerror" ...
  ..$ method             : chr "anova"
  ..$ parms              : NULL
  ..$ control            :List of 9
  .. ..$ minsplit      : num 2
  .. ..$ minbucket     : num 1
  .. ..$ cp            : num 0.001
  .. ..$ maxcompete    : int 4
  .. ..$ maxsurrogate  : int 5
  .. ..$ usesurrogate  : int 2
  .. ..$ surrogatestyle: int 0
  .. ..$ maxdepth      : int 30
  .. ..$ xval          : num 10
  ..$ functions          :List of 2
  .. ..$ summary:function (yval, dev, wt, ylevel, digits)  
  .. ..$ text   :function (yval, dev, wt, ylevel, digits, n, use.n)  
  ..$ numresp            : int 1
  ..$ splits             : num [1:44, 1:5] 3000 3000 0 1935 1935 ...
  .. ..- attr(*, "dimnames")=List of 2
  .. .. ..$ : chr [1:44] "carat" "clarity" "clarity" "carat" ...
  .. .. ..$ : chr [1:5] "count" "ncat" "improve" "index" ...
  ..$ csplit             : int [1:21, 1:8] 1 3 3 3 2 1 1 1 1 1 ...
  ..$ variable.importance: Named num [1:2] 4.10e+10 5.97e+09
  .. ..- attr(*, "names")= chr [1:2] "carat" "clarity"
  ..$ y                  : Named int [1:3000] 580 650 630 706 1080 3082 3328 4229 1895 3546 ...
  .. ..- attr(*, "names")= chr [1:3000] "1" "2" "3" "4" ...
  ..$ ordered            : Named logi [1:2] FALSE FALSE
  .. ..- attr(*, "names")= chr [1:2] "carat" "clarity"
  ..$ residuals          : Named num [1:3000] -199 -129 -149 -73 301 ...
  .. ..- attr(*, "names")= chr [1:3000] "1" "2" "3" "4" ...
  ..$ model              : tibble [3,000 × 3] (S3: tbl_df/tbl/data.frame)
  .. ..$ price  : int [1:3000] 580 650 630 706 1080 3082 3328 4229 1895 3546 ...
  .. ..$ carat  : num [1:3000] 0.32 0.34 0.3 0.35 0.4 0.6 0.88 0.93 0.51 1.01 ...
  .. ..$ clarity: Factor w/ 8 levels "I1","SI2","SI1",..: 5 3 4 6 4 7 3 3 6 2 ...
  .. ..- attr(*, "description")= chr "## Diamond prices\n\nPrices of 3,000 round cut diamonds\n\n### Description\n\nA dataset containing the prices a"| __truncated__
  ..$ var_types          : Named chr [1:3] "integer" "numeric" "factor"
  .. ..- attr(*, "names")= chr [1:3] "price" "carat" "clarity"
  ..- attr(*, "xlevels")=List of 1
  .. ..$ clarity: chr [1:8] "I1" "SI2" "SI1" "VS2" ...
  ..- attr(*, "class")= chr "rpart"
 $ crtree_input:List of 6
  ..$ formula:Class 'formula'  language price ~ .
  .. .. ..- attr(*, ".Environment")=<environment: 0x139a10570> 
  ..$ data   : tibble [3,000 × 3] (S3: tbl_df/tbl/data.frame)
  .. ..$ price  : int [1:3000] 580 650 630 706 1080 3082 3328 4229 1895 3546 ...
  .. ..$ carat  : num [1:3000] 0.32 0.34 0.3 0.35 0.4 0.6 0.88 0.93 0.51 1.01 ...
  .. ..$ clarity: Factor w/ 8 levels "I1","SI2","SI1",..: 5 3 4 6 4 7 3 3 6 2 ...
  .. ..- attr(*, "description")= chr "## Diamond prices\n\nPrices of 3,000 round cut diamonds\n\n### Description\n\nA dataset containing the prices a"| __truncated__
  ..$ method : chr "anova"
  ..$ parms  :List of 1
  .. ..$ split: chr "gini"
  ..$ weights: NULL
  ..$ control:List of 9
  .. ..$ minsplit      : num 2
  .. ..$ minbucket     : num 1
  .. ..$ cp            : num 0.001
  .. ..$ maxcompete    : int 4
  .. ..$ maxsurrogate  : int 5
  .. ..$ usesurrogate  : int 2
  .. ..$ surrogatestyle: int 0
  .. ..$ maxdepth      : int 30
  .. ..$ xval          : num 10
 $ parms       :List of 1
  ..$ split: chr "gini"
 $ control     :List of 9
  ..$ minsplit      : num 2
  ..$ minbucket     : num 1
  ..$ cp            : num 0.001
  ..$ maxcompete    : int 4
  ..$ maxsurrogate  : int 5
  ..$ usesurrogate  : int 2
  ..$ surrogatestyle: int 0
  ..$ maxdepth      : int 30
  ..$ xval          : num 10
 $ form        : chr "price ~ . "
 $ method      : chr "anova"
 $ rv          : int [1:3000] 580 650 630 706 1080 3082 3328 4229 1895 3546 ...
 $ not_vary    : chr(0) 
 $ df_name     : chr "diamonds"
 $ vars        : chr [1:2] "carat" "clarity"
 $ rvar        : chr "price"
 $ evar        : chr [1:2] "carat" "clarity"
 $ type        : chr "regression"
 $ lev         : chr ""
 $ wts         : NULL
 $ minsplit    : num 2
 $ minbucket   : num 1
 $ cp          : num 0.001
 $ pcp         : logi NA
 $ nodes       : logi NA
 $ K           : num 10
 $ seed        : num 1234
 $ split       : chr "gini"
 $ prior       : logi NA
 $ adjprob     : logi TRUE
 $ cost        : logi NA
 $ margin      : logi NA
 $ check       : chr ""
 $ data_filter : chr ""
 $ arr         : chr ""
 $ rows        : NULL
 - attr(*, "class")= chr [1:3] "crtree" "model" "list"
> 
> 
> 
> cleanEx()
> nameEx("cv.crtree")
> ### * cv.crtree
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: cv.crtree
> ### Title: Cross-validation for Classification and Regression Trees
> ### Aliases: cv.crtree
> 
> ### ** Examples
> 
> ## Not run: 
> ##D result <- crtree(dvd, "buy", c("coupon", "purch", "last"))
> ##D cv.crtree(result, cp = 0.0001, pcp = seq(0, 0.01, length.out = 11))
> ##D cv.crtree(result, cp = 0.0001, pcp = c(0, 0.001, 0.002), fun = profit, cost = 1, margin = 5)
> ##D result <- crtree(diamonds, "price", c("carat", "color", "clarity"), type = "regression", cp = 0.001)
> ##D cv.crtree(result, cp = 0.001, pcp = seq(0, 0.01, length.out = 11), fun = MAE)
> ## End(Not run)
> 
> 
> 
> 
> cleanEx()
> nameEx("cv.gbt")
> ### * cv.gbt
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: cv.gbt
> ### Title: Cross-validation for Gradient Boosted Trees
> ### Aliases: cv.gbt
> 
> ### ** Examples
> 
> ## Not run: 
> ##D result <- gbt(dvd, "buy", c("coupon", "purch", "last"))
> ##D cv.gbt(result, params = list(max_depth = 1:6))
> ##D cv.gbt(result, params = list(max_depth = 1:6), fun = "logloss")
> ##D cv.gbt(
> ##D   result,
> ##D   params = list(learning_rate = seq(0.1, 1.0, 0.1)),
> ##D   maximize = TRUE, fun = profit, cost = 1, margin = 5
> ##D )
> ##D result <- gbt(diamonds, "price", c("carat", "color", "clarity"), type = "regression")
> ##D cv.gbt(result, params = list(max_depth = 1:2, min_child_weight = 1:2))
> ##D cv.gbt(result, params = list(learning_rate = seq(0.1, 0.5, 0.1)), fun = Rsq, maximize = TRUE)
> ##D cv.gbt(result, params = list(learning_rate = seq(0.1, 0.5, 0.1)), fun = MAE, maximize = FALSE)
> ##D rig_wrap <- function(preds, dtrain) {
> ##D   labels <- xgboost::getinfo(dtrain, "label")
> ##D   value <- rig(preds, labels, lev = 1)
> ##D   list(metric = "rig", value = value)
> ##D }
> ##D result <- gbt(titanic, "survived", c("pclass", "sex"), eval_metric = rig_wrap, maximize = TRUE)
> ##D cv.gbt(result, params = list(learning_rate = seq(0.1, 0.5, 0.1)))
> ## End(Not run)
> 
> 
> 
> 
> cleanEx()
> nameEx("cv.nn")
> ### * cv.nn
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: cv.nn
> ### Title: Cross-validation for a Neural Network
> ### Aliases: cv.nn
> 
> ### ** Examples
> 
> ## Not run: 
> ##D result <- nn(dvd, "buy", c("coupon", "purch", "last"))
> ##D cv.nn(result, decay = seq(0, 1, .5), size = 1:2)
> ##D cv.nn(result, decay = seq(0, 1, .5), size = 1:2, fun = profit, cost = 1, margin = 5)
> ##D result <- nn(diamonds, "price", c("carat", "color", "clarity"), type = "regression")
> ##D cv.nn(result, decay = seq(0, 1, .5), size = 1:2)
> ##D cv.nn(result, decay = seq(0, 1, .5), size = 1:2, fun = Rsq)
> ## End(Not run)
> 
> 
> 
> 
> cleanEx()
> nameEx("cv.rforest")
> ### * cv.rforest
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: cv.rforest
> ### Title: Cross-validation for a Random Forest
> ### Aliases: cv.rforest
> 
> ### ** Examples
> 
> ## Not run: 
> ##D result <- rforest(dvd, "buy", c("coupon", "purch", "last"))
> ##D cv.rforest(
> ##D   result,
> ##D   mtry = 1:3, min.node.size = seq(1, 10, 5),
> ##D   num.trees = c(100, 200), sample.fraction = 0.632
> ##D )
> ##D result <- rforest(titanic, "survived", c("pclass", "sex"), max.depth = 1)
> ##D cv.rforest(result, mtry = 1:3, min.node.size = seq(1, 10, 5))
> ##D cv.rforest(result, mtry = 1:3, num.trees = c(100, 200), fun = profit, cost = 1, margin = 5)
> ##D result <- rforest(diamonds, "price", c("carat", "color", "clarity"), type = "regression")
> ##D cv.rforest(result, mtry = 1:3, min.node.size = 1)
> ##D cv.rforest(result, mtry = 1:3, min.node.size = 1, fun = Rsq)
> ## End(Not run)
> 
> 
> 
> 
> cleanEx()
> nameEx("dtree")
> ### * dtree
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: dtree
> ### Title: Create a decision tree
> ### Aliases: dtree
> 
> ### ** Examples
> 
> yaml::as.yaml(movie_contract) %>% cat()
name: Sign contract
variables:
  legal fees: 5000
type: decision
Sign with Movie Company:
  cost: legal fees
  type: chance
  Small Box Office:
    p: 0.3
    payoff: 200000
  Medium Box Office:
    p: 0.6
    payoff: 1000000
  Large Box Office:
    p: 0.1
    payoff: 3000000
Sign with TV Network:
  payoff: 900000
> dtree(movie_contract, opt = "max") %>% summary(output = TRUE)
Decision tree input:
name: Sign contract
variables:
    legal fees: 5000
type: decision
Sign with Movie Company:
    cost: legal fees
    type: chance
    Small Box Office:
        p: 0.3
        payoff: 200000
    Medium Box Office:
        p: 0.6
        payoff: 1000000
    Large Box Office:
        p: 0.1
        payoff: 3000000
Sign with TV Network:
    payoff: 900000

Variable input values:
               
legal fees 5000

Initial decision tree:
                             Probability       Payoff     Cost     Type
 Sign contract                                                         
  ¦--Sign with Movie Company                          5,000.00 decision
  ¦   ¦--Small Box Office        30.00 %   200,000.00            chance
  ¦   ¦--Medium Box Office       60.00 % 1,000,000.00            chance
  ¦   °--Large Box Office        10.00 % 3,000,000.00            chance
  °--Sign with TV Network                  900,000.00          decision

Final decision tree:
                             Probability       Payoff     Cost     Type
 Sign contract                             955,000.00                  
  ¦--Sign with Movie Company               955,000.00 5,000.00 decision
  ¦   ¦--Small Box Office        30.00 %   200,000.00            chance
  ¦   ¦--Medium Box Office       60.00 % 1,000,000.00            chance
  ¦   °--Large Box Office        10.00 % 3,000,000.00            chance
  °--Sign with TV Network                  900,000.00          decision
> dtree(movie_contract)$payoff
          Sign contract Sign with Movie Company        Small Box Office 
                 955000                  955000                  200000 
      Medium Box Office        Large Box Office    Sign with TV Network 
                1000000                 3000000                  900000 
> dtree(movie_contract)$prob
          Sign contract Sign with Movie Company        Small Box Office 
                     NA                      NA                     0.3 
      Medium Box Office        Large Box Office    Sign with TV Network 
                    0.6                     0.1                      NA 
> dtree(movie_contract)$solution_df
                        level                   label  payoff prob cost
Sign contract               1           Sign contract  955000   NA   NA
Sign with Movie Company     2 Sign with Movie Company  955000   NA 5000
Small Box Office            3        Small Box Office  200000  0.3   NA
Medium Box Office           3       Medium Box Office 1000000  0.6   NA
Large Box Office            3        Large Box Office 3000000  0.1   NA
Sign with TV Network        2    Sign with TV Network  900000   NA   NA
                            type
Sign contract           decision
Sign with Movie Company   chance
Small Box Office        terminal
Medium Box Office       terminal
Large Box Office        terminal
Sign with TV Network    terminal
> 
> 
> 
> 
> cleanEx()
> nameEx("evalbin")
> ### * evalbin
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: evalbin
> ### Title: Evaluate the performance of different (binary) classification
> ###   models
> ### Aliases: evalbin
> 
> ### ** Examples
> 
> data.frame(buy = dvd$buy, pred1 = runif(20000), pred2 = ifelse(dvd$buy == "yes", 1, 0)) %>%
+   evalbin(c("pred1", "pred2"), "buy") %>%
+   str()
List of 12
 $ dataset    : tibble [12 × 13] (S3: tbl_df/tbl/data.frame)
  ..$ pred         : Factor w/ 2 levels "pred1","pred2": 1 1 1 1 1 1 1 1 1 1 ...
  ..$ bins         : int [1:12] 10 9 8 7 6 5 4 3 2 1 ...
  ..$ nr_obs       : int [1:12] 2000 2000 2000 2000 2000 2000 2000 2000 2000 2000 ...
  ..$ nr_resp      : int [1:12] 513 555 530 502 548 515 485 550 554 494 ...
  ..$ resp_rate    : num [1:12] 0.257 0.278 0.265 0.251 0.274 ...
  ..$ gains        : num [1:12] 0.0978 0.1058 0.101 0.0957 0.1045 ...
  ..$ profit       : num [1:12] -974 -1864 -2804 -3800 -4704 ...
  ..$ ROME         : num [1:12] -0.487 -0.466 -0.467 -0.475 -0.47 ...
  ..$ cum_prop     : num [1:12] 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 ...
  ..$ cum_resp     : int [1:12] 513 1068 1598 2100 2648 3163 3648 4198 4752 5246 ...
  ..$ cum_resp_rate: num [1:12] 0.257 0.267 0.266 0.263 0.265 ...
  ..$ cum_lift     : num [1:12] 0.978 1.018 1.015 1.001 1.01 ...
  ..$ cum_gains    : num [1:12] 0.0978 0.2036 0.3046 0.4003 0.5048 ...
 $ df_name    : chr "."
 $ data_filter: chr ""
 $ arr        : chr ""
 $ rows       : NULL
 $ train      : chr "All"
 $ pred       : chr [1:2] "pred1" "pred2"
 $ rvar       : chr "buy"
 $ lev        : chr "yes"
 $ qnt        : num 10
 $ cost       : num 1
 $ margin     : num 2
 - attr(*, "class")= chr [1:2] "evalbin" "list"
> 
> 
> 
> cleanEx()
> nameEx("evalreg")
> ### * evalreg
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: evalreg
> ### Title: Evaluate the performance of different regression models
> ### Aliases: evalreg
> 
> ### ** Examples
> 
> data.frame(price = diamonds$price, pred1 = rnorm(3000), pred2 = diamonds$price) %>%
+   evalreg(pred = c("pred1", "pred2"), "price") %>%
+   str()
List of 12
 $ rv         : int [1:3000] 580 650 630 706 1080 3082 3328 4229 1895 3546 ...
 $ dat        :'data.frame':	2 obs. of  6 variables:
  ..$ Type     : chr [1:2] "All" "All"
  ..$ Predictor: chr [1:2] "pred1" "pred2"
  ..$ n        : int [1:2] 3000 3000
  ..$ Rsq      : num [1:2] 0.0000698 1
  ..$ RMSE     : num [1:2] 5560 0
  ..$ MAE      : num [1:2] 3907 0
 $ vars       : chr [1:3] "pred1" "pred2" "price"
 $ df_name    : chr "."
 $ dataset    :'data.frame':	3000 obs. of  3 variables:
  ..$ price: int [1:3000] 580 650 630 706 1080 3082 3328 4229 1895 3546 ...
  ..$ pred1: num [1:3000] -0.626 0.184 -0.836 1.595 0.33 ...
  ..$ pred2: int [1:3000] 580 650 630 706 1080 3082 3328 4229 1895 3546 ...
 $ pred       : chr [1:2] "pred1" "pred2"
 $ rvar       : chr "price"
 $ train      : chr "All"
 $ data_filter: chr ""
 $ arr        : chr ""
 $ rows       : NULL
 $ envir      :<environment: 0x12e6b1d90> 
 - attr(*, "class")= chr [1:2] "evalreg" "list"
> 
> 
> 
> 
> cleanEx()
> nameEx("find_max")
> ### * find_max
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: find_max
> ### Title: Find maximum value of a vector
> ### Aliases: find_max
> 
> ### ** Examples
> 
> find_max(1:10, 21:30)
[1] 30
> 
> 
> 
> 
> cleanEx()
> nameEx("find_min")
> ### * find_min
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: find_min
> ### Title: Find minimum value of a vector
> ### Aliases: find_min
> 
> ### ** Examples
> 
> find_min(1:10, 21:30)
[1] 21
> 
> 
> 
> 
> cleanEx()
> nameEx("gbt")
> ### * gbt
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: gbt
> ### Title: Gradient Boosted Trees using XGBoost
> ### Aliases: gbt
> 
> ### ** Examples
> 
> ## Not run: 
> ##D gbt(titanic, "survived", c("pclass", "sex"), lev = "Yes") %>% summary()
> ##D gbt(titanic, "survived", c("pclass", "sex")) %>% str()
> ## End(Not run)
> gbt(titanic, "survived", c("pclass", "sex"), lev = "Yes", early_stopping_rounds = 0) %>% summary()
Gradient Boosted Trees (XGBoost)
Type                 : Classification
Data                 : titanic
Response variable    : survived
Level                : Yes in survived
Explanatory variables: pclass, sex 
Max depth            : 6 
Learning rate (eta)  : 0.3 
Min split loss       : 0 
Min child weight     : 1 
Sub-sample           : 1 
Nr of rounds (trees) : 100 
Early stopping rounds: 0 
Nr obs               : 1,043 

Iteration history:

[1]	train-auc:0.824504 
[2]	train-auc:0.824504 
Stopping. Best iteration:
[1]	train-auc:0.824504
> gbt(titanic, "survived", c("pclass", "sex"), early_stopping_rounds = 0) %>% str()
List of 28
 $ check                : chr ""
 $ model                :List of 15
  ..$ handle         :Class 'xgb.Booster.handle' <externalptr> 
  ..$ raw            : raw [1:6230] 7b 4c 00 00 ...
  ..$ best_iteration : num 1
  ..$ best_ntreelimit: int 1
  ..$ best_score     : num 0.825
  ..$ best_msg       : chr "[1]\ttrain-auc:0.824504"
  ..$ niter          : int 2
  ..$ evaluation_log :Classes ‘data.table’ and 'data.frame':	2 obs. of  2 variables:
  .. ..$ iter     : num [1:2] 1 2
  .. ..$ train_auc: num [1:2] 0.825 0.825
  .. ..- attr(*, ".internal.selfref")=<externalptr> 
  ..$ call           : language xgb.train(params = params, data = dtrain, nrounds = nrounds, watchlist = watchlist,      verbose = verbose, print| __truncated__ ...
  ..$ params         :List of 9
  .. ..$ max_depth          : num 6
  .. ..$ learning_rate      : num 0.3
  .. ..$ min_split_loss     : num 0
  .. ..$ min_child_weight   : num 1
  .. ..$ subsample          : num 1
  .. ..$ nthread            : num 12
  .. ..$ objective          : chr "binary:logistic"
  .. ..$ eval_metric        : chr "auc"
  .. ..$ validate_parameters: logi TRUE
  ..$ callbacks      :List of 3
  .. ..$ cb.print.evaluation:function (env = parent.frame())  
  .. .. ..- attr(*, "call")= language cb.print.evaluation(period = print_every_n)
  .. .. ..- attr(*, "name")= chr "cb.print.evaluation"
  .. ..$ cb.evaluation.log  :function (env = parent.frame(), finalize = FALSE)  
  .. .. ..- attr(*, "call")= language cb.evaluation.log()
  .. .. ..- attr(*, "name")= chr "cb.evaluation.log"
  .. ..$ cb.early.stop      :function (env = parent.frame(), finalize = FALSE)  
  .. .. ..- attr(*, "call")= language cb.early.stop(stopping_rounds = early_stopping_rounds, maximize = maximize,      verbose = verbose)
  .. .. ..- attr(*, "name")= chr "cb.early.stop"
  ..$ feature_names  : chr [1:3] "pclass2nd" "pclass3rd" "sexmale"
  ..$ nfeatures      : int 3
  ..$ importance     :Classes ‘data.table’ and 'data.frame':	3 obs. of  4 variables:
  .. ..$ Feature  : chr [1:3] "sexmale" "pclass3rd" "pclass2nd"
  .. ..$ Gain     : num [1:3] 0.758 0.21 0.032
  .. ..$ Cover    : num [1:3] 0.435 0.435 0.13
  .. ..$ Frequency: num [1:3] 0.25 0.5 0.25
  .. ..- attr(*, ".internal.selfref")=<externalptr> 
  ..$ model          : tibble [1,043 × 3] (S3: tbl_df/tbl/data.frame)
  .. ..$ survived: Factor w/ 2 levels "Yes","No": 1 1 2 2 2 1 1 2 1 2 ...
  .. ..$ pclass  : Factor w/ 3 levels "1st","2nd","3rd": 1 1 1 1 1 1 1 1 1 1 ...
  .. ..$ sex     : Factor w/ 2 levels "female","male": 1 2 1 2 1 2 1 2 1 2 ...
  .. ..- attr(*, "description")= chr "## Titanic\n\nThis dataset describes the survival status of individual passengers on the Titanic. The titanic d"| __truncated__
  ..- attr(*, "class")= chr "xgb.Booster"
 $ output               : chr [1:7] "[1]\ttrain-auc:0.824504 " "Will train until train_auc hasn't improved in 0 rounds." "" "[2]\ttrain-auc:0.824504 " ...
 $ check_args           :function (arg, default, inp = gbt_input)  
 $ extra_args_names     : NULL
 $ extra_args           : list()
 $ gbt_input            :List of 10
  ..$ max_depth            : num 6
  ..$ learning_rate        : num 0.3
  ..$ min_split_loss       : num 0
  ..$ nrounds              : num 100
  ..$ min_child_weight     : num 1
  ..$ subsample            : num 1
  ..$ early_stopping_rounds: num 0
  ..$ nthread              : num 12
  ..$ objective            : chr "binary:logistic"
  ..$ eval_metric          : chr "auc"
 $ not_vary             : chr(0) 
 $ nr_obs               : int 1043
 $ df_name              : chr "titanic"
 $ vars                 : chr [1:2] "pclass" "sex"
 $ rvar                 : chr "survived"
 $ evar                 : chr [1:2] "pclass" "sex"
 $ type                 : chr "classification"
 $ lev                  : chr "Yes"
 $ max_depth            : num 6
 $ learning_rate        : num 0.3
 $ min_split_loss       : num 0
 $ min_child_weight     : num 1
 $ subsample            : num 1
 $ nrounds              : num 100
 $ early_stopping_rounds: num 0
 $ nthread              : num 12
 $ wts                  : NULL
 $ seed                 : chr NA
 $ data_filter          : chr ""
 $ arr                  : chr ""
 $ rows                 : NULL
 - attr(*, "class")= chr [1:3] "gbt" "model" "list"
> gbt(titanic, "survived", c("pclass", "sex"), eval_metric = paste0("error@", 0.5 / 6)) %>% str()
List of 28
 $ check                : chr ""
 $ model                :List of 15
  ..$ handle         :Class 'xgb.Booster.handle' <externalptr> 
  ..$ raw            : raw [1:15518] 7b 4c 00 00 ...
  ..$ best_iteration : num 1
  ..$ best_ntreelimit: int 1
  ..$ best_score     : num 0.593
  ..$ best_msg       : chr "[1]\ttrain-error@0.0833333:0.592522"
  ..$ niter          : int 11
  ..$ evaluation_log :Classes ‘data.table’ and 'data.frame':	11 obs. of  2 variables:
  .. ..$ iter                 : num [1:11] 1 2 3 4 5 6 7 8 9 10 ...
  .. ..$ train_error@0.0833333: num [1:11] 0.593 0.593 0.593 0.593 0.593 ...
  .. ..- attr(*, ".internal.selfref")=<externalptr> 
  ..$ call           : language xgb.train(params = params, data = dtrain, nrounds = nrounds, watchlist = watchlist,      verbose = verbose, print| __truncated__ ...
  ..$ params         :List of 9
  .. ..$ max_depth          : num 6
  .. ..$ learning_rate      : num 0.3
  .. ..$ min_split_loss     : num 0
  .. ..$ min_child_weight   : num 1
  .. ..$ subsample          : num 1
  .. ..$ nthread            : num 12
  .. ..$ objective          : chr "binary:logistic"
  .. ..$ eval_metric        : chr "error@0.0833333333333333"
  .. ..$ validate_parameters: logi TRUE
  ..$ callbacks      :List of 3
  .. ..$ cb.print.evaluation:function (env = parent.frame())  
  .. .. ..- attr(*, "call")= language cb.print.evaluation(period = print_every_n)
  .. .. ..- attr(*, "name")= chr "cb.print.evaluation"
  .. ..$ cb.evaluation.log  :function (env = parent.frame(), finalize = FALSE)  
  .. .. ..- attr(*, "call")= language cb.evaluation.log()
  .. .. ..- attr(*, "name")= chr "cb.evaluation.log"
  .. ..$ cb.early.stop      :function (env = parent.frame(), finalize = FALSE)  
  .. .. ..- attr(*, "call")= language cb.early.stop(stopping_rounds = early_stopping_rounds, maximize = maximize,      verbose = verbose)
  .. .. ..- attr(*, "name")= chr "cb.early.stop"
  ..$ feature_names  : chr [1:3] "pclass2nd" "pclass3rd" "sexmale"
  ..$ nfeatures      : int 3
  ..$ importance     :Classes ‘data.table’ and 'data.frame':	3 obs. of  4 variables:
  .. ..$ Feature  : chr [1:3] "sexmale" "pclass3rd" "pclass2nd"
  .. ..$ Gain     : num [1:3] 0.7291 0.2356 0.0353
  .. ..$ Cover    : num [1:3] 0.393 0.364 0.243
  .. ..$ Frequency: num [1:3] 0.208 0.415 0.377
  .. ..- attr(*, ".internal.selfref")=<externalptr> 
  ..$ model          : tibble [1,043 × 3] (S3: tbl_df/tbl/data.frame)
  .. ..$ survived: Factor w/ 2 levels "Yes","No": 1 1 2 2 2 1 1 2 1 2 ...
  .. ..$ pclass  : Factor w/ 3 levels "1st","2nd","3rd": 1 1 1 1 1 1 1 1 1 1 ...
  .. ..$ sex     : Factor w/ 2 levels "female","male": 1 2 1 2 1 2 1 2 1 2 ...
  .. ..- attr(*, "description")= chr "## Titanic\n\nThis dataset describes the survival status of individual passengers on the Titanic. The titanic d"| __truncated__
  ..- attr(*, "class")= chr "xgb.Booster"
 $ output               : chr [1:16] "[1]\ttrain-error@0.0833333:0.592522 " "Will train until train_error@0.0833333 hasn't improved in 10 rounds." "" "[2]\ttrain-error@0.0833333:0.592522 " ...
 $ check_args           :function (arg, default, inp = gbt_input)  
 $ extra_args_names     : chr "eval_metric"
 $ extra_args           :List of 1
  ..$ eval_metric: chr "error@0.0833333333333333"
 $ gbt_input            :List of 10
  ..$ max_depth            : num 6
  ..$ learning_rate        : num 0.3
  ..$ min_split_loss       : num 0
  ..$ nrounds              : num 100
  ..$ min_child_weight     : num 1
  ..$ subsample            : num 1
  ..$ early_stopping_rounds: num 10
  ..$ nthread              : num 12
  ..$ objective            : chr "binary:logistic"
  ..$ eval_metric          : chr "error@0.0833333333333333"
 $ not_vary             : chr(0) 
 $ nr_obs               : int 1043
 $ df_name              : chr "titanic"
 $ vars                 : chr [1:2] "pclass" "sex"
 $ rvar                 : chr "survived"
 $ evar                 : chr [1:2] "pclass" "sex"
 $ type                 : chr "classification"
 $ lev                  : chr "Yes"
 $ max_depth            : num 6
 $ learning_rate        : num 0.3
 $ min_split_loss       : num 0
 $ min_child_weight     : num 1
 $ subsample            : num 1
 $ nrounds              : num 100
 $ early_stopping_rounds: num 10
 $ nthread              : num 12
 $ wts                  : NULL
 $ seed                 : chr NA
 $ data_filter          : chr ""
 $ arr                  : chr ""
 $ rows                 : NULL
 - attr(*, "class")= chr [1:3] "gbt" "model" "list"
> gbt(diamonds, "price", c("carat", "clarity"), type = "regression") %>% summary()
Gradient Boosted Trees (XGBoost)
Type                 : Regression
Data                 : diamonds
Response variable    : price
Explanatory variables: carat, clarity 
Max depth            : 6 
Learning rate (eta)  : 0.3 
Min split loss       : 0 
Min child weight     : 1 
Sub-sample           : 1 
Nr of rounds (trees) : 100 
Early stopping rounds: 10 
Nr obs               : 3,000 

Iteration history:

[1]	train-rmse:4004.124564 
[2]	train-rmse:2934.489873 
[3]	train-rmse:2209.099322 
[4]	train-rmse:1731.832307 
[5]	train-rmse:1429.542216 
[6]	train-rmse:1236.558689 
[7]	train-rmse:1112.986228 
[8]	train-rmse:1041.411201 
[9]	train-rmse:994.494654 
[10]	train-rmse:968.975971 
...
[91]	train-rmse:747.640950 
[92]	train-rmse:747.167040 
[93]	train-rmse:745.880577 
[94]	train-rmse:744.933682 
[95]	train-rmse:744.060439 
[96]	train-rmse:743.978694 
[97]	train-rmse:743.872742 
[98]	train-rmse:743.621308 
[99]	train-rmse:743.458939 
[100]	train-rmse:743.281788 > rig_wrap <- function(preds, dtrain) {
+   labels <- xgboost::getinfo(dtrain, "label")
+   value <- rig(preds, labels, lev = 1)
+   list(metric = "rig", value = value)
+ }
> gbt(titanic, "survived", c("pclass", "sex"), eval_metric = rig_wrap, maximize = TRUE) %>% str()
List of 28
 $ check                : chr ""
 $ model                :List of 15
  ..$ handle         :Class 'xgb.Booster.handle' <externalptr> 
  ..$ raw            : raw [1:16496] 7b 4c 00 00 ...
  ..$ best_iteration : num 2
  ..$ best_ntreelimit: int 2
  ..$ best_score     : num -3.82
  ..$ best_msg       : chr "[2]\ttrain-rig:-3.818049"
  ..$ niter          : int 12
  ..$ evaluation_log :Classes ‘data.table’ and 'data.frame':	12 obs. of  2 variables:
  .. ..$ iter     : num [1:12] 1 2 3 4 5 6 7 8 9 10 ...
  .. ..$ train_rig: num [1:12] -3.96 -3.82 -4.1 -4.1 -4.1 ...
  .. ..- attr(*, ".internal.selfref")=<externalptr> 
  ..$ call           : language xgb.train(params = params, data = dtrain, nrounds = nrounds, watchlist = watchlist,      verbose = verbose, print| __truncated__ ...
  ..$ params         :List of 8
  .. ..$ max_depth          : num 6
  .. ..$ learning_rate      : num 0.3
  .. ..$ min_split_loss     : num 0
  .. ..$ min_child_weight   : num 1
  .. ..$ subsample          : num 1
  .. ..$ nthread            : num 12
  .. ..$ objective          : chr "binary:logistic"
  .. ..$ validate_parameters: logi TRUE
  ..$ callbacks      :List of 3
  .. ..$ cb.print.evaluation:function (env = parent.frame())  
  .. .. ..- attr(*, "call")= language cb.print.evaluation(period = print_every_n)
  .. .. ..- attr(*, "name")= chr "cb.print.evaluation"
  .. ..$ cb.evaluation.log  :function (env = parent.frame(), finalize = FALSE)  
  .. .. ..- attr(*, "call")= language cb.evaluation.log()
  .. .. ..- attr(*, "name")= chr "cb.evaluation.log"
  .. ..$ cb.early.stop      :function (env = parent.frame(), finalize = FALSE)  
  .. .. ..- attr(*, "call")= language cb.early.stop(stopping_rounds = early_stopping_rounds, maximize = maximize,      verbose = verbose)
  .. .. ..- attr(*, "name")= chr "cb.early.stop"
  ..$ feature_names  : chr [1:3] "pclass2nd" "pclass3rd" "sexmale"
  ..$ nfeatures      : int 3
  ..$ importance     :Classes ‘data.table’ and 'data.frame':	3 obs. of  4 variables:
  .. ..$ Feature  : chr [1:3] "sexmale" "pclass3rd" "pclass2nd"
  .. ..$ Gain     : num [1:3] 0.7287 0.2359 0.0355
  .. ..$ Cover    : num [1:3] 0.392 0.361 0.246
  .. ..$ Frequency: num [1:3] 0.207 0.414 0.379
  .. ..- attr(*, ".internal.selfref")=<externalptr> 
  ..$ model          : tibble [1,043 × 3] (S3: tbl_df/tbl/data.frame)
  .. ..$ survived: Factor w/ 2 levels "Yes","No": 1 1 2 2 2 1 1 2 1 2 ...
  .. ..$ pclass  : Factor w/ 3 levels "1st","2nd","3rd": 1 1 1 1 1 1 1 1 1 1 ...
  .. ..$ sex     : Factor w/ 2 levels "female","male": 1 2 1 2 1 2 1 2 1 2 ...
  .. ..- attr(*, "description")= chr "## Titanic\n\nThis dataset describes the survival status of individual passengers on the Titanic. The titanic d"| __truncated__
  ..- attr(*, "class")= chr "xgb.Booster"
 $ output               : chr [1:17] "[1]\ttrain-rig:-3.957425 " "Will train until train_rig hasn't improved in 10 rounds." "" "[2]\ttrain-rig:-3.818049 " ...
 $ check_args           :function (arg, default, inp = gbt_input)  
 $ extra_args_names     : chr [1:2] "eval_metric" "maximize"
 $ extra_args           :List of 2
  ..$ eval_metric:function (preds, dtrain)  
  ..$ maximize   : logi TRUE
 $ gbt_input            :List of 11
  ..$ max_depth            : num 6
  ..$ learning_rate        : num 0.3
  ..$ min_split_loss       : num 0
  ..$ nrounds              : num 100
  ..$ min_child_weight     : num 1
  ..$ subsample            : num 1
  ..$ early_stopping_rounds: num 10
  ..$ nthread              : num 12
  ..$ objective            : chr "binary:logistic"
  ..$ eval_metric          :function (preds, dtrain)  
  ..$ maximize             : logi TRUE
 $ not_vary             : chr(0) 
 $ nr_obs               : int 1043
 $ df_name              : chr "titanic"
 $ vars                 : chr [1:2] "pclass" "sex"
 $ rvar                 : chr "survived"
 $ evar                 : chr [1:2] "pclass" "sex"
 $ type                 : chr "classification"
 $ lev                  : chr "Yes"
 $ max_depth            : num 6
 $ learning_rate        : num 0.3
 $ min_split_loss       : num 0
 $ min_child_weight     : num 1
 $ subsample            : num 1
 $ nrounds              : num 100
 $ early_stopping_rounds: num 10
 $ nthread              : num 12
 $ wts                  : NULL
 $ seed                 : chr NA
 $ data_filter          : chr ""
 $ arr                  : chr ""
 $ rows                 : NULL
 - attr(*, "class")= chr [1:3] "gbt" "model" "list"
> 
> 
> 
> cleanEx()
> nameEx("logistic")
> ### * logistic
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: logistic
> ### Title: Logistic regression
> ### Aliases: logistic
> 
> ### ** Examples
> 
> logistic(titanic, "survived", c("pclass", "sex"), lev = "Yes") %>% summary()
Logistic regression (GLM)
Data                 : titanic
Response variable    : survived
Level                : Yes in survived
Explanatory variables: pclass, sex 
Null hyp.: there is no effect of x on survived
Alt. hyp.: there is an effect of x on survived

                OR    OR% coefficient std.error z.value p.value    
 (Intercept)                    2.151     0.188  11.420  < .001 ***
 pclass|2nd  0.409 -59.1%      -0.893     0.208  -4.290  < .001 ***
 pclass|3rd  0.181 -81.9%      -1.712     0.191  -8.953  < .001 ***
 sex|male    0.080 -92.0%      -2.522     0.163 -15.447  < .001 ***

Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Pseudo R-squared:0.281, Adjusted Pseudo R-squared:0.277
AUC: 0.821, Log-likelihood: -506.562, AIC: 1021.124, BIC: 1040.924
Chi-squared: 396.861 df(3), p.value < .001 
Nr obs: 1,043 

> logistic(titanic, "survived", c("pclass", "sex")) %>% str()
List of 19
 $ type       : chr "classification"
 $ coeff      :'data.frame':	4 obs. of  8 variables:
  ..$ label      : chr [1:4] "(Intercept)" "pclass|2nd" "pclass|3rd" "sex|male"
  ..$ OR         : num [1:4] 8.5966 0.4093 0.1806 0.0803
  ..$ OR%        : num [1:4] 7.597 -0.591 -0.819 -0.92
  ..$ coefficient: num [1:4] 2.151 -0.893 -1.712 -2.522
  ..$ std.error  : num [1:4] 0.188 0.208 0.191 0.163
  ..$ z.value    : num [1:4] 11.42 -4.29 -8.95 -15.45
  ..$ p.value    : num [1:4] 0.0000000000000000000000000000033121349629888584156697274 0.00001789369574551756382290411406810193284400156699121| __truncated__
  ..$ sig_star   : chr [1:4] "***" "***" "***" "***"
 $ model      :List of 30
  ..$ coefficients     : Named num [1:4] 2.151 -0.893 -1.712 -2.522
  .. ..- attr(*, "names")= chr [1:4] "(Intercept)" "pclass2nd" "pclass3rd" "sexmale"
  ..$ residuals        : Named num [1:1043] 1.12 2.45 -9.6 -1.69 -9.6 ...
  .. ..- attr(*, "names")= chr [1:1043] "1" "2" "3" "4" ...
  ..$ fitted.values    : Named num [1:1043] 0.896 0.408 0.896 0.408 0.896 ...
  .. ..- attr(*, "names")= chr [1:1043] "1" "2" "3" "4" ...
  ..$ effects          : Named num [1:1043] 4.269 0.522 -5.63 15.447 -2.89 ...
  .. ..- attr(*, "names")= chr [1:1043] "(Intercept)" "pclass2nd" "pclass3rd" "sexmale" ...
  ..$ R                : num [1:4, 1:4] -12.8 0 0 0 -3.5 ...
  .. ..- attr(*, "dimnames")=List of 2
  .. .. ..$ : chr [1:4] "(Intercept)" "pclass2nd" "pclass3rd" "sexmale"
  .. .. ..$ : chr [1:4] "(Intercept)" "pclass2nd" "pclass3rd" "sexmale"
  ..$ rank             : int 4
  ..$ qr               :List of 5
  .. ..$ qr   : num [1:1043, 1:4] -12.81 0.0384 0.0239 0.0384 0.0239 ...
  .. .. ..- attr(*, "dimnames")=List of 2
  .. .. .. ..$ : chr [1:1043] "1" "2" "3" "4" ...
  .. .. .. ..$ : chr [1:4] "(Intercept)" "pclass2nd" "pclass3rd" "sexmale"
  .. ..$ rank : int 4
  .. ..$ qraux: num [1:4] 1.02 1.02 1.03 1.02
  .. ..$ pivot: int [1:4] 1 2 3 4
  .. ..$ tol  : num 0.00000000001
  .. ..- attr(*, "class")= chr "qr"
  ..$ family           :List of 12
  .. ..$ family    : chr "binomial"
  .. ..$ link      : chr "logit"
  .. ..$ linkfun   :function (mu)  
  .. ..$ linkinv   :function (eta)  
  .. ..$ variance  :function (mu)  
  .. ..$ dev.resids:function (y, mu, wt)  
  .. ..$ aic       :function (y, n, mu, wt, dev)  
  .. ..$ mu.eta    :function (eta)  
  .. ..$ initialize: language {     if (NCOL(y) == 1) { ...
  .. ..$ validmu   :function (mu)  
  .. ..$ valideta  :function (eta)  
  .. ..$ simulate  :function (object, nsim)  
  .. ..- attr(*, "class")= chr "family"
  ..$ linear.predictors: Named num [1:1043] 2.151 -0.371 2.151 -0.371 2.151 ...
  .. ..- attr(*, "names")= chr [1:1043] "1" "2" "3" "4" ...
  ..$ deviance         : num 1013
  ..$ aic              : num 1021
  ..$ null.deviance    : num 1410
  ..$ iter             : int 4
  ..$ weights          : Named num [1:1043] 0.0933 0.2416 0.0933 0.2416 0.0933 ...
  .. ..- attr(*, "names")= chr [1:1043] "1" "2" "3" "4" ...
  ..$ prior.weights    : Named num [1:1043] 1 1 1 1 1 1 1 1 1 1 ...
  .. ..- attr(*, "names")= chr [1:1043] "1" "2" "3" "4" ...
  ..$ df.residual      : int 1039
  ..$ df.null          : int 1042
  ..$ y                : Named num [1:1043] 1 1 0 0 0 1 1 0 1 0 ...
  .. ..- attr(*, "names")= chr [1:1043] "1" "2" "3" "4" ...
  ..$ converged        : logi TRUE
  ..$ boundary         : logi FALSE
  ..$ model            :'data.frame':	1043 obs. of  3 variables:
  .. ..$ survived: logi [1:1043] TRUE TRUE FALSE FALSE FALSE TRUE ...
  .. ..$ pclass  : Factor w/ 3 levels "1st","2nd","3rd": 1 1 1 1 1 1 1 1 1 1 ...
  .. ..$ sex     : Factor w/ 2 levels "female","male": 1 2 1 2 1 2 1 2 1 2 ...
  .. ..- attr(*, "terms")=Classes 'terms', 'formula'  language survived ~ pclass + sex
  .. .. .. ..- attr(*, "variables")= language list(survived, pclass, sex)
  .. .. .. ..- attr(*, "factors")= int [1:3, 1:2] 0 1 0 0 0 1
  .. .. .. .. ..- attr(*, "dimnames")=List of 2
  .. .. .. .. .. ..$ : chr [1:3] "survived" "pclass" "sex"
  .. .. .. .. .. ..$ : chr [1:2] "pclass" "sex"
  .. .. .. ..- attr(*, "term.labels")= chr [1:2] "pclass" "sex"
  .. .. .. ..- attr(*, "order")= int [1:2] 1 1
  .. .. .. ..- attr(*, "intercept")= int 1
  .. .. .. ..- attr(*, "response")= int 1
  .. .. .. ..- attr(*, ".Environment")=<environment: 0x12dcbfa58> 
  .. .. .. ..- attr(*, "predvars")= language list(survived, pclass, sex)
  .. .. .. ..- attr(*, "dataClasses")= Named chr [1:3] "logical" "factor" "factor"
  .. .. .. .. ..- attr(*, "names")= chr [1:3] "survived" "pclass" "sex"
  ..$ call             : language glm(formula = form_upper, family = binomial(link = "logit"), data = dataset,      weights = wts)
  ..$ formula          :Class 'formula'  language survived ~ pclass + sex
  .. .. ..- attr(*, ".Environment")=<environment: 0x12dcbfa58> 
  ..$ terms            :Classes 'terms', 'formula'  language survived ~ pclass + sex
  .. .. ..- attr(*, "variables")= language list(survived, pclass, sex)
  .. .. ..- attr(*, "factors")= int [1:3, 1:2] 0 1 0 0 0 1
  .. .. .. ..- attr(*, "dimnames")=List of 2
  .. .. .. .. ..$ : chr [1:3] "survived" "pclass" "sex"
  .. .. .. .. ..$ : chr [1:2] "pclass" "sex"
  .. .. ..- attr(*, "term.labels")= chr [1:2] "pclass" "sex"
  .. .. ..- attr(*, "order")= int [1:2] 1 1
  .. .. ..- attr(*, "intercept")= int 1
  .. .. ..- attr(*, "response")= int 1
  .. .. ..- attr(*, ".Environment")=<environment: 0x12dcbfa58> 
  .. .. ..- attr(*, "predvars")= language list(survived, pclass, sex)
  .. .. ..- attr(*, "dataClasses")= Named chr [1:3] "logical" "factor" "factor"
  .. .. .. ..- attr(*, "names")= chr [1:3] "survived" "pclass" "sex"
  ..$ data             : tibble [1,043 × 3] (S3: tbl_df/tbl/data.frame)
  .. ..$ survived: logi [1:1043] TRUE TRUE FALSE FALSE FALSE TRUE ...
  .. ..$ pclass  : Factor w/ 3 levels "1st","2nd","3rd": 1 1 1 1 1 1 1 1 1 1 ...
  .. ..$ sex     : Factor w/ 2 levels "female","male": 1 2 1 2 1 2 1 2 1 2 ...
  .. ..- attr(*, "description")= chr "## Titanic\n\nThis dataset describes the survival status of individual passengers on the Titanic. The titanic d"| __truncated__
  ..$ offset           : NULL
  ..$ control          :List of 3
  .. ..$ epsilon: num 0.00000001
  .. ..$ maxit  : num 25
  .. ..$ trace  : logi FALSE
  ..$ method           : chr "glm.fit"
  ..$ contrasts        :List of 2
  .. ..$ pclass: chr "contr.treatment"
  .. ..$ sex   : chr "contr.treatment"
  ..$ xlevels          :List of 2
  .. ..$ pclass: chr [1:3] "1st" "2nd" "3rd"
  .. ..$ sex   : chr [1:2] "female" "male"
  ..- attr(*, "class")= chr [1:2] "glm" "lm"
 $ mmx        : tibble [1,043 × 3] (S3: tbl_df/tbl/data.frame)
  ..$ survived: logi [1:1043] TRUE TRUE FALSE FALSE FALSE TRUE ...
  ..$ pclass  : Factor w/ 3 levels "1st","2nd","3rd": 1 1 1 1 1 1 1 1 1 1 ...
  ..$ sex     : Factor w/ 2 levels "female","male": 1 2 1 2 1 2 1 2 1 2 ...
  ..- attr(*, "description")= chr "## Titanic\n\nThis dataset describes the survival status of individual passengers on the Titanic. The titanic d"| __truncated__
 $ rv         : Factor w/ 2 levels "Yes","No": 1 1 2 2 2 1 1 2 1 2 ...
 $ not_vary   : chr(0) 
 $ df_name    : chr "titanic"
 $ vars       : chr [1:2] "pclass" "sex"
 $ rvar       : chr "survived"
 $ evar       : chr [1:2] "pclass" "sex"
 $ lev        : chr "Yes"
 $ int        : chr ""
 $ wts        : NULL
 $ check      : chr ""
 $ form       : symbol 
 $ ci_type    : chr "profile"
 $ data_filter: chr ""
 $ arr        : chr ""
 $ rows       : NULL
 - attr(*, "class")= chr [1:3] "logistic" "model" "list"
> 
> 
> 
> cleanEx()
> nameEx("mnl")
> ### * mnl
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: mnl
> ### Title: Multinomial logistic regression
> ### Aliases: mnl
> 
> ### ** Examples
> 
> result <- mnl(
+   ketchup,
+   rvar = "choice",
+   evar = c("price.heinz28", "price.heinz32", "price.heinz41", "price.hunts32"),
+   lev = "heinz28"
+ )
> str(result)
List of 18
 $ umod       :List of 26
  ..$ n            : num [1:3] 1 0 4
  ..$ nunits       : int 6
  ..$ nconn        : num [1:7] 0 0 0 2 4 6 8
  ..$ conn         : num [1:8] 0 1 0 1 0 1 0 1
  ..$ nsunits      : num 2
  ..$ decay        : num 0
  ..$ entropy      : logi FALSE
  ..$ softmax      : logi TRUE
  ..$ censored     : logi FALSE
  ..$ value        : num 3139
  ..$ wts          : num [1:8] 0 0 0 0.538 0 ...
  ..$ convergence  : int 0
  ..$ fitted.values: num [1:2798, 1:4] 0.304 0.304 0.304 0.304 0.304 ...
  .. ..- attr(*, "dimnames")=List of 2
  .. .. ..$ : chr [1:2798] "1" "2" "3" "4" ...
  .. .. ..$ : chr [1:4] "heinz28" "heinz32" "heinz41" "hunts32"
  ..$ residuals    : num [1:2798, 1:4] 0.696 0.696 0.696 0.696 0.696 ...
  .. ..- attr(*, "dimnames")=List of 2
  .. .. ..$ : chr [1:2798] "1" "2" "3" "4" ...
  .. .. ..$ : chr [1:4] "heinz28" "heinz32" "heinz41" "hunts32"
  ..$ lev          : chr [1:4] "heinz28" "heinz32" "heinz41" "hunts32"
  ..$ call         : language (function (formula, data, weights, subset, na.action, contrasts = NULL,      Hess = FALSE, summ = 0, censored = F| __truncated__ ...
  ..$ terms        :Classes 'terms', 'formula'  language choice ~ 1
  .. .. ..- attr(*, "variables")= language list(choice)
  .. .. ..- attr(*, "factors")= int(0) 
  .. .. ..- attr(*, "term.labels")= chr(0) 
  .. .. ..- attr(*, "order")= int(0) 
  .. .. ..- attr(*, "intercept")= int 1
  .. .. ..- attr(*, "response")= int 1
  .. .. ..- attr(*, ".Environment")=<environment: 0x1382f4eb8> 
  .. .. ..- attr(*, "predvars")= language list(choice)
  .. .. ..- attr(*, "dataClasses")= Named chr "factor"
  .. .. .. ..- attr(*, "names")= chr "choice"
  ..$ weights      : num [1:2798, 1] 1 1 1 1 1 1 1 1 1 1 ...
  .. ..- attr(*, "dimnames")=List of 2
  .. .. ..$ : chr [1:2798] "1" "2" "3" "4" ...
  .. .. ..$ : NULL
  ..$ deviance     : num 6278
  ..$ rank         : int 1
  ..$ lab          : chr [1:4] "heinz28" "heinz32" "heinz41" "hunts32"
  ..$ coefnames    : chr "(Intercept)"
  ..$ vcoefnames   : chr "(Intercept)"
  ..$ edf          : num 3
  ..$ AIC          : num 6284
  ..$ model        :'data.frame':	2798 obs. of  1 variable:
  .. ..$ choice: Factor w/ 4 levels "heinz28","heinz32",..: 1 1 1 1 1 1 1 3 1 1 ...
  .. ..- attr(*, "terms")=Classes 'terms', 'formula'  language choice ~ 1
  .. .. .. ..- attr(*, "variables")= language list(choice)
  .. .. .. ..- attr(*, "factors")= int(0) 
  .. .. .. ..- attr(*, "term.labels")= chr(0) 
  .. .. .. ..- attr(*, "order")= int(0) 
  .. .. .. ..- attr(*, "intercept")= int 1
  .. .. .. ..- attr(*, "response")= int 1
  .. .. .. ..- attr(*, ".Environment")=<environment: 0x1382f4eb8> 
  .. .. .. ..- attr(*, "predvars")= language list(choice)
  .. .. .. ..- attr(*, "dataClasses")= Named chr "factor"
  .. .. .. .. ..- attr(*, "names")= chr "choice"
  ..- attr(*, "class")= chr [1:2] "multinom" "nnet"
 $ coeff      :'data.frame':	15 obs. of  8 variables:
  ..$ level      : chr [1:15] "heinz32" "heinz32" "heinz32" "heinz32" ...
  ..$ label      : chr [1:15] "(Intercept)" "price.heinz28" "price.heinz32" "price.heinz41" ...
  ..$ RRR        : num [1:15] 118.647 3 0.101 0.686 1.055 ...
  ..$ coefficient: num [1:15] 4.7762 1.0987 -2.2963 -0.377 0.0532 ...
  ..$ std.error  : num [1:15] 0.7403 0.0761 0.1348 0.1185 0.1031 ...
  ..$ z.value    : num [1:15] 6.452 14.439 -17.033 -3.182 0.516 ...
  ..$ p.value    : num [1:15] 0.0000000001106116796149442650545317328934747606178357060002781508956 0.00000000000000000000000000000000000000000| __truncated__ ...
  ..$ sig_star   : chr [1:15] "***" "***" "***" "** " ...
 $ model      :List of 30
  ..$ n            : num [1:3] 5 0 4
  ..$ nunits       : int 10
  ..$ nconn        : num [1:11] 0 0 0 0 0 0 0 6 12 18 ...
  ..$ conn         : num [1:24] 0 1 2 3 4 5 0 1 2 3 ...
  ..$ nsunits      : num 6
  ..$ decay        : num 0
  ..$ entropy      : logi FALSE
  ..$ softmax      : logi TRUE
  ..$ censored     : logi FALSE
  ..$ value        : num 2511
  ..$ wts          : num [1:24] 0 0 0 0 0 ...
  ..$ convergence  : int 0
  ..$ fitted.values: num [1:2798, 1:4] 0.2725 0.5914 0.0675 0.2725 0.1817 ...
  .. ..- attr(*, "dimnames")=List of 2
  .. .. ..$ : chr [1:2798] "1" "2" "3" "4" ...
  .. .. ..$ : chr [1:4] "heinz28" "heinz32" "heinz41" "hunts32"
  ..$ residuals    : num [1:2798, 1:4] 0.728 0.409 0.933 0.728 0.818 ...
  .. ..- attr(*, "dimnames")=List of 2
  .. .. ..$ : chr [1:2798] "1" "2" "3" "4" ...
  .. .. ..$ : chr [1:4] "heinz28" "heinz32" "heinz41" "hunts32"
  ..$ lev          : chr [1:4] "heinz28" "heinz32" "heinz41" "hunts32"
  ..$ call         : language (function (formula, data, weights, subset, na.action, contrasts = NULL,      Hess = FALSE, summ = 0, censored = F| __truncated__ ...
  ..$ terms        :Classes 'terms', 'formula'  language choice ~ price.heinz28 + price.heinz32 + price.heinz41 + price.hunts32
  .. .. ..- attr(*, "variables")= language list(choice, price.heinz28, price.heinz32, price.heinz41, price.hunts32)
  .. .. ..- attr(*, "factors")= int [1:5, 1:4] 0 1 0 0 0 0 0 1 0 0 ...
  .. .. .. ..- attr(*, "dimnames")=List of 2
  .. .. .. .. ..$ : chr [1:5] "choice" "price.heinz28" "price.heinz32" "price.heinz41" ...
  .. .. .. .. ..$ : chr [1:4] "price.heinz28" "price.heinz32" "price.heinz41" "price.hunts32"
  .. .. ..- attr(*, "term.labels")= chr [1:4] "price.heinz28" "price.heinz32" "price.heinz41" "price.hunts32"
  .. .. ..- attr(*, "order")= int [1:4] 1 1 1 1
  .. .. ..- attr(*, "intercept")= int 1
  .. .. ..- attr(*, "response")= int 1
  .. .. ..- attr(*, ".Environment")=<environment: 0x1382f4eb8> 
  .. .. ..- attr(*, "predvars")= language list(choice, price.heinz28, price.heinz32, price.heinz41, price.hunts32)
  .. .. ..- attr(*, "dataClasses")= Named chr [1:5] "factor" "numeric" "numeric" "numeric" ...
  .. .. .. ..- attr(*, "names")= chr [1:5] "choice" "price.heinz28" "price.heinz32" "price.heinz41" ...
  ..$ weights      : num [1:2798, 1] 1 1 1 1 1 1 1 1 1 1 ...
  .. ..- attr(*, "dimnames")=List of 2
  .. .. ..$ : chr [1:2798] "1" "2" "3" "4" ...
  .. .. ..$ : NULL
  ..$ deviance     : num 5022
  ..$ rank         : int 5
  ..$ lab          : chr [1:4] "heinz28" "heinz32" "heinz41" "hunts32"
  ..$ coefnames    : chr [1:5] "(Intercept)" "price.heinz28" "price.heinz32" "price.heinz41" ...
  ..$ vcoefnames   : chr [1:5] "(Intercept)" "price.heinz28" "price.heinz32" "price.heinz41" ...
  ..$ xlevels      : Named list()
  ..$ edf          : num 15
  ..$ AIC          : num 5052
  ..$ model        :'data.frame':	2798 obs. of  5 variables:
  .. ..$ choice       : Factor w/ 4 levels "heinz28","heinz32",..: 1 1 1 1 1 1 1 3 1 1 ...
  .. ..$ price.heinz28: num [1:2798] 5.2 5.2 4.6 5.2 4.6 ...
  .. ..$ price.heinz32: num [1:2798] 3.7 4.3 2.5 3.7 3 ...
  .. ..$ price.heinz41: num [1:2798] 4.6 4.6 4.6 4.6 4.6 ...
  .. ..$ price.hunts32: num [1:2798] 3.4 4.4 4.8 3.4 4.8 ...
  .. ..- attr(*, "terms")=Classes 'terms', 'formula'  language choice ~ price.heinz28 + price.heinz32 + price.heinz41 + price.hunts32
  .. .. .. ..- attr(*, "variables")= language list(choice, price.heinz28, price.heinz32, price.heinz41, price.hunts32)
  .. .. .. ..- attr(*, "factors")= int [1:5, 1:4] 0 1 0 0 0 0 0 1 0 0 ...
  .. .. .. .. ..- attr(*, "dimnames")=List of 2
  .. .. .. .. .. ..$ : chr [1:5] "choice" "price.heinz28" "price.heinz32" "price.heinz41" ...
  .. .. .. .. .. ..$ : chr [1:4] "price.heinz28" "price.heinz32" "price.heinz41" "price.hunts32"
  .. .. .. ..- attr(*, "term.labels")= chr [1:4] "price.heinz28" "price.heinz32" "price.heinz41" "price.hunts32"
  .. .. .. ..- attr(*, "order")= int [1:4] 1 1 1 1
  .. .. .. ..- attr(*, "intercept")= int 1
  .. .. .. ..- attr(*, "response")= int 1
  .. .. .. ..- attr(*, ".Environment")=<environment: 0x1382f4eb8> 
  .. .. .. ..- attr(*, "predvars")= language list(choice, price.heinz28, price.heinz32, price.heinz41, price.hunts32)
  .. .. .. ..- attr(*, "dataClasses")= Named chr [1:5] "factor" "numeric" "numeric" "numeric" ...
  .. .. .. .. ..- attr(*, "names")= chr [1:5] "choice" "price.heinz28" "price.heinz32" "price.heinz41" ...
  ..$ null.deviance:Class 'logLik' : 6278 (df=3)
  ..$ logLik       :Class 'logLik' : -2511 (df=15)
  ..$ nobs         : int 2798
  ..- attr(*, "class")= chr [1:2] "multinom" "nnet"
 $ mnl_input  :List of 5
  ..$ formula:Class 'formula'  language choice ~ price.heinz28 + price.heinz32 + price.heinz41 + price.hunts32
  .. .. ..- attr(*, ".Environment")=<environment: 0x1382f4eb8> 
  ..$ weights: NULL
  ..$ data   :'data.frame':	2798 obs. of  5 variables:
  .. ..$ choice       : Factor w/ 4 levels "heinz28","heinz32",..: 1 1 1 1 1 1 1 3 1 1 ...
  .. ..$ price.heinz28: num [1:2798] 5.2 5.2 4.6 5.2 4.6 ...
  .. ..$ price.heinz32: num [1:2798] 3.7 4.3 2.5 3.7 3 ...
  .. ..$ price.heinz41: num [1:2798] 4.6 4.6 4.6 4.6 4.6 ...
  .. ..$ price.hunts32: num [1:2798] 3.4 4.4 4.8 3.4 4.8 ...
  .. ..- attr(*, "description")= chr "## Choice of ketchup brands\n\nChoice behavior for 300 individuals in a panel of households in Springfield, Mis"| __truncated__
  ..$ model  : logi TRUE
  ..$ trace  : logi FALSE
 $ mmx        :List of 2
  ..$ min:'data.frame':	1 obs. of  4 variables:
  .. ..$ price.heinz28: num 0.1
  .. ..$ price.heinz32: num 0.3
  .. ..$ price.heinz41: num 2
  .. ..$ price.hunts32: num 0.3
  ..$ max:'data.frame':	1 obs. of  4 variables:
  .. ..$ price.heinz28: num 12
  .. ..$ price.heinz32: num 4.3
  .. ..$ price.heinz41: num 7.2
  .. ..$ price.hunts32: num 8.7
 $ rv         : Factor w/ 4 levels "heinz28","heinz32",..: 1 1 1 1 1 1 1 3 1 1 ...
 $ not_vary   : chr(0) 
 $ df_name    : chr "ketchup"
 $ vars       : chr [1:4] "price.heinz28" "price.heinz32" "price.heinz41" "price.hunts32"
 $ rvar       : chr "choice"
 $ evar       : chr [1:4] "price.heinz28" "price.heinz32" "price.heinz41" "price.hunts32"
 $ lev        : chr [1:4] "heinz28" "heinz32" "heinz41" "hunts32"
 $ int        : chr ""
 $ wts        : NULL
 $ check      : chr ""
 $ data_filter: chr ""
 $ arr        : chr ""
 $ rows       : NULL
 - attr(*, "class")= chr [1:3] "mnl" "model" "list"
> 
> 
> 
> 
> cleanEx()
> nameEx("nb")
> ### * nb
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: nb
> ### Title: Naive Bayes using e1071::naiveBayes
> ### Aliases: nb
> 
> ### ** Examples
> 
> nb(titanic, "survived", c("pclass", "sex", "age")) %>% summary()
Naive Bayes Classifier
Data                 : titanic
Response variable    : survived
Levels               : Yes, No in survived
Explanatory variables: pclass, sex, age
Laplace              : 0
Nr obs               : 1,043 

A-priori probabilities:
survived
  Yes    No 
0.407 0.593 

Conditional probabilities (categorical) or means & st.dev (numeric):
        pclass
survived   1st   2nd   3rd
     Yes 0.421 0.271 0.308
     No  0.167 0.236 0.597

        sex
survived female  male
     Yes  0.682 0.318
     No   0.155 0.845

        age
survived   mean st.dev
     Yes 28.819 15.004
     No  30.497 13.881

> nb(titanic, "survived", c("pclass", "sex", "age")) %>% str()
List of 12
 $ model      :List of 7
  ..$ apriori  : 'table' int [1:2(1d)] 425 618
  .. ..- attr(*, "dimnames")=List of 1
  .. .. ..$ dataset[[1]]: chr [1:2] "Yes" "No"
  ..$ tables   :List of 3
  .. ..$ pclass: 'table' num [1:2, 1:3] 0.421 0.167 0.271 0.236 0.308 ...
  .. .. ..- attr(*, "dimnames")=List of 2
  .. .. .. ..$ dataset[[1]]: chr [1:2] "Yes" "No"
  .. .. .. ..$ pclass      : chr [1:3] "1st" "2nd" "3rd"
  .. ..$ sex   : 'table' num [1:2, 1:2] 0.682 0.155 0.318 0.845
  .. .. ..- attr(*, "dimnames")=List of 2
  .. .. .. ..$ dataset[[1]]: chr [1:2] "Yes" "No"
  .. .. .. ..$ sex         : chr [1:2] "female" "male"
  .. ..$ age   : num [1:2, 1:2] 28.8 30.5 15 13.9
  .. .. ..- attr(*, "dimnames")=List of 2
  .. .. .. ..$ dataset[[1]]: chr [1:2] "Yes" "No"
  .. .. .. ..$ age         : NULL
  ..$ levels   : chr [1:2] "Yes" "No"
  ..$ isnumeric: Named logi [1:3] FALSE FALSE TRUE
  .. ..- attr(*, "names")= chr [1:3] "pclass" "sex" "age"
  ..$ call     : language naiveBayes.default(x = dataset[, -1, drop = FALSE], y = dataset[[1]], laplace = laplace)
  ..$ residuals: logi NA
  ..$ model    : tibble [1,043 × 4] (S3: tbl_df/tbl/data.frame)
  .. ..$ survived: Factor w/ 2 levels "Yes","No": 1 1 2 2 2 1 1 2 1 2 ...
  .. ..$ pclass  : Factor w/ 3 levels "1st","2nd","3rd": 1 1 1 1 1 1 1 1 1 1 ...
  .. ..$ sex     : Factor w/ 2 levels "female","male": 1 2 1 2 1 2 1 2 1 2 ...
  .. ..$ age     : num [1:1043] 29 0.917 2 30 25 ...
  .. ..- attr(*, "description")= chr "## Titanic\n\nThis dataset describes the survival status of individual passengers on the Titanic. The titanic d"| __truncated__
  ..- attr(*, "class")= chr "naiveBayes"
 $ form       :Class 'formula'  language survived ~ pclass + sex + age
  .. ..- attr(*, ".Environment")=<environment: 0x1383e4f68> 
 $ lev        : chr [1:2] "Yes" "No"
 $ vars       : chr [1:3] "pclass" "sex" "age"
 $ not_vary   : chr(0) 
 $ df_name    : chr "titanic"
 $ rvar       : chr "survived"
 $ evar       : chr [1:3] "pclass" "sex" "age"
 $ laplace    : num 0
 $ data_filter: chr ""
 $ arr        : chr ""
 $ rows       : NULL
 - attr(*, "class")= chr [1:3] "nb" "model" "list"
> 
> 
> 
> 
> cleanEx()
> nameEx("nn")
> ### * nn
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: nn
> ### Title: Neural Networks using nnet
> ### Aliases: nn
> 
> ### ** Examples
> 
> nn(titanic, "survived", c("pclass", "sex"), lev = "Yes") %>% summary()
Neural Network
Activation function  : Logistic (classification)
Data                 : titanic
Response variable    : survived
Level                : Yes in survived
Explanatory variables: pclass, sex 
Network size         : 1 
Parameter decay      : 0.5 
Network              : 3-1-1 with 6 weights
Nr obs               : 1,043 
Weights              :
   b->h1 i1->h1 i2->h1 i3->h1 
   -1.93   0.90   2.46   3.12 
   b->o h1->o 
   2.94 -4.73  
> nn(titanic, "survived", c("pclass", "sex")) %>% str()
List of 22
 $ coefnames  : chr [1:3] "pclass|2nd" "pclass|3rd" "sex|male"
 $ model      :List of 21
  ..$ n            : num [1:3] 3 1 1
  ..$ nunits       : int 6
  ..$ nconn        : num [1:7] 0 0 0 0 0 4 6
  ..$ conn         : num [1:6] 0 1 2 3 0 4
  ..$ nsunits      : int 6
  ..$ decay        : num 0.5
  ..$ entropy      : logi TRUE
  ..$ softmax      : logi FALSE
  ..$ censored     : logi FALSE
  ..$ value        : num 515
  ..$ wts          : num [1:6] -1.931 0.903 2.463 3.119 2.941 ...
  ..$ convergence  : int 0
  ..$ fitted.values: num [1:1043, 1] 0.912 0.336 0.912 0.336 0.912 ...
  .. ..- attr(*, "dimnames")=List of 2
  .. .. ..$ : chr [1:1043] "1" "2" "3" "4" ...
  .. .. ..$ : NULL
  ..$ residuals    : Named num [1:1043] 0.0877 0.6639 -0.9123 -0.3361 -0.9123 ...
  .. ..- attr(*, "names")= chr [1:1043] "1" "2" "3" "4" ...
  ..$ lev          : chr [1:2] "0" "1"
  ..$ call         : language nnet.formula(formula = survived ~ ., data = list(survived = c(TRUE, TRUE,  FALSE, FALSE, FALSE, TRUE, TRUE, FALSE| __truncated__ ...
  ..$ terms        :Classes 'terms', 'formula'  language survived ~ pclass + sex
  .. .. ..- attr(*, "variables")= language list(survived, pclass, sex)
  .. .. ..- attr(*, "factors")= int [1:3, 1:2] 0 1 0 0 0 1
  .. .. .. ..- attr(*, "dimnames")=List of 2
  .. .. .. .. ..$ : chr [1:3] "survived" "pclass" "sex"
  .. .. .. .. ..$ : chr [1:2] "pclass" "sex"
  .. .. ..- attr(*, "term.labels")= chr [1:2] "pclass" "sex"
  .. .. ..- attr(*, "order")= int [1:2] 1 1
  .. .. ..- attr(*, "intercept")= int 1
  .. .. ..- attr(*, "response")= int 1
  .. .. ..- attr(*, ".Environment")=<environment: 0x13922b8e8> 
  .. .. ..- attr(*, "predvars")= language list(survived, pclass, sex)
  .. .. ..- attr(*, "dataClasses")= Named chr [1:3] "logical" "factor" "factor"
  .. .. .. ..- attr(*, "names")= chr [1:3] "survived" "pclass" "sex"
  ..$ coefnames    : chr [1:3] "pclass2nd" "pclass3rd" "sexmale"
  ..$ contrasts    :List of 2
  .. ..$ pclass: chr "contr.treatment"
  .. ..$ sex   : chr "contr.treatment"
  ..$ xlevels      :List of 2
  .. ..$ pclass: chr [1:3] "1st" "2nd" "3rd"
  .. ..$ sex   : chr [1:2] "female" "male"
  ..$ model        : tibble [1,043 × 3] (S3: tbl_df/tbl/data.frame)
  .. ..$ survived: logi [1:1043] TRUE TRUE FALSE FALSE FALSE TRUE ...
  .. ..$ pclass  : Factor w/ 3 levels "1st","2nd","3rd": 1 1 1 1 1 1 1 1 1 1 ...
  .. ..$ sex     : Factor w/ 2 levels "female","male": 1 2 1 2 1 2 1 2 1 2 ...
  .. ..- attr(*, "description")= chr "## Titanic\n\nThis dataset describes the survival status of individual passengers on the Titanic. The titanic d"| __truncated__
  ..- attr(*, "class")= chr [1:2] "nnet.formula" "nnet"
 $ nninput    :List of 11
  ..$ formula:Class 'formula'  language survived ~ .
  .. .. ..- attr(*, ".Environment")=<environment: 0x13922b8e8> 
  ..$ rang   : num 0.1
  ..$ size   : num 1
  ..$ decay  : num 0.5
  ..$ weights: NULL
  ..$ maxit  : num 10000
  ..$ linout : logi FALSE
  ..$ entropy: logi TRUE
  ..$ skip   : logi FALSE
  ..$ trace  : logi FALSE
  ..$ data   : tibble [1,043 × 3] (S3: tbl_df/tbl/data.frame)
  .. ..$ survived: logi [1:1043] TRUE TRUE FALSE FALSE FALSE TRUE ...
  .. ..$ pclass  : Factor w/ 3 levels "1st","2nd","3rd": 1 1 1 1 1 1 1 1 1 1 ...
  .. ..$ sex     : Factor w/ 2 levels "female","male": 1 2 1 2 1 2 1 2 1 2 ...
  .. ..- attr(*, "description")= chr "## Titanic\n\nThis dataset describes the survival status of individual passengers on the Titanic. The titanic d"| __truncated__
 $ entropy    : logi TRUE
 $ linout     : logi FALSE
 $ rv         : Factor w/ 2 levels "Yes","No": 1 1 2 2 2 1 1 2 1 2 ...
 $ not_vary   : chr(0) 
 $ df_name    : chr "titanic"
 $ vars       : chr [1:2] "pclass" "sex"
 $ rvar       : chr "survived"
 $ evar       : chr [1:2] "pclass" "sex"
 $ type       : chr "classification"
 $ lev        : chr "Yes"
 $ size       : num 1
 $ decay      : num 0.5
 $ wts        : NULL
 $ seed       : chr NA
 $ check      : chr "standardize"
 $ form       :Class 'formula'  language survived ~ .
  .. ..- attr(*, ".Environment")=<environment: 0x13922b8e8> 
 $ data_filter: chr ""
 $ arr        : chr ""
 $ rows       : NULL
 - attr(*, "class")= chr [1:3] "nn" "model" "list"
> nn(diamonds, "price", c("carat", "clarity"), type = "regression") %>% summary()
Neural Network
Activation function  : Linear (regression)
Data                 : diamonds
Response variable    : price
Explanatory variables: carat, clarity 
Network size         : 1 
Parameter decay      : 0.5 
Network              : 8-1-1 with 11 weights
Nr obs               : 3,000 
Weights              :
   b->h1 i1->h1 i2->h1 i3->h1 i4->h1 i5->h1 i6->h1 i7->h1 i8->h1 
   -1.71   2.01   0.31   0.50   0.69   0.76   0.96   0.94   1.01 
   b->o h1->o 
  -0.72  2.56  
> 
> 
> 
> cleanEx()
> nameEx("onehot")
> ### * onehot
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: onehot
> ### Title: One hot encoding of data.frames
> ### Aliases: onehot
> 
> ### ** Examples
> 
> head(onehot(diamonds, df = TRUE))
      (Intercept) price carat claritySI2 claritySI1 clarityVS2 clarityVS1
7718            1   580  0.32          0          0          0          1
27822           1   650  0.34          0          1          0          0
23018           1   630  0.30          0          0          1          0
29608           1   706  0.35          0          0          0          0
39543           1  1080  0.40          0          0          1          0
1919            1  3082  0.60          0          0          0          0
      clarityVVS2 clarityVVS1 clarityIF cutGood cutVery Good cutPremium
7718            0           0         0       0            0          0
27822           0           0         0       0            1          0
23018           0           0         0       0            1          0
29608           1           0         0       0            0          0
39543           0           0         0       0            0          1
1919            0           1         0       0            0          0
      cutIdeal colorE colorF colorG colorH colorI colorJ depth table    x    y
7718         1      0      0      0      1      0      0  61.0  56.0 4.43 4.45
27822        0      0      0      1      0      0      0  63.4  57.0 4.45 4.42
23018        0      0      0      1      0      0      0  63.1  58.0 4.27 4.23
29608        1      0      0      0      1      0      0  59.2  56.0 4.60 4.65
39543        0      0      1      0      0      0      0  62.6  58.0 4.72 4.68
1919         1      1      0      0      0      0      0  62.5  53.7 5.35 5.43
         z  date
7718  2.71 15396
27822 2.81 15396
23018 2.68 15396
29608 2.74 15396
39543 2.94 15396
1919  3.38 15396
> head(onehot(diamonds, all = TRUE, df = TRUE))
      price carat clarityI1 claritySI2 claritySI1 clarityVS2 clarityVS1
7718    580  0.32         0          0          0          0          1
27822   650  0.34         0          0          1          0          0
23018   630  0.30         0          0          0          1          0
29608   706  0.35         0          0          0          0          0
39543  1080  0.40         0          0          0          1          0
1919   3082  0.60         0          0          0          0          0
      clarityVVS2 clarityVVS1 clarityIF cutFair cutGood cutVery Good cutPremium
7718            0           0         0       0       0            0          0
27822           0           0         0       0       0            1          0
23018           0           0         0       0       0            1          0
29608           1           0         0       0       0            0          0
39543           0           0         0       0       0            0          1
1919            0           1         0       0       0            0          0
      cutIdeal colorD colorE colorF colorG colorH colorI colorJ depth table
7718         1      0      0      0      0      1      0      0  61.0  56.0
27822        0      0      0      0      1      0      0      0  63.4  57.0
23018        0      0      0      0      1      0      0      0  63.1  58.0
29608        1      0      0      0      0      1      0      0  59.2  56.0
39543        0      0      0      1      0      0      0      0  62.6  58.0
1919         1      0      1      0      0      0      0      0  62.5  53.7
         x    y    z  date
7718  4.43 4.45 2.71 15396
27822 4.45 4.42 2.81 15396
23018 4.27 4.23 2.68 15396
29608 4.60 4.65 2.74 15396
39543 4.72 4.68 2.94 15396
1919  5.35 5.43 3.38 15396
> 
> 
> 
> cleanEx()
> nameEx("plot.confusion")
> ### * plot.confusion
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: plot.confusion
> ### Title: Plot method for the confusion matrix
> ### Aliases: plot.confusion
> 
> ### ** Examples
> 
> data.frame(buy = dvd$buy, pred1 = runif(20000), pred2 = ifelse(dvd$buy == "yes", 1, 0)) %>%
+   confusion(c("pred1", "pred2"), "buy") %>%
+   plot()
> 
> 
> 
> cleanEx()
> nameEx("plot.crtree")
> ### * plot.crtree
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: plot.crtree
> ### Title: Plot method for the crtree function
> ### Aliases: plot.crtree
> 
> ### ** Examples
> 
> result <- crtree(titanic, "survived", c("pclass", "sex"), lev = "Yes")
> plot(result)
> result <- crtree(diamonds, "price", c("carat", "clarity", "cut"))
> plot(result, plots = "prune")
> result <- crtree(dvd, "buy", c("coupon", "purch", "last"), cp = .01)
> plot(result, plots = "imp")
> 
> 
> 
> 
> cleanEx()
> nameEx("plot.dtree")
> ### * plot.dtree
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: plot.dtree
> ### Title: Plot method for the dtree function
> ### Aliases: plot.dtree
> 
> ### ** Examples
> 
> dtree(movie_contract, opt = "max") %>% plot()
> dtree(movie_contract, opt = "max") %>% plot(final = TRUE, orient = "TD")
> 
> 
> 
> 
> cleanEx()
> nameEx("plot.evalbin")
> ### * plot.evalbin
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: plot.evalbin
> ### Title: Plot method for the evalbin function
> ### Aliases: plot.evalbin
> 
> ### ** Examples
> 
> data.frame(buy = dvd$buy, pred1 = runif(20000), pred2 = ifelse(dvd$buy == "yes", 1, 0)) %>%
+   evalbin(c("pred1", "pred2"), "buy") %>%
+   plot()
> 
> 
> 
> cleanEx()
> nameEx("plot.evalreg")
> ### * plot.evalreg
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: plot.evalreg
> ### Title: Plot method for the evalreg function
> ### Aliases: plot.evalreg
> 
> ### ** Examples
> 
> data.frame(price = diamonds$price, pred1 = rnorm(3000), pred2 = diamonds$price) %>%
+   evalreg(pred = c("pred1", "pred2"), "price") %>%
+   plot()
> 
> 
> 
> 
> cleanEx()
> nameEx("plot.gbt")
> ### * plot.gbt
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: plot.gbt
> ### Title: Plot method for the gbt function
> ### Aliases: plot.gbt
> 
> ### ** Examples
> 
> result <- gbt(titanic, "survived", c("pclass", "sex"), early_stopping_rounds = 0)
> plot(result)
> 
> 
> 
> 
> cleanEx()
> nameEx("plot.logistic")
> ### * plot.logistic
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: plot.logistic
> ### Title: Plot method for the logistic function
> ### Aliases: plot.logistic
> 
> ### ** Examples
> 
> result <- logistic(titanic, "survived", c("pclass", "sex"), lev = "Yes")
> plot(result, plots = "coef")
Waiting for profiling to be done...
> 
> 
> 
> cleanEx()
> nameEx("plot.mnl")
> ### * plot.mnl
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: plot.mnl
> ### Title: Plot method for the mnl function
> ### Aliases: plot.mnl
> 
> ### ** Examples
> 
> result <- mnl(
+   ketchup,
+   rvar = "choice",
+   evar = c("price.heinz28", "price.heinz32", "price.heinz41", "price.hunts32"),
+   lev = "heinz28"
+ )
> plot(result, plots = "coef")
Warning: `position_dodge()` requires non-overlapping x intervals
> 
> 
> 
> 
> cleanEx()
> nameEx("plot.mnl.predict")
> ### * plot.mnl.predict
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: plot.mnl.predict
> ### Title: Plot method for mnl.predict function
> ### Aliases: plot.mnl.predict
> 
> ### ** Examples
> 
> result <- mnl(
+   ketchup,
+   rvar = "choice",
+   evar = c("price.heinz28", "price.heinz32", "price.heinz41", "price.hunts32"),
+   lev = "heinz28"
+ )
> pred <- predict(result, pred_cmd = "price.heinz28 = seq(3, 5, 0.1)")
> plot(pred, xvar = "price.heinz28")
> 
> 
> 
> 
> cleanEx()
> nameEx("plot.model.predict")
> ### * plot.model.predict
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: plot.model.predict
> ### Title: Plot method for model.predict functions
> ### Aliases: plot.model.predict
> 
> ### ** Examples
> 
> regress(diamonds, "price", c("carat", "clarity")) %>%
+   predict(pred_cmd = "carat = 1:10") %>%
+   plot(xvar = "carat")
> logistic(titanic, "survived", c("pclass", "sex", "age"), lev = "Yes") %>%
+   predict(pred_cmd = c("pclass = levels(pclass)", "sex = levels(sex)", "age = 0:100")) %>%
+   plot(xvar = "age", color = "sex", facet_col = "pclass")
> 
> 
> 
> 
> cleanEx()
> nameEx("plot.nb")
> ### * plot.nb
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: plot.nb
> ### Title: Plot method for the nb function
> ### Aliases: plot.nb
> 
> ### ** Examples
> 
> result <- nb(titanic, "survived", c("pclass", "sex"))
> plot(result)
> result <- nb(titanic, "pclass", c("sex", "age"))
> plot(result)
> 
> 
> 
> 
> cleanEx()
> nameEx("plot.nb.predict")
> ### * plot.nb.predict
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: plot.nb.predict
> ### Title: Plot method for nb.predict function
> ### Aliases: plot.nb.predict
> 
> ### ** Examples
> 
> result <- nb(titanic, "survived", c("pclass", "sex", "age"))
> pred <- predict(
+   result,
+   pred_cmd = c("pclass = levels(pclass)", "sex = levels(sex)", "age = seq(0, 100, 20)")
+ )
> plot(pred, xvar = "age", facet_col = "sex", facet_row = "pclass")
> pred <- predict(result, pred_data = titanic)
> plot(pred, xvar = "age", facet_col = "sex")
> 
> 
> 
> 
> cleanEx()
> nameEx("plot.nn")
> ### * plot.nn
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: plot.nn
> ### Title: Plot method for the nn function
> ### Aliases: plot.nn
> 
> ### ** Examples
> 
> result <- nn(titanic, "survived", c("pclass", "sex"), lev = "Yes")
> plot(result, plots = "net")
> plot(result, plots = "olden")
> 
> 
> 
> cleanEx()
> nameEx("plot.regress")
> ### * plot.regress
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: plot.regress
> ### Title: Plot method for the regress function
> ### Aliases: plot.regress
> 
> ### ** Examples
> 
> result <- regress(diamonds, "price", c("carat", "clarity"))
> plot(result, plots = "coef", conf_lev = .99, intercept = TRUE)
> ## Not run: 
> ##D plot(result, plots = "dist")
> ##D plot(result, plots = "scatter", lines = c("line", "loess"))
> ##D plot(result, plots = "resid_pred", lines = "line")
> ##D plot(result, plots = "dashboard", lines = c("line", "loess"))
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("plot.rforest")
> ### * plot.rforest
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: plot.rforest
> ### Title: Plot method for the rforest function
> ### Aliases: plot.rforest
> 
> ### ** Examples
> 
> result <- rforest(titanic, "survived", c("pclass", "sex"), lev = "Yes")
> 
> 
> 
> 
> cleanEx()
> nameEx("plot.rforest.predict")
> ### * plot.rforest.predict
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: plot.rforest.predict
> ### Title: Plot method for rforest.predict function
> ### Aliases: plot.rforest.predict
> 
> ### ** Examples
> 
> result <- mnl(
+   ketchup,
+   rvar = "choice",
+   evar = c("price.heinz28", "price.heinz32", "price.heinz41", "price.hunts32"),
+   lev = "heinz28"
+ )
> pred <- predict(result, pred_cmd = "price.heinz28 = seq(3, 5, 0.1)")
> plot(pred, xvar = "price.heinz28")
> 
> 
> 
> 
> cleanEx()
> nameEx("plot.simulater")
> ### * plot.simulater
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: plot.simulater
> ### Title: Plot method for the simulater function
> ### Aliases: plot.simulater
> 
> ### ** Examples
> 
> simdat <- simulater(
+   const = "cost 3",
+   norm = "demand 2000 1000",
+   discrete = "price 5 8 .3 .7",
+   form = "profit = demand * (price - cost)",
+   seed = 1234
+ )
> plot(simdat, bins = 25)
> 
> 
> 
> 
> cleanEx()
> nameEx("predict.crtree")
> ### * predict.crtree
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: predict.crtree
> ### Title: Predict method for the crtree function
> ### Aliases: predict.crtree
> 
> ### ** Examples
> 
> result <- crtree(titanic, "survived", c("pclass", "sex"), lev = "Yes")
> predict(result, pred_cmd = "pclass = levels(pclass)")
Classification and regression trees
Data                 : titanic 
Response variable    : survived 
Level(s)             : Yes in survived 
Explanatory variables: pclass, sex 
Prediction command   : pclass = levels(pclass) 

  sex pclass Prediction
 male    1st      0.205
 male    2nd      0.205
 male    3rd      0.205
> result <- crtree(titanic, "survived", "pclass", lev = "Yes")
> predict(result, pred_data = titanic) %>% head()
Classification and regression trees
Data                 : titanic 
Response variable    : survived 
Level(s)             : Yes in survived 
Explanatory variables: pclass 
Prediction dataset   : titanic 

 pclass Prediction
    1st      0.635
    1st      0.635
    1st      0.635
    1st      0.635
    1st      0.635
    1st      0.635
> 
> 
> 
> cleanEx()
> nameEx("predict.gbt")
> ### * predict.gbt
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: predict.gbt
> ### Title: Predict method for the gbt function
> ### Aliases: predict.gbt
> 
> ### ** Examples
> 
> result <- gbt(titanic, "survived", c("pclass", "sex"), early_stopping_rounds = 0)
> predict(result, pred_cmd = "pclass = levels(pclass)")
Gradiant Boosted Trees
Data                 : titanic 
Response variable    : survived 
Level(s)             : Yes in survived 
Explanatory variables: pclass, sex 
Prediction command   : pclass = levels(pclass) 

  sex pclass Prediction
 male    1st      0.457
 male    2nd      0.398
 male    3rd      0.403
> result <- gbt(diamonds, "price", "carat:color", type = "regression")
> predict(result, pred_cmd = "carat = 1:3")
Gradiant Boosted Trees
Data                 : diamonds 
Response variable    : price 
Explanatory variables: carat, clarity, cut, color 
Prediction command   : carat = 1:3 

 clarity   cut color carat Prediction
     SI1 Ideal     G     1   4855.246
     SI1 Ideal     G     2  16499.922
     SI1 Ideal     G     3  19909.562
> predict(result, pred_data = diamonds) %>% head()
Gradiant Boosted Trees
Data                 : diamonds 
Response variable    : price 
Explanatory variables: carat, clarity, cut, color 
Prediction dataset   : diamonds 

 carat clarity       cut color Prediction
 0.320     VS1     Ideal     H    579.664
 0.340     SI1 Very Good     G    546.916
 0.300     VS2 Very Good     G    566.852
 0.350    VVS2     Ideal     H    720.258
 0.400     VS2   Premium     F    909.794
 0.600    VVS1     Ideal     E   3264.439
> 
> 
> 
> cleanEx()
> nameEx("predict.logistic")
> ### * predict.logistic
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: predict.logistic
> ### Title: Predict method for the logistic function
> ### Aliases: predict.logistic
> 
> ### ** Examples
> 
> result <- logistic(titanic, "survived", c("pclass", "sex"), lev = "Yes")
> predict(result, pred_cmd = "pclass = levels(pclass)")
Logistic regression (GLM)
Data                 : titanic 
Response variable    : survived 
Level(s)             : Yes in survived 
Explanatory variables: pclass, sex 
Interval             : confidence 
Prediction command   : pclass = levels(pclass) 

  sex pclass Prediction  2.5% 97.5%
 male    1st      0.408 0.340 0.480
 male    2nd      0.220 0.170 0.280
 male    3rd      0.111 0.086 0.142
> logistic(titanic, "survived", c("pclass", "sex"), lev = "Yes") %>%
+   predict(pred_cmd = "sex = c('male','female')")
Logistic regression (GLM)
Data                 : titanic 
Response variable    : survived 
Level(s)             : Yes in survived 
Explanatory variables: pclass, sex 
Interval             : confidence 
Prediction command   : sex = c('male', 'female') 

 pclass    sex Prediction  2.5% 97.5%
    3rd   male      0.111 0.086 0.142
    3rd female      0.608 0.540 0.673
> logistic(titanic, "survived", c("pclass", "sex"), lev = "Yes") %>%
+   predict(pred_data = titanic)
Logistic regression (GLM)
Data                 : titanic 
Response variable    : survived 
Level(s)             : Yes in survived 
Explanatory variables: pclass, sex 
Interval             : confidence 
Prediction dataset   : titanic 
Rows shown           : 10 of 1,043 

 pclass    sex Prediction  2.5% 97.5%
    1st female      0.896 0.856 0.926
    1st   male      0.408 0.340 0.480
    1st female      0.896 0.856 0.926
    1st   male      0.408 0.340 0.480
    1st female      0.896 0.856 0.926
    1st   male      0.408 0.340 0.480
    1st female      0.896 0.856 0.926
    1st   male      0.408 0.340 0.480
    1st female      0.896 0.856 0.926
    1st   male      0.408 0.340 0.480
> 
> 
> 
> cleanEx()
> nameEx("predict.mnl")
> ### * predict.mnl
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: predict.mnl
> ### Title: Predict method for the mnl function
> ### Aliases: predict.mnl
> 
> ### ** Examples
> 
> result <- mnl(
+   ketchup,
+   rvar = "choice",
+   evar = c("price.heinz28", "price.heinz32", "price.heinz41", "price.hunts32"),
+   lev = "heinz28"
+ )
> predict(result, pred_cmd = "price.heinz28 = seq(3, 5, 0.1)")
Multinomial logistic regression (MNL)
Data                 : ketchup 
Response variable    : choice 
Level(s)             : heinz28, heinz32, heinz41, hunts32 in choice 
Explanatory variables: price.heinz28, price.heinz32, price.heinz41, price.hunts32 
Prediction command   : price.heinz28 = seq(3,  5,  0.1) 
Rows shown           : 10 of 21 

 price.heinz32 price.heinz41 price.hunts32 price.heinz28 heinz28 heinz32
         3.143         4.634         3.355         3.000   0.621   0.304
         3.143         4.634         3.355         3.100   0.596   0.325
         3.143         4.634         3.355         3.200   0.570   0.348
         3.143         4.634         3.355         3.300   0.544   0.370
         3.143         4.634         3.355         3.400   0.517   0.393
         3.143         4.634         3.355         3.500   0.491   0.416
         3.143         4.634         3.355         3.600   0.464   0.439
         3.143         4.634         3.355         3.700   0.437   0.462
         3.143         4.634         3.355         3.800   0.411   0.484
         3.143         4.634         3.355         3.900   0.385   0.507
 heinz41 hunts32
   0.051   0.023
   0.053   0.025
   0.055   0.028
   0.056   0.030
   0.057   0.032
   0.059   0.035
   0.060   0.038
   0.061   0.040
   0.062   0.043
   0.062   0.046
> predict(result, pred_data = slice(ketchup, 1:20))
Multinomial logistic regression (MNL)
Data                 : ketchup 
Response variable    : choice 
Level(s)             : heinz28, heinz32, heinz41, hunts32 in choice 
Explanatory variables: price.heinz28, price.heinz32, price.heinz41, price.hunts32 
Prediction dataset   : slice(ketchup, 1:20) 
Rows shown           : 10 of 20 

 price.heinz28 price.heinz32 price.heinz41 price.hunts32 heinz28 heinz32
         5.200         3.700         4.600         3.400   0.272   0.423
         5.200         4.300         4.600         4.400   0.591   0.244
         4.600         2.500         4.600         4.800   0.067   0.918
         5.200         3.700         4.600         3.400   0.272   0.423
         4.600         3.000         4.600         4.800   0.182   0.784
         4.700         3.000         5.000         3.000   0.175   0.657
         4.600         3.100         5.100         4.100   0.258   0.706
         4.700         3.400         4.600         3.100   0.262   0.460
         4.700         3.400         5.000         3.100   0.294   0.444
         5.000         3.400         5.000         2.800   0.177   0.365
 heinz41 hunts32
   0.098   0.206
   0.121   0.043
   0.015   0.000
   0.098   0.206
   0.033   0.001
   0.035   0.133
   0.027   0.010
   0.081   0.197
   0.049   0.213
   0.041   0.418
> 
> 
> 
> 
> cleanEx()
> nameEx("predict.nb")
> ### * predict.nb
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: predict.nb
> ### Title: Predict method for the nb function
> ### Aliases: predict.nb
> 
> ### ** Examples
> 
> result <- nb(titanic, "survived", c("pclass", "sex", "age"))
> predict(result, pred_data = titanic)
Naive Bayes Classifier
Data                 : titanic 
Response variable    : survived 
Level(s)             : Yes, No in survived 
Explanatory variables: pclass, sex, age 
Prediction dataset   : titanic 
Rows shown           : 10 of 1,043 

 pclass    sex    age   Yes    No
    1st female 29.000 0.877 0.123
    1st   male  0.917 0.510 0.490
    1st female  2.000 0.922 0.078
    1st   male 30.000 0.376 0.624
    1st female 25.000 0.881 0.119
    1st   male 48.000 0.372 0.628
    1st female 63.000 0.891 0.109
    1st   male 39.000 0.367 0.633
    1st female 53.000 0.878 0.122
    1st   male 71.000 0.451 0.549
> predict(result, pred_data = titanic, pred_names = c("Yes", "No"))
Naive Bayes Classifier
Data                 : titanic 
Response variable    : survived 
Level(s)             : Yes, No in survived 
Explanatory variables: pclass, sex, age 
Prediction dataset   : titanic 
Rows shown           : 10 of 1,043 

 pclass    sex    age   Yes    No
    1st female 29.000 0.877 0.123
    1st   male  0.917 0.510 0.490
    1st female  2.000 0.922 0.078
    1st   male 30.000 0.376 0.624
    1st female 25.000 0.881 0.119
    1st   male 48.000 0.372 0.628
    1st female 63.000 0.891 0.109
    1st   male 39.000 0.367 0.633
    1st female 53.000 0.878 0.122
    1st   male 71.000 0.451 0.549
> predict(result, pred_cmd = "pclass = levels(pclass)")
Naive Bayes Classifier
Data                 : titanic 
Response variable    : survived 
Level(s)             : Yes, No in survived 
Explanatory variables: pclass, sex, age 
Prediction command   : pclass = levels(pclass) 

    age  sex pclass   Yes    No
 29.813 male    1st 0.377 0.623
 29.813 male    2nd 0.215 0.785
 29.813 male    3rd 0.110 0.890
> result <- nb(titanic, "pclass", c("survived", "sex", "age"))
> predict(result, pred_data = titanic)
Naive Bayes Classifier
Data                 : titanic 
Response variable    : pclass 
Level(s)             : 1st, 2nd, 3rd in pclass 
Explanatory variables: survived, sex, age 
Prediction dataset   : titanic 
Rows shown           : 10 of 1,043 

 survived    sex    age   1st   2nd   3rd
      Yes female 29.000 0.410 0.303 0.287
      Yes   male  0.917 0.117 0.315 0.568
       No female  2.000 0.058 0.253 0.689
       No   male 30.000 0.107 0.222 0.672
       No female 25.000 0.133 0.258 0.609
      Yes   male 48.000 0.633 0.236 0.130
      Yes female 63.000 0.891 0.098 0.011
       No   male 39.000 0.196 0.262 0.542
      Yes female 53.000 0.792 0.165 0.043
       No   male 71.000 0.821 0.153 0.026
> predict(result, pred_data = titanic, pred_names = c("1st", "2nd", "3rd"))
Naive Bayes Classifier
Data                 : titanic 
Response variable    : pclass 
Level(s)             : 1st, 2nd, 3rd in pclass 
Explanatory variables: survived, sex, age 
Prediction dataset   : titanic 
Rows shown           : 10 of 1,043 

 survived    sex    age   1st   2nd   3rd
      Yes female 29.000 0.410 0.303 0.287
      Yes   male  0.917 0.117 0.315 0.568
       No female  2.000 0.058 0.253 0.689
       No   male 30.000 0.107 0.222 0.672
       No female 25.000 0.133 0.258 0.609
      Yes   male 48.000 0.633 0.236 0.130
      Yes female 63.000 0.891 0.098 0.011
       No   male 39.000 0.196 0.262 0.542
      Yes female 53.000 0.792 0.165 0.043
       No   male 71.000 0.821 0.153 0.026
> predict(result, pred_data = titanic, pred_names = "")
Naive Bayes Classifier
Data                 : titanic 
Response variable    : pclass 
Level(s)             : 1st, 2nd, 3rd in pclass 
Explanatory variables: survived, sex, age 
Prediction dataset   : titanic 
Rows shown           : 10 of 1,043 

 survived    sex    age   1st   2nd   3rd
      Yes female 29.000 0.410 0.303 0.287
      Yes   male  0.917 0.117 0.315 0.568
       No female  2.000 0.058 0.253 0.689
       No   male 30.000 0.107 0.222 0.672
       No female 25.000 0.133 0.258 0.609
      Yes   male 48.000 0.633 0.236 0.130
      Yes female 63.000 0.891 0.098 0.011
       No   male 39.000 0.196 0.262 0.542
      Yes female 53.000 0.792 0.165 0.043
       No   male 71.000 0.821 0.153 0.026
> 
> 
> 
> 
> cleanEx()
> nameEx("predict.nn")
> ### * predict.nn
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: predict.nn
> ### Title: Predict method for the nn function
> ### Aliases: predict.nn
> 
> ### ** Examples
> 
> result <- nn(titanic, "survived", c("pclass", "sex"), lev = "Yes")
> predict(result, pred_cmd = "pclass = levels(pclass)")
Neural Network
Data                 : titanic 
Response variable    : survived 
Level(s)             : Yes in survived 
Explanatory variables: pclass, sex 
Prediction command   : pclass = levels(pclass) 

  sex pclass Prediction
 male    1st      0.336
 male    2nd      0.220
 male    3rd      0.159
> result <- nn(diamonds, "price", "carat:color", type = "regression")
> predict(result, pred_cmd = "carat = 1:3")
Neural Network
Data                 : diamonds 
Response variable    : price 
Explanatory variables: carat, clarity, cut, color 
Prediction command   : carat = 1:3 

 clarity   cut color carat Prediction
     SI1 Ideal     G     1   4987.184
     SI1 Ideal     G     2  15496.843
     SI1 Ideal     G     3  18541.229
> predict(result, pred_data = diamonds) %>% head()
Neural Network
Data                 : diamonds 
Response variable    : price 
Explanatory variables: carat, clarity, cut, color 
Prediction dataset   : diamonds 

 carat clarity       cut color Prediction
 0.320     VS1     Ideal     H    570.452
 0.340     SI1 Very Good     G    354.854
 0.300     VS2 Very Good     G    493.733
 0.350    VVS2     Ideal     H    999.900
 0.400     VS2   Premium     F   1080.268
 0.600    VVS1     Ideal     E   3524.279
> 
> 
> 
> cleanEx()
> nameEx("predict.regress")
> ### * predict.regress
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: predict.regress
> ### Title: Predict method for the regress function
> ### Aliases: predict.regress
> 
> ### ** Examples
> 
> result <- regress(diamonds, "price", c("carat", "clarity"))
> predict(result, pred_cmd = "carat = 1:10")
Linear regression (OLS)
Data                 : diamonds 
Response variable    : price 
Explanatory variables: carat, clarity 
Interval             : confidence 
Prediction command   : carat = 1:10 

 clarity carat Prediction      2.5%     97.5%     +/-
     SI1     1   5265.569  5174.776  5356.362  90.793
     SI1     2  13703.599 13557.662 13849.536 145.937
     SI1     3  22141.629 21908.326 22374.933 233.303
     SI1     4  30579.660 30251.571 30907.748 328.088
     SI1     5  39017.690 38592.329 39443.051 425.361
     SI1     6  47455.720 46931.983 47979.458 523.738
     SI1     7  55893.751 55271.056 56516.445 622.695
     SI1     8  64331.781 63609.787 65053.775 721.994
     SI1     9  72769.811 71948.301 73591.322 821.511
     SI1    10  81207.842 80286.667 82129.017 921.175
> predict(result, pred_cmd = "clarity = levels(clarity)")
Linear regression (OLS)
Data                 : diamonds 
Response variable    : price 
Explanatory variables: carat, clarity 
Interval             : confidence 
Prediction command   : clarity = levels(clarity) 

 carat clarity Prediction     2.5%    97.5%     +/-
 0.794      I1    -78.806 -462.319  304.707 383.513
 0.794     SI2   2711.953 2603.644 2820.263 108.310
 0.794     SI1   3529.725 3440.015 3619.436  89.711
 0.794     VS2   4171.100 4077.495 4264.704  93.605
 0.794     VS1   4383.150 4268.576 4497.725 114.574
 0.794    VVS2   5030.670 4886.596 5174.743 144.074
 0.794    VVS1   4948.863 4785.838 5111.888 163.025
 0.794      IF   5186.364 4942.495 5430.234 243.869
> result <- regress(diamonds, "price", c("carat", "clarity"), int = "carat:clarity")
> predict(result, pred_data = diamonds) %>% head()
Linear regression (OLS)
Data                 : diamonds 
Response variable    : price 
Explanatory variables: carat, clarity 
Interval             : confidence 
Prediction dataset   : diamonds 

 carat clarity Prediction     2.5%    97.5%     +/-
 0.320     VS1    240.946   88.847  393.045 152.099
 0.340     SI1   -119.580 -249.112    9.952 129.532
 0.300     VS2     -1.681 -129.827  126.465 128.146
 0.350    VVS2    854.443  690.259 1018.626 164.184
 0.400     VS2    842.564  727.982  957.146 114.582
 0.600    VVS1   3450.421 3290.503 3610.339 159.918
> 
> 
> 
> 
> cleanEx()
> nameEx("predict.rforest")
> ### * predict.rforest
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: predict.rforest
> ### Title: Predict method for the rforest function
> ### Aliases: predict.rforest
> 
> ### ** Examples
> 
> result <- rforest(titanic, "survived", c("pclass", "sex"), lev = "Yes")
> predict(result, pred_cmd = "pclass = levels(pclass)")
Warning: `all_equal()` was deprecated in dplyr 1.1.0.
ℹ Please use `all.equal()` instead.
ℹ And manually order the rows/cols as needed
ℹ The deprecated feature was likely used in the radiant.model package.
  Please report the issue at
  <https://github.com/radiant-rstats/radiant.model/issues/>.
Random Forest
Data                 : titanic 
Response variable    : survived 
Level(s)             : Yes in survived 
Explanatory variables: pclass, sex 
Prediction command   : pclass = levels(pclass) 
Additional arguments : OOB = NULL 

  sex pclass   Yes    No
 male    1st 0.343 0.657
 male    2nd 0.210 0.790
 male    3rd 0.198 0.802
> result <- rforest(diamonds, "price", "carat:color", type = "regression")
> predict(result, pred_cmd = "carat = 1:3")
Random Forest
Data                 : diamonds 
Response variable    : price 
Explanatory variables: carat, clarity, cut, color 
Prediction command   : carat = 1:3 
Additional arguments : OOB = NULL 

 clarity   cut color carat Prediction
     SI1 Ideal     G     1   5077.777
     SI1 Ideal     G     2  16863.723
     SI1 Ideal     G     3  17535.768
> predict(result, pred_data = diamonds) %>% head()
Using OOB predictions after comparing the training and prediction data
Using OOB predictions
Random Forest
Data                 : diamonds 
Response variable    : price 
Explanatory variables: carat, clarity, cut, color 
Prediction dataset   : diamonds 
Additional arguments : OOB = NULL 

 carat clarity       cut color Prediction
 0.320     VS1     Ideal     H    563.886
 0.340     SI1 Very Good     G    530.041
 0.300     VS2 Very Good     G    541.706
 0.350    VVS2     Ideal     H    802.670
 0.400     VS2   Premium     F    923.686
 0.600    VVS1     Ideal     E   2884.521
> 
> 
> 
> 
> cleanEx()
> nameEx("profit")
> ### * profit
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: profit
> ### Title: Calculate Profit based on cost:margin ratio
> ### Aliases: profit
> 
> ### ** Examples
> 
> profit(runif(20000), dvd$buy, "yes", cost = 1, margin = 2)
[1] -4779
> profit(ifelse(dvd$buy == "yes", 1, 0), dvd$buy, "yes", cost = 1, margin = 20)
[1] 99674
> profit(ifelse(dvd$buy == "yes", 1, 0), dvd$buy)
[1] 5246
> 
> 
> 
> cleanEx()
> nameEx("radiant.model")
> ### * radiant.model
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: radiant.model
> ### Title: radiant.model
> ### Aliases: radiant.model
> 
> ### ** Examples
> 
> ## Not run: 
> ##D radiant.model()
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("radiant.model_viewer")
> ### * radiant.model_viewer
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: radiant.model_viewer
> ### Title: Launch radiant.model in the Rstudio viewer
> ### Aliases: radiant.model_viewer
> 
> ### ** Examples
> 
> ## Not run: 
> ##D radiant.model_viewer()
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("radiant.model_window")
> ### * radiant.model_window
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: radiant.model_window
> ### Title: Launch radiant.model in an Rstudio window
> ### Aliases: radiant.model_window
> 
> ### ** Examples
> 
> ## Not run: 
> ##D radiant.model_window()
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("regress")
> ### * regress
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: regress
> ### Title: Linear regression using OLS
> ### Aliases: regress
> 
> ### ** Examples
> 
> regress(diamonds, "price", c("carat", "clarity"), check = "standardize") %>% summary()
Linear regression (OLS)
Data     : diamonds 
Response variable    : price 
Explanatory variables: carat, clarity 
Null hyp.: the effect of x on price is zero
Alt. hyp.: the effect of x on price is not zero
**Standardized coefficients shown (2 X SD)**

              coefficient std.error t.value p.value    
 (Intercept)       -0.504     0.025 -20.379  < .001 ***
 carat              1.010     0.006 165.125  < .001 ***
 clarity|SI2        0.353     0.025  13.857  < .001 ***
 clarity|SI1        0.456     0.025  17.997  < .001 ***
 clarity|VS2        0.537     0.025  21.080  < .001 ***
 clarity|VS1        0.564     0.026  21.809  < .001 ***
 clarity|VVS2       0.646     0.027  24.307  < .001 ***
 clarity|VVS1       0.635     0.027  23.466  < .001 ***
 clarity|IF         0.665     0.030  22.534  < .001 ***

Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

R-squared: 0.904,  Adjusted R-squared: 0.904 
F-statistic: 3530.024 df(8,2991), p.value < .001
Nr obs: 3,000 

> regress(diamonds, "price", c("carat", "clarity")) %>% str()
List of 15
 $ type       : chr "regression"
 $ coeff      :'data.frame':	9 obs. of  6 variables:
  ..$ label      : chr [1:9] "(Intercept)" "carat" "clarity|SI2" "clarity|SI1" ...
  ..$ coefficient: num [1:9] -6781 8438 2791 3609 4250 ...
  ..$ std.error  : num [1:9] 205 51.1 201.4 200.5 201.6 ...
  ..$ t.value    : num [1:9] -33.1 165.1 13.9 18 21.1 ...
  ..$ p.value    : num [1:9] 7.76e-205 0.00 2.28e-42 7.76e-69 4.38e-92 ...
  ..$ sig_star   : chr [1:9] "***" "***" "***" "***" ...
 $ model      :List of 13
  ..$ coefficients : Named num [1:9] -6781 8438 2791 3609 4250 ...
  .. ..- attr(*, "names")= chr [1:9] "(Intercept)" "carat" "claritySI2" "claritySI1" ...
  ..$ residuals    : Named num [1:3000] 199 954 630 -576 236 ...
  .. ..- attr(*, "names")= chr [1:3000] "1" "2" "3" "4" ...
  ..$ effects      : Named num [1:3000] -214005 200946 -26340 -18929 7642 ...
  .. ..- attr(*, "names")= chr [1:3000] "(Intercept)" "carat" "claritySI2" "claritySI1" ...
  ..$ rank         : int 9
  ..$ fitted.values: Named num [1:3000] 381.133 -303.531 0.322 1281.793 844.125 ...
  .. ..- attr(*, "names")= chr [1:3000] "1" "2" "3" "4" ...
  ..$ assign       : int [1:9] 0 1 2 2 2 2 2 2 2
  ..$ qr           :List of 5
  .. ..$ qr   : num [1:3000, 1:9] -54.7723 0.0183 0.0183 0.0183 0.0183 ...
  .. .. ..- attr(*, "dimnames")=List of 2
  .. .. .. ..$ : chr [1:3000] "1" "2" "3" "4" ...
  .. .. .. ..$ : chr [1:9] "(Intercept)" "carat" "claritySI2" "claritySI1" ...
  .. .. ..- attr(*, "assign")= int [1:9] 0 1 2 2 2 2 2 2 2
  .. .. ..- attr(*, "contrasts")=List of 1
  .. .. .. ..$ clarity: chr "contr.treatment"
  .. ..$ qraux: num [1:9] 1.02 1.02 1 1.01 1.03 ...
  .. ..$ pivot: int [1:9] 1 2 3 4 5 6 7 8 9
  .. ..$ tol  : num 0.0000001
  .. ..$ rank : int 9
  .. ..- attr(*, "class")= chr "qr"
  ..$ df.residual  : int 2991
  ..$ contrasts    :List of 1
  .. ..$ clarity: chr "contr.treatment"
  ..$ xlevels      :List of 1
  .. ..$ clarity: chr [1:8] "I1" "SI2" "SI1" "VS2" ...
  ..$ call         : language lm(formula = form_upper, data = dataset)
  ..$ terms        :Classes 'terms', 'formula'  language price ~ carat + clarity
  .. .. ..- attr(*, "variables")= language list(price, carat, clarity)
  .. .. ..- attr(*, "factors")= int [1:3, 1:2] 0 1 0 0 0 1
  .. .. .. ..- attr(*, "dimnames")=List of 2
  .. .. .. .. ..$ : chr [1:3] "price" "carat" "clarity"
  .. .. .. .. ..$ : chr [1:2] "carat" "clarity"
  .. .. ..- attr(*, "term.labels")= chr [1:2] "carat" "clarity"
  .. .. ..- attr(*, "order")= int [1:2] 1 1
  .. .. ..- attr(*, "intercept")= int 1
  .. .. ..- attr(*, "response")= int 1
  .. .. ..- attr(*, ".Environment")=<environment: 0x12b726400> 
  .. .. ..- attr(*, "predvars")= language list(price, carat, clarity)
  .. .. ..- attr(*, "dataClasses")= Named chr [1:3] "numeric" "numeric" "factor"
  .. .. .. ..- attr(*, "names")= chr [1:3] "price" "carat" "clarity"
  ..$ model        :'data.frame':	3000 obs. of  3 variables:
  .. ..$ price  : int [1:3000] 580 650 630 706 1080 3082 3328 4229 1895 3546 ...
  .. ..$ carat  : num [1:3000] 0.32 0.34 0.3 0.35 0.4 0.6 0.88 0.93 0.51 1.01 ...
  .. ..$ clarity: Factor w/ 8 levels "I1","SI2","SI1",..: 5 3 4 6 4 7 3 3 6 2 ...
  .. ..- attr(*, "terms")=Classes 'terms', 'formula'  language price ~ carat + clarity
  .. .. .. ..- attr(*, "variables")= language list(price, carat, clarity)
  .. .. .. ..- attr(*, "factors")= int [1:3, 1:2] 0 1 0 0 0 1
  .. .. .. .. ..- attr(*, "dimnames")=List of 2
  .. .. .. .. .. ..$ : chr [1:3] "price" "carat" "clarity"
  .. .. .. .. .. ..$ : chr [1:2] "carat" "clarity"
  .. .. .. ..- attr(*, "term.labels")= chr [1:2] "carat" "clarity"
  .. .. .. ..- attr(*, "order")= int [1:2] 1 1
  .. .. .. ..- attr(*, "intercept")= int 1
  .. .. .. ..- attr(*, "response")= int 1
  .. .. .. ..- attr(*, ".Environment")=<environment: 0x12b726400> 
  .. .. .. ..- attr(*, "predvars")= language list(price, carat, clarity)
  .. .. .. ..- attr(*, "dataClasses")= Named chr [1:3] "numeric" "numeric" "factor"
  .. .. .. .. ..- attr(*, "names")= chr [1:3] "price" "carat" "clarity"
  ..- attr(*, "class")= chr "lm"
 $ mmx        :List of 2
  ..$ min: tibble [1 × 2] (S3: tbl_df/tbl/data.frame)
  .. ..$ price: int 338
  .. ..$ carat: num 0.2
  ..$ max: tibble [1 × 2] (S3: tbl_df/tbl/data.frame)
  .. ..$ price: int 18791
  .. ..$ carat: num 3
 $ vars       : chr [1:2] "carat" "clarity"
 $ not_vary   : chr(0) 
 $ df_name    : chr "diamonds"
 $ rvar       : chr "price"
 $ evar       : chr [1:2] "carat" "clarity"
 $ int        : chr ""
 $ check      : chr ""
 $ form       : symbol 
 $ data_filter: chr ""
 $ arr        : chr ""
 $ rows       : NULL
 - attr(*, "class")= chr [1:3] "regress" "model" "list"
> 
> 
> 
> 
> cleanEx()
> nameEx("repeater")
> ### * repeater
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: repeater
> ### Title: Repeated simulation
> ### Aliases: repeater
> 
> ### ** Examples
> 
> simdat <- simulater(
+   const = c("var_cost 5", "fixed_cost 1000"),
+   norm = "E 0 100;",
+   discrete = "price 6 8 .3 .7;",
+   form = c(
+     "demand = 1000 - 50*price + E",
+     "profit = demand*(price-var_cost) - fixed_cost",
+     "profit_small = profit < 100"
+   ),
+   seed = 1234
+ )
> 
> repdat <- repeater(
+   simdat,
+   nr = 12,
+   vars = c("E", "price"),
+   sum_vars = "profit",
+   byvar = ".sim",
+   form = "profit_365 = profit_sum < 36500",
+   seed = 1234,
+ )
> 
> head(repdat)
  .sim profit_sum profit_365
1    1   7998.972       TRUE
2    2   7450.843       TRUE
3    3   5747.208       TRUE
4    4   2773.283       TRUE
5    5   6401.404       TRUE
6    6   6750.054       TRUE
> summary(repdat)
Repeated simulation
Simulations   : 1,000 
Repetitions   : 12 
Re-simulated  : E, price 
Group by      : Simulation 
Function      : sum 
Random  seed  : 1234 
Simulated data: simdat 
Repeat data   : repdat 
Formulas      :
	profit_365 = profit_sum < 36500

Variables:
            n_obs       mean         sd       min        p25     median
 profit_sum 1,000 5,644.9274 2,007.1863 -223.6821 4,372.1464 5,741.6681
        p75         max
 7,043.5267 11,255.2636

Logicals:
           TRUE (nr)   TRUE (prop)
profit_365       1,000           1

> plot(repdat)
> 
> 
> 
> 
> cleanEx()
> nameEx("rforest")
> ### * rforest
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: rforest
> ### Title: Random Forest using Ranger
> ### Aliases: rforest
> 
> ### ** Examples
> 
> rforest(titanic, "survived", c("pclass", "sex"), lev = "Yes") %>% summary()
Random Forest (Ranger)
Type                 : Classification
Data                 : titanic
Response variable    : survived
Level                : Yes in survived
Explanatory variables: pclass, sex 
Mtry                 : 
Number of trees      : 100 
Min node size        : 1 
Sample fraction      : 1 
Number of threads    : 12 
Nr obs               : 1,043 
OOB prediction error : 0.155 
> rforest(titanic, "survived", c("pclass", "sex")) %>% str()
List of 23
 $ check          : chr ""
 $ model          :List of 15
  ..$ predictions              : num [1:1043, 1:2] 0.851 0.363 0.839 0.334 0.841 ...
  .. ..- attr(*, "dimnames")=List of 2
  .. .. ..$ : NULL
  .. .. ..$ : chr [1:2] "Yes" "No"
  ..$ num.trees                : num 100
  ..$ num.independent.variables: num 2
  ..$ mtry                     : num 1
  ..$ min.node.size            : num 1
  ..$ variable.importance      : Named num [1:2] 0.0323 0.1141
  .. ..- attr(*, "names")= chr [1:2] "pclass" "sex"
  ..$ prediction.error         : num 0.155
  ..$ forest                   :List of 11
  .. ..$ num.trees                 : num 100
  .. ..$ child.nodeIDs             :List of 100
  .. .. ..$ :List of 2
  .. .. .. ..$ : num [1:3] 1 0 0
  .. .. .. ..$ : num [1:3] 2 0 0
  .. .. ..$ :List of 2
  .. .. .. ..$ : num [1:3] 1 0 0
  .. .. .. ..$ : num [1:3] 2 0 0
  .. .. ..$ :List of 2
  .. .. .. ..$ : num [1:7] 1 0 3 5 0 0 0
  .. .. .. ..$ : num [1:7] 2 0 4 6 0 0 0
  .. .. ..$ :List of 2
  .. .. .. ..$ : num [1:7] 1 3 0 5 0 0 0
  .. .. .. ..$ : num [1:7] 2 4 0 6 0 0 0
  .. .. ..$ :List of 2
  .. .. .. ..$ : num [1:11] 1 3 5 7 0 0 9 0 0 0 ...
  .. .. .. ..$ : num [1:11] 2 4 6 8 0 0 10 0 0 0 ...
  .. .. ..$ :List of 2
  .. .. .. ..$ : num [1:9] 1 3 5 7 0 0 0 0 0
  .. .. .. ..$ : num [1:9] 2 4 6 8 0 0 0 0 0
  .. .. ..$ :List of 2
  .. .. .. ..$ : num [1:11] 1 3 5 7 0 0 9 0 0 0 ...
  .. .. .. ..$ : num [1:11] 2 4 6 8 0 0 10 0 0 0 ...
  .. .. ..$ :List of 2
  .. .. .. ..$ : num [1:7] 1 3 0 5 0 0 0
  .. .. .. ..$ : num [1:7] 2 4 0 6 0 0 0
  .. .. ..$ :List of 2
  .. .. .. ..$ : num [1:7] 1 0 3 5 0 0 0
  .. .. .. ..$ : num [1:7] 2 0 4 6 0 0 0
  .. .. ..$ :List of 2
  .. .. .. ..$ : num [1:5] 1 3 0 0 0
  .. .. .. ..$ : num [1:5] 2 4 0 0 0
  .. .. ..$ :List of 2
  .. .. .. ..$ : num [1:5] 1 3 0 0 0
  .. .. .. ..$ : num [1:5] 2 4 0 0 0
  .. .. ..$ :List of 2
  .. .. .. ..$ : num [1:5] 1 0 3 0 0
  .. .. .. ..$ : num [1:5] 2 0 4 0 0
  .. .. ..$ :List of 2
  .. .. .. ..$ : num [1:9] 1 3 5 0 0 0 7 0 0
  .. .. .. ..$ : num [1:9] 2 4 6 0 0 0 8 0 0
  .. .. ..$ :List of 2
  .. .. .. ..$ : num [1:5] 1 3 0 0 0
  .. .. .. ..$ : num [1:5] 2 4 0 0 0
  .. .. ..$ :List of 2
  .. .. .. ..$ : num [1:11] 1 3 5 0 0 7 9 0 0 0 ...
  .. .. .. ..$ : num [1:11] 2 4 6 0 0 8 10 0 0 0 ...
  .. .. ..$ :List of 2
  .. .. .. ..$ : num [1:3] 1 0 0
  .. .. .. ..$ : num [1:3] 2 0 0
  .. .. ..$ :List of 2
  .. .. .. ..$ : num [1:9] 1 3 5 7 0 0 0 0 0
  .. .. .. ..$ : num [1:9] 2 4 6 8 0 0 0 0 0
  .. .. ..$ :List of 2
  .. .. .. ..$ : num [1:5] 1 3 0 0 0
  .. .. .. ..$ : num [1:5] 2 4 0 0 0
  .. .. ..$ :List of 2
  .. .. .. ..$ : num [1:9] 1 3 5 7 0 0 0 0 0
  .. .. .. ..$ : num [1:9] 2 4 6 8 0 0 0 0 0
  .. .. ..$ :List of 2
  .. .. .. ..$ : num [1:7] 1 0 3 0 5 0 0
  .. .. .. ..$ : num [1:7] 2 0 4 0 6 0 0
  .. .. ..$ :List of 2
  .. .. .. ..$ : num [1:11] 1 3 5 7 0 0 9 0 0 0 ...
  .. .. .. ..$ : num [1:11] 2 4 6 8 0 0 10 0 0 0 ...
  .. .. ..$ :List of 2
  .. .. .. ..$ : num [1:7] 1 3 0 5 0 0 0
  .. .. .. ..$ : num [1:7] 2 4 0 6 0 0 0
  .. .. ..$ :List of 2
  .. .. .. ..$ : num [1:7] 1 0 3 0 5 0 0
  .. .. .. ..$ : num [1:7] 2 0 4 0 6 0 0
  .. .. ..$ :List of 2
  .. .. .. ..$ : num [1:5] 1 0 3 0 0
  .. .. .. ..$ : num [1:5] 2 0 4 0 0
  .. .. ..$ :List of 2
  .. .. .. ..$ : num [1:3] 1 0 0
  .. .. .. ..$ : num [1:3] 2 0 0
  .. .. ..$ :List of 2
  .. .. .. ..$ : num [1:9] 1 0 3 5 7 0 0 0 0
  .. .. .. ..$ : num [1:9] 2 0 4 6 8 0 0 0 0
  .. .. ..$ :List of 2
  .. .. .. ..$ : num [1:7] 1 0 3 0 5 0 0
  .. .. .. ..$ : num [1:7] 2 0 4 0 6 0 0
  .. .. ..$ :List of 2
  .. .. .. ..$ : num [1:9] 1 3 5 7 0 0 0 0 0
  .. .. .. ..$ : num [1:9] 2 4 6 8 0 0 0 0 0
  .. .. ..$ :List of 2
  .. .. .. ..$ : num [1:7] 1 3 0 5 0 0 0
  .. .. .. ..$ : num [1:7] 2 4 0 6 0 0 0
  .. .. ..$ :List of 2
  .. .. .. ..$ : num [1:9] 1 3 0 5 7 0 0 0 0
  .. .. .. ..$ : num [1:9] 2 4 0 6 8 0 0 0 0
  .. .. ..$ :List of 2
  .. .. .. ..$ : num [1:7] 1 3 0 5 0 0 0
  .. .. .. ..$ : num [1:7] 2 4 0 6 0 0 0
  .. .. ..$ :List of 2
  .. .. .. ..$ : num [1:5] 1 3 0 0 0
  .. .. .. ..$ : num [1:5] 2 4 0 0 0
  .. .. ..$ :List of 2
  .. .. .. ..$ : num [1:9] 1 3 5 0 7 0 0 0 0
  .. .. .. ..$ : num [1:9] 2 4 6 0 8 0 0 0 0
  .. .. ..$ :List of 2
  .. .. .. ..$ : num [1:3] 1 0 0
  .. .. .. ..$ : num [1:3] 2 0 0
  .. .. ..$ :List of 2
  .. .. .. ..$ : num [1:7] 1 3 5 0 0 0 0
  .. .. .. ..$ : num [1:7] 2 4 6 0 0 0 0
  .. .. ..$ :List of 2
  .. .. .. ..$ : num [1:7] 1 3 5 0 0 0 0
  .. .. .. ..$ : num [1:7] 2 4 6 0 0 0 0
  .. .. ..$ :List of 2
  .. .. .. ..$ : num [1:9] 1 3 5 0 0 0 7 0 0
  .. .. .. ..$ : num [1:9] 2 4 6 0 0 0 8 0 0
  .. .. ..$ :List of 2
  .. .. .. ..$ : num [1:11] 1 3 5 7 9 0 0 0 0 0 ...
  .. .. .. ..$ : num [1:11] 2 4 6 8 10 0 0 0 0 0 ...
  .. .. ..$ :List of 2
  .. .. .. ..$ : num [1:7] 1 3 0 0 5 0 0
  .. .. .. ..$ : num [1:7] 2 4 0 0 6 0 0
  .. .. ..$ :List of 2
  .. .. .. ..$ : num [1:5] 1 3 0 0 0
  .. .. .. ..$ : num [1:5] 2 4 0 0 0
  .. .. ..$ :List of 2
  .. .. .. ..$ : num [1:5] 1 0 3 0 0
  .. .. .. ..$ : num [1:5] 2 0 4 0 0
  .. .. ..$ :List of 2
  .. .. .. ..$ : num [1:11] 1 3 5 7 9 0 0 0 0 0 ...
  .. .. .. ..$ : num [1:11] 2 4 6 8 10 0 0 0 0 0 ...
  .. .. ..$ :List of 2
  .. .. .. ..$ : num [1:5] 1 0 3 0 0
  .. .. .. ..$ : num [1:5] 2 0 4 0 0
  .. .. ..$ :List of 2
  .. .. .. ..$ : num [1:9] 1 3 5 7 0 0 0 0 0
  .. .. .. ..$ : num [1:9] 2 4 6 8 0 0 0 0 0
  .. .. ..$ :List of 2
  .. .. .. ..$ : num [1:3] 1 0 0
  .. .. .. ..$ : num [1:3] 2 0 0
  .. .. ..$ :List of 2
  .. .. .. ..$ : num [1:5] 1 3 0 0 0
  .. .. .. ..$ : num [1:5] 2 4 0 0 0
  .. .. ..$ :List of 2
  .. .. .. ..$ : num [1:7] 1 3 5 0 0 0 0
  .. .. .. ..$ : num [1:7] 2 4 6 0 0 0 0
  .. .. ..$ :List of 2
  .. .. .. ..$ : num [1:11] 1 3 5 7 9 0 0 0 0 0 ...
  .. .. .. ..$ : num [1:11] 2 4 6 8 10 0 0 0 0 0 ...
  .. .. ..$ :List of 2
  .. .. .. ..$ : num [1:7] 1 0 3 0 5 0 0
  .. .. .. ..$ : num [1:7] 2 0 4 0 6 0 0
  .. .. ..$ :List of 2
  .. .. .. ..$ : num [1:7] 1 0 3 0 5 0 0
  .. .. .. ..$ : num [1:7] 2 0 4 0 6 0 0
  .. .. ..$ :List of 2
  .. .. .. ..$ : num [1:5] 1 3 0 0 0
  .. .. .. ..$ : num [1:5] 2 4 0 0 0
  .. .. ..$ :List of 2
  .. .. .. ..$ : num [1:5] 1 0 3 0 0
  .. .. .. ..$ : num [1:5] 2 0 4 0 0
  .. .. ..$ :List of 2
  .. .. .. ..$ : num [1:7] 1 3 5 0 0 0 0
  .. .. .. ..$ : num [1:7] 2 4 6 0 0 0 0
  .. .. ..$ :List of 2
  .. .. .. ..$ : num [1:7] 1 0 3 5 0 0 0
  .. .. .. ..$ : num [1:7] 2 0 4 6 0 0 0
  .. .. ..$ :List of 2
  .. .. .. ..$ : num [1:9] 1 3 5 0 0 0 7 0 0
  .. .. .. ..$ : num [1:9] 2 4 6 0 0 0 8 0 0
  .. .. ..$ :List of 2
  .. .. .. ..$ : num [1:5] 1 3 0 0 0
  .. .. .. ..$ : num [1:5] 2 4 0 0 0
  .. .. ..$ :List of 2
  .. .. .. ..$ : num [1:11] 1 3 5 0 0 7 9 0 0 0 ...
  .. .. .. ..$ : num [1:11] 2 4 6 0 0 8 10 0 0 0 ...
  .. .. ..$ :List of 2
  .. .. .. ..$ : num [1:7] 1 0 3 5 0 0 0
  .. .. .. ..$ : num [1:7] 2 0 4 6 0 0 0
  .. .. ..$ :List of 2
  .. .. .. ..$ : num [1:11] 1 3 5 7 9 0 0 0 0 0 ...
  .. .. .. ..$ : num [1:11] 2 4 6 8 10 0 0 0 0 0 ...
  .. .. ..$ :List of 2
  .. .. .. ..$ : num [1:7] 1 0 3 0 5 0 0
  .. .. .. ..$ : num [1:7] 2 0 4 0 6 0 0
  .. .. ..$ :List of 2
  .. .. .. ..$ : num [1:9] 1 3 5 7 0 0 0 0 0
  .. .. .. ..$ : num [1:9] 2 4 6 8 0 0 0 0 0
  .. .. ..$ :List of 2
  .. .. .. ..$ : num [1:5] 1 3 0 0 0
  .. .. .. ..$ : num [1:5] 2 4 0 0 0
  .. .. ..$ :List of 2
  .. .. .. ..$ : num [1:9] 1 3 5 0 0 0 7 0 0
  .. .. .. ..$ : num [1:9] 2 4 6 0 0 0 8 0 0
  .. .. ..$ :List of 2
  .. .. .. ..$ : num [1:5] 1 3 0 0 0
  .. .. .. ..$ : num [1:5] 2 4 0 0 0
  .. .. ..$ :List of 2
  .. .. .. ..$ : num [1:3] 1 0 0
  .. .. .. ..$ : num [1:3] 2 0 0
  .. .. ..$ :List of 2
  .. .. .. ..$ : num [1:7] 1 3 0 0 5 0 0
  .. .. .. ..$ : num [1:7] 2 4 0 0 6 0 0
  .. .. ..$ :List of 2
  .. .. .. ..$ : num [1:3] 1 0 0
  .. .. .. ..$ : num [1:3] 2 0 0
  .. .. ..$ :List of 2
  .. .. .. ..$ : num [1:9] 1 3 5 0 0 7 0 0 0
  .. .. .. ..$ : num [1:9] 2 4 6 0 0 8 0 0 0
  .. .. ..$ :List of 2
  .. .. .. ..$ : num [1:7] 1 3 5 0 0 0 0
  .. .. .. ..$ : num [1:7] 2 4 6 0 0 0 0
  .. .. ..$ :List of 2
  .. .. .. ..$ : num [1:3] 1 0 0
  .. .. .. ..$ : num [1:3] 2 0 0
  .. .. ..$ :List of 2
  .. .. .. ..$ : num [1:7] 1 3 0 5 0 0 0
  .. .. .. ..$ : num [1:7] 2 4 0 6 0 0 0
  .. .. ..$ :List of 2
  .. .. .. ..$ : num [1:5] 1 3 0 0 0
  .. .. .. ..$ : num [1:5] 2 4 0 0 0
  .. .. ..$ :List of 2
  .. .. .. ..$ : num [1:7] 1 3 5 0 0 0 0
  .. .. .. ..$ : num [1:7] 2 4 6 0 0 0 0
  .. .. ..$ :List of 2
  .. .. .. ..$ : num [1:3] 1 0 0
  .. .. .. ..$ : num [1:3] 2 0 0
  .. .. ..$ :List of 2
  .. .. .. ..$ : num [1:3] 1 0 0
  .. .. .. ..$ : num [1:3] 2 0 0
  .. .. ..$ :List of 2
  .. .. .. ..$ : num [1:9] 1 3 5 0 7 0 0 0 0
  .. .. .. ..$ : num [1:9] 2 4 6 0 8 0 0 0 0
  .. .. ..$ :List of 2
  .. .. .. ..$ : num [1:7] 1 3 5 0 0 0 0
  .. .. .. ..$ : num [1:7] 2 4 6 0 0 0 0
  .. .. ..$ :List of 2
  .. .. .. ..$ : num [1:5] 1 0 3 0 0
  .. .. .. ..$ : num [1:5] 2 0 4 0 0
  .. .. ..$ :List of 2
  .. .. .. ..$ : num [1:9] 1 0 3 5 7 0 0 0 0
  .. .. .. ..$ : num [1:9] 2 0 4 6 8 0 0 0 0
  .. .. ..$ :List of 2
  .. .. .. ..$ : num [1:5] 1 0 3 0 0
  .. .. .. ..$ : num [1:5] 2 0 4 0 0
  .. .. ..$ :List of 2
  .. .. .. ..$ : num [1:9] 1 3 0 5 7 0 0 0 0
  .. .. .. ..$ : num [1:9] 2 4 0 6 8 0 0 0 0
  .. .. ..$ :List of 2
  .. .. .. ..$ : num [1:9] 1 3 5 7 0 0 0 0 0
  .. .. .. ..$ : num [1:9] 2 4 6 8 0 0 0 0 0
  .. .. ..$ :List of 2
  .. .. .. ..$ : num [1:7] 1 0 3 5 0 0 0
  .. .. .. ..$ : num [1:7] 2 0 4 6 0 0 0
  .. .. ..$ :List of 2
  .. .. .. ..$ : num [1:7] 1 0 3 0 5 0 0
  .. .. .. ..$ : num [1:7] 2 0 4 0 6 0 0
  .. .. ..$ :List of 2
  .. .. .. ..$ : num [1:7] 1 3 5 0 0 0 0
  .. .. .. ..$ : num [1:7] 2 4 6 0 0 0 0
  .. .. ..$ :List of 2
  .. .. .. ..$ : num [1:7] 1 3 0 5 0 0 0
  .. .. .. ..$ : num [1:7] 2 4 0 6 0 0 0
  .. .. ..$ :List of 2
  .. .. .. ..$ : num [1:9] 1 3 5 7 0 0 0 0 0
  .. .. .. ..$ : num [1:9] 2 4 6 8 0 0 0 0 0
  .. .. ..$ :List of 2
  .. .. .. ..$ : num [1:5] 1 3 0 0 0
  .. .. .. ..$ : num [1:5] 2 4 0 0 0
  .. .. ..$ :List of 2
  .. .. .. ..$ : num [1:7] 1 0 3 5 0 0 0
  .. .. .. ..$ : num [1:7] 2 0 4 6 0 0 0
  .. .. ..$ :List of 2
  .. .. .. ..$ : num [1:9] 1 3 5 0 0 0 7 0 0
  .. .. .. ..$ : num [1:9] 2 4 6 0 0 0 8 0 0
  .. .. ..$ :List of 2
  .. .. .. ..$ : num [1:3] 1 0 0
  .. .. .. ..$ : num [1:3] 2 0 0
  .. .. ..$ :List of 2
  .. .. .. ..$ : num [1:3] 1 0 0
  .. .. .. ..$ : num [1:3] 2 0 0
  .. .. ..$ :List of 2
  .. .. .. ..$ : num [1:5] 1 0 3 0 0
  .. .. .. ..$ : num [1:5] 2 0 4 0 0
  .. .. ..$ :List of 2
  .. .. .. ..$ : num [1:7] 1 3 0 5 0 0 0
  .. .. .. ..$ : num [1:7] 2 4 0 6 0 0 0
  .. .. ..$ :List of 2
  .. .. .. ..$ : num [1:3] 1 0 0
  .. .. .. ..$ : num [1:3] 2 0 0
  .. .. ..$ :List of 2
  .. .. .. ..$ : num [1:9] 1 0 3 5 7 0 0 0 0
  .. .. .. ..$ : num [1:9] 2 0 4 6 8 0 0 0 0
  .. .. ..$ :List of 2
  .. .. .. ..$ : num [1:7] 1 0 3 0 5 0 0
  .. .. .. ..$ : num [1:7] 2 0 4 0 6 0 0
  .. .. ..$ :List of 2
  .. .. .. ..$ : num [1:5] 1 3 0 0 0
  .. .. .. ..$ : num [1:5] 2 4 0 0 0
  .. .. ..$ :List of 2
  .. .. .. ..$ : num [1:9] 1 0 3 5 7 0 0 0 0
  .. .. .. ..$ : num [1:9] 2 0 4 6 8 0 0 0 0
  .. .. .. [list output truncated]
  .. ..$ split.varIDs              :List of 100
  .. .. ..$ : num [1:3] 1 0 0
  .. .. ..$ : num [1:3] 1 0 0
  .. .. ..$ : num [1:7] 0 0 0 1 0 0 0
  .. .. ..$ : num [1:7] 1 0 0 0 0 0 0
  .. .. ..$ : num [1:11] 1 0 0 0 0 0 0 0 0 0 ...
  .. .. ..$ : num [1:9] 1 0 0 0 0 0 0 0 0
  .. .. ..$ : num [1:11] 1 0 0 0 0 0 0 0 0 0 ...
  .. .. ..$ : num [1:7] 1 0 0 0 0 0 0
  .. .. ..$ : num [1:7] 0 0 0 1 0 0 0
  .. .. ..$ : num [1:5] 1 0 0 0 0
  .. .. ..$ : num [1:5] 0 1 0 0 0
  .. .. ..$ : num [1:5] 1 0 0 0 0
  .. .. ..$ : num [1:9] 0 1 0 0 0 0 1 0 0
  .. .. ..$ : num [1:5] 0 0 0 0 0
  .. .. ..$ : num [1:11] 0 1 0 0 0 1 1 0 0 0 ...
  .. .. ..$ : num [1:3] 1 0 0
  .. .. ..$ : num [1:9] 0 1 1 0 0 0 0 0 0
  .. .. ..$ : num [1:5] 1 0 0 0 0
  .. .. ..$ : num [1:9] 0 1 1 0 0 0 0 0 0
  .. .. ..$ : num [1:7] 0 0 1 0 0 0 0
  .. .. ..$ : num [1:11] 1 0 0 0 0 0 0 0 0 0 ...
  .. .. ..$ : num [1:7] 1 0 0 0 0 0 0
  .. .. ..$ : num [1:7] 0 0 0 0 1 0 0
  .. .. ..$ : num [1:5] 0 0 1 0 0
  .. .. ..$ : num [1:3] 1 0 0
  .. .. ..$ : num [1:9] 0 0 0 1 1 0 0 0 0
  .. .. ..$ : num [1:7] 1 0 0 0 0 0 0
  .. .. ..$ : num [1:9] 1 0 0 0 0 0 0 0 0
  .. .. ..$ : num [1:7] 1 0 0 0 0 0 0
  .. .. ..$ : num [1:9] 0 0 0 1 1 0 0 0 0
  .. .. ..$ : num [1:7] 1 0 0 0 0 0 0
  .. .. ..$ : num [1:5] 1 0 0 0 0
  .. .. ..$ : num [1:9] 0 0 1 0 1 0 0 0 0
  .. .. ..$ : num [1:3] 1 0 0
  .. .. ..$ : num [1:7] 0 1 1 0 0 0 0
  .. .. ..$ : num [1:7] 1 0 0 0 0 0 0
  .. .. ..$ : num [1:9] 0 1 0 0 0 0 1 0 0
  .. .. ..$ : num [1:11] 0 0 1 1 1 0 0 0 0 0 ...
  .. .. ..$ : num [1:7] 0 1 0 0 0 0 0
  .. .. ..$ : num [1:5] 0 0 0 0 0
  .. .. ..$ : num [1:5] 1 0 0 0 0
  .. .. ..$ : num [1:11] 0 0 1 1 1 0 0 0 0 0 ...
  .. .. ..$ : num [1:5] 0 0 0 0 0
  .. .. ..$ : num [1:9] 1 0 0 0 0 0 0 0 0
  .. .. ..$ : num [1:3] 1 0 0
  .. .. ..$ : num [1:5] 1 0 0 0 0
  .. .. ..$ : num [1:7] 1 0 0 0 0 0 0
  .. .. ..$ : num [1:11] 0 1 1 0 0 0 0 0 0 0 ...
  .. .. ..$ : num [1:7] 1 0 0 0 0 0 0
  .. .. ..$ : num [1:7] 1 0 0 0 0 0 0
  .. .. ..$ : num [1:5] 1 0 0 0 0
  .. .. ..$ : num [1:5] 1 0 0 0 0
  .. .. ..$ : num [1:7] 0 1 1 0 0 0 0
  .. .. ..$ : num [1:7] 0 0 1 0 0 0 0
  .. .. ..$ : num [1:9] 0 1 1 0 0 0 0 0 0
  .. .. ..$ : num [1:5] 1 0 0 0 0
  .. .. ..$ : num [1:11] 0 1 1 0 0 0 0 0 0 0 ...
  .. .. ..$ : num [1:7] 0 0 1 0 0 0 0
  .. .. ..$ : num [1:11] 0 0 1 1 1 0 0 0 0 0 ...
  .. .. ..$ : num [1:7] 1 0 0 0 0 0 0
  .. .. ..$ : num [1:9] 0 0 1 1 0 0 0 0 0
  .. .. ..$ : num [1:5] 0 1 0 0 0
  .. .. ..$ : num [1:9] 1 0 0 0 0 0 0 0 0
  .. .. ..$ : num [1:5] 1 0 0 0 0
  .. .. ..$ : num [1:3] 1 0 0
  .. .. ..$ : num [1:7] 0 0 0 0 1 0 0
  .. .. ..$ : num [1:3] 1 0 0
  .. .. ..$ : num [1:9] 0 1 1 0 0 0 0 0 0
  .. .. ..$ : num [1:7] 1 0 0 0 0 0 0
  .. .. ..$ : num [1:3] 1 0 0
  .. .. ..$ : num [1:7] 1 0 0 0 0 0 0
  .. .. ..$ : num [1:5] 0 1 0 0 0
  .. .. ..$ : num [1:7] 0 1 1 0 0 0 0
  .. .. ..$ : num [1:3] 1 0 0
  .. .. ..$ : num [1:3] 1 0 0
  .. .. ..$ : num [1:9] 0 1 1 0 0 0 0 0 0
  .. .. ..$ : num [1:7] 0 1 0 0 0 0 0
  .. .. ..$ : num [1:5] 1 0 0 0 0
  .. .. ..$ : num [1:9] 0 0 1 0 0 0 0 0 0
  .. .. ..$ : num [1:5] 1 0 0 0 0
  .. .. ..$ : num [1:9] 0 1 0 0 0 0 0 0 0
  .. .. ..$ : num [1:9] 1 0 0 0 0 0 0 0 0
  .. .. ..$ : num [1:7] 0 0 0 1 0 0 0
  .. .. ..$ : num [1:7] 0 0 0 0 1 0 0
  .. .. ..$ : num [1:7] 0 0 1 0 0 0 0
  .. .. ..$ : num [1:7] 0 0 0 1 0 0 0
  .. .. ..$ : num [1:9] 1 0 0 0 0 0 0 0 0
  .. .. ..$ : num [1:5] 1 0 0 0 0
  .. .. ..$ : num [1:7] 0 0 0 1 0 0 0
  .. .. ..$ : num [1:9] 1 0 0 0 0 0 0 0 0
  .. .. ..$ : num [1:3] 1 0 0
  .. .. ..$ : num [1:3] 1 0 0
  .. .. ..$ : num [1:5] 1 0 0 0 0
  .. .. ..$ : num [1:7] 0 1 0 0 0 0 0
  .. .. ..$ : num [1:3] 1 0 0
  .. .. ..$ : num [1:9] 0 0 0 1 1 0 0 0 0
  .. .. ..$ : num [1:7] 1 0 0 0 0 0 0
  .. .. ..$ : num [1:5] 1 0 0 0 0
  .. .. ..$ : num [1:9] 0 0 1 0 0 0 0 0 0
  .. .. .. [list output truncated]
  .. ..$ split.values              :List of 100
  .. .. ..$ : num [1:3] 1.5 0 0
  .. .. ..$ : num [1:3] 1.5 0 0
  .. .. ..$ : num [1:7] 1.5 0 2.5 1.5 0 0 0
  .. .. ..$ : num [1:7] 1.5 2.5 0 1.5 0 0 0
  .. .. ..$ : num [1:11] 1.5 2.5 1.5 1.5 0 0 2.5 0 0 0 ...
  .. .. ..$ : num [1:9] 1.5 2.5 1.5 1.5 0 0 0 0 0
  .. .. ..$ : num [1:11] 1.5 2.5 1.5 1.5 0 0 2.5 0 0 0 ...
  .. .. ..$ : num [1:7] 1.5 2.5 0 1.5 0 0 0
  .. .. ..$ : num [1:7] 1.5 0 2.5 1.5 0 0 0
  .. .. ..$ : num [1:5] 1.5 2.5 0 0 0
  .. .. ..$ : num [1:5] 2.5 1.5 0 0 0
  .. .. ..$ : num [1:5] 1.5 0 1.5 0 0
  .. .. ..$ : num [1:9] 1.5 1.5 2.5 0 0 0 1.5 0 0
  .. .. ..$ : num [1:5] 2.5 1.5 0 0 0
  .. .. ..$ : num [1:11] 1.5 1.5 2.5 0 0 1.5 1.5 0 0 0 ...
  .. .. ..$ : num [1:3] 1.5 0 0
  .. .. ..$ : num [1:9] 2.5 1.5 1.5 1.5 0 0 0 0 0
  .. .. ..$ : num [1:5] 1.5 2.5 0 0 0
  .. .. ..$ : num [1:9] 2.5 1.5 1.5 1.5 0 0 0 0 0
  .. .. ..$ : num [1:7] 1.5 0 1.5 0 2.5 0 0
  .. .. ..$ : num [1:11] 1.5 2.5 1.5 1.5 0 0 2.5 0 0 0 ...
  .. .. ..$ : num [1:7] 1.5 2.5 0 1.5 0 0 0
  .. .. ..$ : num [1:7] 1.5 0 2.5 0 1.5 0 0
  .. .. ..$ : num [1:5] 1.5 0 1.5 0 0
  .. .. ..$ : num [1:3] 1.5 0 0
  .. .. ..$ : num [1:9] 1.5 0 2.5 1.5 1.5 0 0 0 0
  .. .. ..$ : num [1:7] 1.5 0 1.5 0 2.5 0 0
  .. .. ..$ : num [1:9] 1.5 2.5 1.5 1.5 0 0 0 0 0
  .. .. ..$ : num [1:7] 1.5 2.5 0 1.5 0 0 0
  .. .. ..$ : num [1:9] 2.5 1.5 0 1.5 1.5 0 0 0 0
  .. .. ..$ : num [1:7] 1.5 2.5 0 1.5 0 0 0
  .. .. ..$ : num [1:5] 1.5 2.5 0 0 0
  .. .. ..$ : num [1:9] 2.5 1.5 1.5 0 1.5 0 0 0 0
  .. .. ..$ : num [1:3] 1.5 0 0
  .. .. ..$ : num [1:7] 2.5 1.5 1.5 0 0 0 0
  .. .. ..$ : num [1:7] 1.5 2.5 1.5 0 0 0 0
  .. .. ..$ : num [1:9] 1.5 1.5 2.5 0 0 0 1.5 0 0
  .. .. ..$ : num [1:11] 2.5 1.5 1.5 1.5 1.5 0 0 0 0 0 ...
  .. .. ..$ : num [1:7] 2.5 1.5 0 0 1.5 0 0
  .. .. ..$ : num [1:5] 2.5 1.5 0 0 0
  .. .. ..$ : num [1:5] 1.5 0 1.5 0 0
  .. .. ..$ : num [1:11] 2.5 1.5 1.5 1.5 1.5 0 0 0 0 0 ...
  .. .. ..$ : num [1:5] 1.5 0 2.5 0 0
  .. .. ..$ : num [1:9] 1.5 2.5 1.5 1.5 0 0 0 0 0
  .. .. ..$ : num [1:3] 1.5 0 0
  .. .. ..$ : num [1:5] 1.5 2.5 0 0 0
  .. .. ..$ : num [1:7] 1.5 2.5 1.5 0 0 0 0
  .. .. ..$ : num [1:11] 2.5 1.5 1.5 1.5 1.5 0 0 0 0 0 ...
  .. .. ..$ : num [1:7] 1.5 0 1.5 0 2.5 0 0
  .. .. ..$ : num [1:7] 1.5 0 1.5 0 2.5 0 0
  .. .. ..$ : num [1:5] 1.5 2.5 0 0 0
  .. .. ..$ : num [1:5] 1.5 0 1.5 0 0
  .. .. ..$ : num [1:7] 1.5 1.5 1.5 0 0 0 0
  .. .. ..$ : num [1:7] 1.5 0 1.5 2.5 0 0 0
  .. .. ..$ : num [1:9] 1.5 1.5 1.5 0 0 0 2.5 0 0
  .. .. ..$ : num [1:5] 1.5 2.5 0 0 0
  .. .. ..$ : num [1:11] 1.5 1.5 1.5 0 0 2.5 2.5 0 0 0 ...
  .. .. ..$ : num [1:7] 1.5 0 1.5 2.5 0 0 0
  .. .. ..$ : num [1:11] 2.5 1.5 1.5 1.5 1.5 0 0 0 0 0 ...
  .. .. ..$ : num [1:7] 1.5 0 1.5 0 2.5 0 0
  .. .. ..$ : num [1:9] 2.5 1.5 1.5 1.5 0 0 0 0 0
  .. .. ..$ : num [1:5] 2.5 1.5 0 0 0
  .. .. ..$ : num [1:9] 1.5 2.5 1.5 0 0 0 2.5 0 0
  .. .. ..$ : num [1:5] 1.5 2.5 0 0 0
  .. .. ..$ : num [1:3] 1.5 0 0
  .. .. ..$ : num [1:7] 2.5 1.5 0 0 1.5 0 0
  .. .. ..$ : num [1:3] 1.5 0 0
  .. .. ..$ : num [1:9] 1.5 1.5 1.5 0 0 2.5 0 0 0
  .. .. ..$ : num [1:7] 1.5 2.5 1.5 0 0 0 0
  .. .. ..$ : num [1:3] 1.5 0 0
  .. .. ..$ : num [1:7] 1.5 2.5 0 1.5 0 0 0
  .. .. ..$ : num [1:5] 2.5 1.5 0 0 0
  .. .. ..$ : num [1:7] 2.5 1.5 1.5 0 0 0 0
  .. .. ..$ : num [1:3] 1.5 0 0
  .. .. ..$ : num [1:3] 1.5 0 0
  .. .. ..$ : num [1:9] 2.5 1.5 1.5 0 1.5 0 0 0 0
  .. .. ..$ : num [1:7] 1.5 1.5 2.5 0 0 0 0
  .. .. ..$ : num [1:5] 1.5 0 1.5 0 0
  .. .. ..$ : num [1:9] 1.5 0 1.5 2.5 2.5 0 0 0 0
  .. .. ..$ : num [1:5] 1.5 0 1.5 0 0
  .. .. ..$ : num [1:9] 2.5 1.5 0 1.5 1.5 0 0 0 0
  .. .. ..$ : num [1:9] 1.5 2.5 1.5 1.5 0 0 0 0 0
  .. .. ..$ : num [1:7] 1.5 0 2.5 1.5 0 0 0
  .. .. ..$ : num [1:7] 1.5 0 2.5 0 1.5 0 0
  .. .. ..$ : num [1:7] 2.5 1.5 1.5 0 0 0 0
  .. .. ..$ : num [1:7] 2.5 1.5 0 1.5 0 0 0
  .. .. ..$ : num [1:9] 1.5 2.5 1.5 1.5 0 0 0 0 0
  .. .. ..$ : num [1:5] 1.5 2.5 0 0 0
  .. .. ..$ : num [1:7] 1.5 0 2.5 1.5 0 0 0
  .. .. ..$ : num [1:9] 1.5 2.5 1.5 0 0 0 2.5 0 0
  .. .. ..$ : num [1:3] 1.5 0 0
  .. .. ..$ : num [1:3] 1.5 0 0
  .. .. ..$ : num [1:5] 1.5 0 1.5 0 0
  .. .. ..$ : num [1:7] 2.5 1.5 0 1.5 0 0 0
  .. .. ..$ : num [1:3] 1.5 0 0
  .. .. ..$ : num [1:9] 1.5 0 2.5 1.5 1.5 0 0 0 0
  .. .. ..$ : num [1:7] 1.5 0 1.5 0 2.5 0 0
  .. .. ..$ : num [1:5] 1.5 2.5 0 0 0
  .. .. ..$ : num [1:9] 1.5 0 1.5 2.5 2.5 0 0 0 0
  .. .. .. [list output truncated]
  .. ..$ is.ordered                : logi [1:2] TRUE TRUE
  .. ..$ class.values              : num [1:2] 1 2
  .. ..$ terminal.class.counts     :List of 100
  .. .. ..$ :List of 3
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num [1:2] 0.743 0.257
  .. .. .. ..$ : num [1:2] 0.203 0.797
  .. .. ..$ :List of 3
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num [1:2] 0.778 0.222
  .. .. .. ..$ : num [1:2] 0.205 0.795
  .. .. ..$ :List of 7
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num [1:2] 0.662 0.338
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num [1:2] 0.258 0.742
  .. .. .. ..$ : num [1:2] 0.888 0.112
  .. .. .. ..$ : num [1:2] 0.143 0.857
  .. .. ..$ :List of 7
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num [1:2] 0.205 0.795
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num [1:2] 0.459 0.541
  .. .. .. ..$ : num [1:2] 0.9568 0.0432
  .. .. .. ..$ : num [1:2] 0.883 0.117
  .. .. ..$ :List of 11
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num [1:2] 0.438 0.562
  .. .. .. ..$ : num [1:2] 0.36 0.64
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num [1:2] 0.9724 0.0276
  .. .. .. ..$ : num [1:2] 0.872 0.128
  .. .. .. ..$ : num [1:2] 0.135 0.865
  .. .. .. ..$ : num [1:2] 0.13 0.87
  .. .. ..$ :List of 9
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num [1:2] 0.539 0.461
  .. .. .. ..$ : num [1:2] 0.422 0.578
  .. .. .. ..$ : num [1:2] 0.156 0.844
  .. .. .. ..$ : num [1:2] 0.9829 0.0171
  .. .. .. ..$ : num [1:2] 0.9032 0.0968
  .. .. ..$ :List of 11
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num [1:2] 0.41 0.59
  .. .. .. ..$ : num [1:2] 0.381 0.619
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num [1:2] 0.9524 0.0476
  .. .. .. ..$ : num [1:2] 0.883 0.117
  .. .. .. ..$ : num [1:2] 0.15 0.85
  .. .. .. ..$ : num [1:2] 0.163 0.837
  .. .. ..$ :List of 7
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num [1:2] 0.231 0.769
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num [1:2] 0.412 0.588
  .. .. .. ..$ : num [1:2] 0.9716 0.0284
  .. .. .. ..$ : num [1:2] 0.888 0.112
  .. .. ..$ :List of 7
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num [1:2] 0.703 0.297
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num [1:2] 0.28 0.72
  .. .. .. ..$ : num [1:2] 0.864 0.136
  .. .. .. ..$ : num [1:2] 0.125 0.875
  .. .. ..$ :List of 5
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num [1:2] 0.223 0.777
  .. .. .. ..$ : num [1:2] 0.9216 0.0784
  .. .. .. ..$ : num [1:2] 0.454 0.546
  .. .. ..$ :List of 5
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num [1:2] 0.232 0.768
  .. .. .. ..$ : num [1:2] 0.9571 0.0429
  .. .. .. ..$ : num [1:2] 0.22 0.78
  .. .. ..$ :List of 5
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num [1:2] 0.715 0.285
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num [1:2] 0.347 0.653
  .. .. .. ..$ : num [1:2] 0.159 0.841
  .. .. ..$ :List of 9
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num [1:2] 0.99315 0.00685
  .. .. .. ..$ : num [1:2] 0.42 0.58
  .. .. .. ..$ : num [1:2] 0.445 0.555
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num [1:2] 0.49 0.51
  .. .. .. ..$ : num [1:2] 0.14 0.86
  .. .. ..$ :List of 5
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num [1:2] 0.246 0.754
  .. .. .. ..$ : num [1:2] 0.587 0.413
  .. .. .. ..$ : num [1:2] 0.459 0.541
  .. .. ..$ :List of 11
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num [1:2] 0.9624 0.0376
  .. .. .. ..$ : num [1:2] 0.331 0.669
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num [1:2] 0.9111 0.0889
  .. .. .. ..$ : num [1:2] 0.13 0.87
  .. .. .. ..$ : num [1:2] 0.503 0.497
  .. .. .. ..$ : num [1:2] 0.16 0.84
  .. .. ..$ :List of 3
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num [1:2] 0.757 0.243
  .. .. .. ..$ : num [1:2] 0.199 0.801
  .. .. ..$ :List of 9
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num [1:2] 0.239 0.761
  .. .. .. ..$ : num [1:2] 0.486 0.514
  .. .. .. ..$ : num [1:2] 0.164 0.836
  .. .. .. ..$ : num [1:2] 0.9714 0.0286
  .. .. .. ..$ : num [1:2] 0.9123 0.0877
  .. .. ..$ :List of 5
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num [1:2] 0.233 0.767
  .. .. .. ..$ : num [1:2] 0.9312 0.0688
  .. .. .. ..$ : num [1:2] 0.544 0.456
  .. .. ..$ :List of 9
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num [1:2] 0.23 0.77
  .. .. .. ..$ : num [1:2] 0.476 0.524
  .. .. .. ..$ : num [1:2] 0.19 0.81
  .. .. .. ..$ : num [1:2] 0.9643 0.0357
  .. .. .. ..$ : num [1:2] 0.868 0.132
  .. .. ..$ :List of 7
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num [1:2] 0.689 0.311
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num [1:2] 0.654 0.346
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num [1:2] 0.131 0.869
  .. .. .. ..$ : num [1:2] 0.193 0.807
  .. .. ..$ :List of 11
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num [1:2] 0.506 0.494
  .. .. .. ..$ : num [1:2] 0.426 0.574
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num [1:2] 0.9754 0.0246
  .. .. .. ..$ : num [1:2] 0.86 0.14
  .. .. .. ..$ : num [1:2] 0.144 0.856
  .. .. .. ..$ : num [1:2] 0.175 0.825
  .. .. ..$ :List of 7
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num [1:2] 0.198 0.802
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num [1:2] 0.419 0.581
  .. .. .. ..$ : num [1:2] 0.9549 0.0451
  .. .. .. ..$ : num [1:2] 0.9 0.1
  .. .. ..$ :List of 7
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num [1:2] 0.713 0.287
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num [1:2] 0.405 0.595
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num [1:2] 0.48 0.52
  .. .. .. ..$ : num [1:2] 0.232 0.768
  .. .. ..$ :List of 5
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num [1:2] 0.617 0.383
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num [1:2] 0.604 0.396
  .. .. .. ..$ : num [1:2] 0.161 0.839
  .. .. ..$ :List of 3
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num [1:2] 0.717 0.283
  .. .. .. ..$ : num [1:2] 0.189 0.811
  .. .. ..$ :List of 9
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num [1:2] 0.611 0.389
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num [1:2] 0.9273 0.0727
  .. .. .. ..$ : num [1:2] 0.126 0.874
  .. .. .. ..$ : num [1:2] 0.457 0.543
  .. .. .. ..$ : num [1:2] 0.162 0.838
  .. .. ..$ :List of 7
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num [1:2] 0.795 0.205
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num [1:2] 0.31 0.69
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num [1:2] 0.112 0.888
  .. .. .. ..$ : num [1:2] 0.162 0.838
  .. .. ..$ :List of 9
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num [1:2] 0.541 0.459
  .. .. .. ..$ : num [1:2] 0.36 0.64
  .. .. .. ..$ : num [1:2] 0.163 0.837
  .. .. .. ..$ : num [1:2] 0.9854 0.0146
  .. .. .. ..$ : num [1:2] 0.893 0.107
  .. .. ..$ :List of 7
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num [1:2] 0.197 0.803
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num [1:2] 0.517 0.483
  .. .. .. ..$ : num [1:2] 0.9724 0.0276
  .. .. .. ..$ : num [1:2] 0.9099 0.0901
  .. .. ..$ :List of 9
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num [1:2] 0.271 0.729
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num [1:2] 0.9697 0.0303
  .. .. .. ..$ : num [1:2] 0.351 0.649
  .. .. .. ..$ : num [1:2] 0.9318 0.0682
  .. .. .. ..$ : num [1:2] 0.225 0.775
  .. .. ..$ :List of 7
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num [1:2] 0.221 0.779
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num [1:2] 0.507 0.493
  .. .. .. ..$ : num [1:2] 0.9481 0.0519
  .. .. .. ..$ : num [1:2] 0.883 0.117
  .. .. ..$ :List of 5
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num [1:2] 0.202 0.798
  .. .. .. ..$ : num [1:2] 0.9163 0.0837
  .. .. .. ..$ : num [1:2] 0.5 0.5
  .. .. ..$ :List of 9
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num [1:2] 0.654 0.346
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num [1:2] 0.441 0.559
  .. .. .. ..$ : num [1:2] 0.182 0.818
  .. .. .. ..$ : num [1:2] 0.895 0.105
  .. .. .. ..$ : num [1:2] 0.15 0.85
  .. .. ..$ :List of 3
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num [1:2] 0.787 0.213
  .. .. .. ..$ : num [1:2] 0.188 0.812
  .. .. ..$ :List of 7
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num [1:2] 0.891 0.109
  .. .. .. ..$ : num [1:2] 0.229 0.771
  .. .. .. ..$ : num [1:2] 0.447 0.553
  .. .. .. ..$ : num [1:2] 0.148 0.852
  .. .. ..$ :List of 7
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num [1:2] 0.9495 0.0505
  .. .. .. ..$ : num [1:2] 0.475 0.525
  .. .. .. ..$ : num [1:2] 0.352 0.648
  .. .. .. ..$ : num [1:2] 0.122 0.878
  .. .. ..$ :List of 9
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num [1:2] 0.975 0.025
  .. .. .. ..$ : num [1:2] 0.384 0.616
  .. .. .. ..$ : num [1:2] 0.388 0.612
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num [1:2] 0.444 0.556
  .. .. .. ..$ : num [1:2] 0.196 0.804
  .. .. ..$ :List of 11
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num [1:2] 0.439 0.561
  .. .. .. ..$ : num [1:2] 0.191 0.809
  .. .. .. ..$ : num [1:2] 0.9635 0.0365
  .. .. .. ..$ : num [1:2] 0.387 0.613
  .. .. .. ..$ : num [1:2] 0.9107 0.0893
  .. .. .. ..$ : num [1:2] 0.103 0.897
  .. .. ..$ :List of 7
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num [1:2] 0.282 0.718
  .. .. .. ..$ : num [1:2] 0.9116 0.0884
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num [1:2] 0.343 0.657
  .. .. .. ..$ : num [1:2] 0.133 0.867
  .. .. ..$ :List of 5
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num [1:2] 0.262 0.738
  .. .. .. ..$ : num [1:2] 0.629 0.371
  .. .. .. ..$ : num [1:2] 0.45 0.55
  .. .. ..$ :List of 5
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num [1:2] 0.744 0.256
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num [1:2] 0.344 0.656
  .. .. .. ..$ : num [1:2] 0.151 0.849
  .. .. ..$ :List of 11
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num [1:2] 0.504 0.496
  .. .. .. ..$ : num [1:2] 0.169 0.831
  .. .. .. ..$ : num [1:2] 0.963 0.037
  .. .. .. ..$ : num [1:2] 0.346 0.654
  .. .. .. ..$ : num [1:2] 0.9259 0.0741
  .. .. .. ..$ : num [1:2] 0.136 0.864
  .. .. ..$ :List of 5
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num [1:2] 0.655 0.345
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num [1:2] 0.45 0.55
  .. .. .. ..$ : num [1:2] 0.296 0.704
  .. .. ..$ :List of 9
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num [1:2] 0.413 0.587
  .. .. .. ..$ : num [1:2] 0.347 0.653
  .. .. .. ..$ : num [1:2] 0.129 0.871
  .. .. .. ..$ : num [1:2] 0.96 0.04
  .. .. .. ..$ : num [1:2] 0.9009 0.0991
  .. .. ..$ :List of 3
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num [1:2] 0.775 0.225
  .. .. .. ..$ : num [1:2] 0.194 0.806
  .. .. ..$ :List of 5
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num [1:2] 0.209 0.791
  .. .. .. ..$ : num [1:2] 0.9412 0.0588
  .. .. .. ..$ : num [1:2] 0.459 0.541
  .. .. ..$ :List of 7
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num [1:2] 0.9227 0.0773
  .. .. .. ..$ : num [1:2] 0.393 0.607
  .. .. .. ..$ : num [1:2] 0.348 0.652
  .. .. .. ..$ : num [1:2] 0.152 0.848
  .. .. ..$ :List of 11
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num [1:2] 0.403 0.597
  .. .. .. ..$ : num [1:2] 0.172 0.828
  .. .. .. ..$ : num [1:2] 0.9565 0.0435
  .. .. .. ..$ : num [1:2] 0.9464 0.0536
  .. .. .. ..$ : num [1:2] 0.386 0.614
  .. .. .. ..$ : num [1:2] 0.137 0.863
  .. .. ..$ :List of 7
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num [1:2] 0.772 0.228
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num [1:2] 0.39 0.61
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num [1:2] 0.139 0.861
  .. .. .. ..$ : num [1:2] 0.169 0.831
  .. .. ..$ :List of 7
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num [1:2] 0.768 0.232
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num [1:2] 0.389 0.611
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num [1:2] 0.149 0.851
  .. .. .. ..$ : num [1:2] 0.171 0.829
  .. .. ..$ :List of 5
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num [1:2] 0.198 0.802
  .. .. .. ..$ : num [1:2] 0.934 0.066
  .. .. .. ..$ : num [1:2] 0.422 0.578
  .. .. ..$ :List of 5
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num [1:2] 0.759 0.241
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num [1:2] 0.273 0.727
  .. .. .. ..$ : num [1:2] 0.147 0.853
  .. .. ..$ :List of 7
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num [1:2] 0.9516 0.0484
  .. .. .. ..$ : num [1:2] 0.386 0.614
  .. .. .. ..$ : num [1:2] 0.595 0.405
  .. .. .. ..$ : num [1:2] 0.164 0.836
  .. .. ..$ :List of 7
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num [1:2] 0.579 0.421
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num [1:2] 0.205 0.795
  .. .. .. ..$ : num [1:2] 0.899 0.101
  .. .. .. ..$ : num [1:2] 0.538 0.462
  .. .. ..$ :List of 9
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num [1:2] 0.9778 0.0222
  .. .. .. ..$ : num [1:2] 0.433 0.567
  .. .. .. ..$ : num [1:2] 0.65 0.35
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num [1:2] 0.111 0.889
  .. .. .. ..$ : num [1:2] 0.177 0.823
  .. .. ..$ :List of 5
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num [1:2] 0.201 0.799
  .. .. .. ..$ : num [1:2] 0.9191 0.0809
  .. .. .. ..$ : num [1:2] 0.455 0.545
  .. .. ..$ :List of 11
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num [1:2] 0.9688 0.0312
  .. .. .. ..$ : num [1:2] 0.403 0.597
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num [1:2] 0.9375 0.0625
  .. .. .. ..$ : num [1:2] 0.475 0.525
  .. .. .. ..$ : num [1:2] 0.132 0.868
  .. .. .. ..$ : num [1:2] 0.136 0.864
  .. .. ..$ :List of 7
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num [1:2] 0.687 0.313
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num [1:2] 0.169 0.831
  .. .. .. ..$ : num [1:2] 0.854 0.146
  .. .. .. ..$ : num [1:2] 0.486 0.514
  .. .. ..$ :List of 11
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num [1:2] 0.421 0.579
  .. .. .. ..$ : num [1:2] 0.14 0.86
  .. .. .. ..$ : num [1:2] 0.9661 0.0339
  .. .. .. ..$ : num [1:2] 0.396 0.604
  .. .. .. ..$ : num [1:2] 0.872 0.128
  .. .. .. ..$ : num [1:2] 0.127 0.873
  .. .. ..$ :List of 7
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num [1:2] 0.723 0.278
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num [1:2] 0.338 0.662
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num [1:2] 0.147 0.853
  .. .. .. ..$ : num [1:2] 0.126 0.874
  .. .. ..$ :List of 9
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num [1:2] 0.434 0.566
  .. .. .. ..$ : num [1:2] 0.404 0.596
  .. .. .. ..$ : num [1:2] 0.165 0.835
  .. .. .. ..$ : num [1:2] 0.9426 0.0574
  .. .. .. ..$ : num [1:2] 0.432 0.568
  .. .. ..$ :List of 5
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num [1:2] 0.249 0.751
  .. .. .. ..$ : num [1:2] 0.9419 0.0581
  .. .. .. ..$ : num [1:2] 0.23 0.77
  .. .. ..$ :List of 9
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num [1:2] 0.938 0.062
  .. .. .. ..$ : num [1:2] 0.47 0.53
  .. .. .. ..$ : num [1:2] 0.373 0.627
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num [1:2] 0.148 0.852
  .. .. .. ..$ : num [1:2] 0.168 0.832
  .. .. ..$ :List of 5
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num [1:2] 0.21 0.79
  .. .. .. ..$ : num [1:2] 0.9417 0.0583
  .. .. .. ..$ : num [1:2] 0.462 0.538
  .. .. ..$ :List of 3
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num [1:2] 0.769 0.231
  .. .. .. ..$ : num [1:2] 0.219 0.781
  .. .. ..$ :List of 7
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num [1:2] 0.249 0.751
  .. .. .. ..$ : num [1:2] 0.6 0.4
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num [1:2] 0.838 0.162
  .. .. .. ..$ : num [1:2] 0.159 0.841
  .. .. ..$ :List of 3
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num [1:2] 0.762 0.238
  .. .. .. ..$ : num [1:2] 0.224 0.776
  .. .. ..$ :List of 9
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num [1:2] 0.9643 0.0357
  .. .. .. ..$ : num [1:2] 0.262 0.738
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num [1:2] 0.16 0.84
  .. .. .. ..$ : num [1:2] 0.868 0.132
  .. .. .. ..$ : num [1:2] 0.487 0.513
  .. .. ..$ :List of 7
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num [1:2] 0.9265 0.0735
  .. .. .. ..$ : num [1:2] 0.478 0.522
  .. .. .. ..$ : num [1:2] 0.297 0.703
  .. .. .. ..$ : num [1:2] 0.166 0.834
  .. .. ..$ :List of 3
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num [1:2] 0.761 0.239
  .. .. .. ..$ : num [1:2] 0.206 0.794
  .. .. ..$ :List of 7
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num [1:2] 0.198 0.802
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num [1:2] 0.494 0.506
  .. .. .. ..$ : num [1:2] 0.9483 0.0517
  .. .. .. ..$ : num [1:2] 0.9029 0.0971
  .. .. ..$ :List of 5
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num [1:2] 0.266 0.734
  .. .. .. ..$ : num [1:2] 0.9582 0.0418
  .. .. .. ..$ : num [1:2] 0.283 0.717
  .. .. ..$ :List of 7
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num [1:2] 0.9438 0.0562
  .. .. .. ..$ : num [1:2] 0.243 0.757
  .. .. .. ..$ : num [1:2] 0.519 0.481
  .. .. .. ..$ : num [1:2] 0.187 0.813
  .. .. ..$ :List of 3
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num [1:2] 0.777 0.223
  .. .. .. ..$ : num [1:2] 0.221 0.779
  .. .. ..$ :List of 3
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num [1:2] 0.784 0.216
  .. .. .. ..$ : num [1:2] 0.222 0.778
  .. .. ..$ :List of 9
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num [1:2] 0.9213 0.0787
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num [1:2] 0.467 0.533
  .. .. .. ..$ : num [1:2] 0.142 0.858
  .. .. .. ..$ : num [1:2] 0.371 0.629
  .. .. .. ..$ : num [1:2] 0.168 0.832
  .. .. ..$ :List of 7
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num [1:2] 0.9597 0.0403
  .. .. .. ..$ : num [1:2] 0.353 0.647
  .. .. .. ..$ : num [1:2] 0.417 0.583
  .. .. .. ..$ : num [1:2] 0.245 0.755
  .. .. ..$ :List of 5
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num [1:2] 0.747 0.253
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num [1:2] 0.378 0.622
  .. .. .. ..$ : num [1:2] 0.165 0.835
  .. .. ..$ :List of 9
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num [1:2] 0.643 0.357
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num [1:2] 0.9072 0.0928
  .. .. .. ..$ : num [1:2] 0.53 0.47
  .. .. .. ..$ : num [1:2] 0.111 0.889
  .. .. .. ..$ : num [1:2] 0.168 0.832
  .. .. ..$ :List of 5
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num [1:2] 0.737 0.263
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num [1:2] 0.356 0.644
  .. .. .. ..$ : num [1:2] 0.155 0.845
  .. .. ..$ :List of 9
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num [1:2] 0.248 0.752
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num [1:2] 0.9469 0.0531
  .. .. .. ..$ : num [1:2] 0.878 0.122
  .. .. .. ..$ : num [1:2] 0.326 0.674
  .. .. .. ..$ : num [1:2] 0.143 0.857
  .. .. ..$ :List of 9
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num [1:2] 0.486 0.514
  .. .. .. ..$ : num [1:2] 0.298 0.702
  .. .. .. ..$ : num [1:2] 0.133 0.867
  .. .. .. ..$ : num [1:2] 0.9632 0.0368
  .. .. .. ..$ : num [1:2] 0.9107 0.0893
  .. .. ..$ :List of 7
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num [1:2] 0.681 0.319
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num [1:2] 0.253 0.747
  .. .. .. ..$ : num [1:2] 0.828 0.172
  .. .. .. ..$ : num [1:2] 0.115 0.885
  .. .. ..$ :List of 7
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num [1:2] 0.688 0.312
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num [1:2] 0.437 0.563
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num [1:2] 0.448 0.552
  .. .. .. ..$ : num [1:2] 0.181 0.819
  .. .. ..$ :List of 7
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num [1:2] 0.641 0.359
  .. .. .. ..$ : num [1:2] 0.46 0.54
  .. .. .. ..$ : num [1:2] 0.471 0.529
  .. .. .. ..$ : num [1:2] 0.184 0.816
  .. .. ..$ :List of 7
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num [1:2] 0.238 0.762
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num [1:2] 0.475 0.525
  .. .. .. ..$ : num [1:2] 0.9675 0.0325
  .. .. .. ..$ : num [1:2] 0.346 0.654
  .. .. ..$ :List of 9
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num [1:2] 0.42 0.58
  .. .. .. ..$ : num [1:2] 0.3 0.7
  .. .. .. ..$ : num [1:2] 0.165 0.835
  .. .. .. ..$ : num [1:2] 0.9677 0.0323
  .. .. .. ..$ : num [1:2] 0.9009 0.0991
  .. .. ..$ :List of 5
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num [1:2] 0.188 0.812
  .. .. .. ..$ : num [1:2] 0.9614 0.0386
  .. .. .. ..$ : num [1:2] 0.51 0.49
  .. .. ..$ :List of 7
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num [1:2] 0.633 0.367
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num [1:2] 0.271 0.729
  .. .. .. ..$ : num [1:2] 0.836 0.164
  .. .. .. ..$ : num [1:2] 0.147 0.853
  .. .. ..$ :List of 9
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num [1:2] 0.9062 0.0938
  .. .. .. ..$ : num [1:2] 0.494 0.506
  .. .. .. ..$ : num [1:2] 0.301 0.699
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num [1:2] 0.178 0.822
  .. .. .. ..$ : num [1:2] 0.181 0.819
  .. .. ..$ :List of 3
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num [1:2] 0.766 0.234
  .. .. .. ..$ : num [1:2] 0.198 0.802
  .. .. ..$ :List of 3
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num [1:2] 0.747 0.253
  .. .. .. ..$ : num [1:2] 0.222 0.778
  .. .. ..$ :List of 5
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num [1:2] 0.78 0.22
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num [1:2] 0.297 0.703
  .. .. .. ..$ : num [1:2] 0.169 0.831
  .. .. ..$ :List of 7
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num [1:2] 0.263 0.737
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num [1:2] 0.272 0.728
  .. .. .. ..$ : num [1:2] 0.942 0.058
  .. .. .. ..$ : num [1:2] 0.845 0.155
  .. .. ..$ :List of 3
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num [1:2] 0.774 0.226
  .. .. .. ..$ : num [1:2] 0.175 0.825
  .. .. ..$ :List of 9
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num [1:2] 0.64 0.36
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num [1:2] 0.898 0.102
  .. .. .. ..$ : num [1:2] 0.136 0.864
  .. .. .. ..$ : num [1:2] 0.429 0.571
  .. .. .. ..$ : num [1:2] 0.186 0.814
  .. .. ..$ :List of 7
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num [1:2] 0.73 0.27
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num [1:2] 0.404 0.596
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num [1:2] 0.132 0.868
  .. .. .. ..$ : num [1:2] 0.159 0.841
  .. .. ..$ :List of 5
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num [1:2] 0.19 0.81
  .. .. .. ..$ : num [1:2] 0.9214 0.0786
  .. .. .. ..$ : num [1:2] 0.5 0.5
  .. .. ..$ :List of 9
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num [1:2] 0.641 0.359
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num(0) 
  .. .. .. ..$ : num [1:2] 0.894 0.106
  .. .. .. ..$ : num [1:2] 0.453 0.547
  .. .. .. ..$ : num [1:2] 0.132 0.868
  .. .. .. ..$ : num [1:2] 0.149 0.851
  .. .. .. [list output truncated]
  .. ..$ levels                    : chr [1:2] "Yes" "No"
  .. ..$ independent.variable.names: chr [1:2] "pclass" "sex"
  .. ..$ treetype                  : chr "Probability estimation"
  .. ..$ covariate.levels          :List of 2
  .. .. ..$ pclass: chr [1:3] "1st" "2nd" "3rd"
  .. .. ..$ sex   : chr [1:2] "female" "male"
  .. ..- attr(*, "class")= chr "ranger.forest"
  ..$ splitrule                : chr "gini"
  ..$ treetype                 : chr "Probability estimation"
  ..$ call                     : language (function (formula = NULL, data = NULL, num.trees = 500, mtry = NULL, importance = "none",      write.forest = TR| __truncated__ ...
  ..$ importance.mode          : chr "permutation"
  ..$ num.samples              : int 1043
  ..$ replace                  : logi TRUE
  ..$ model                    : tibble [1,043 × 3] (S3: tbl_df/tbl/data.frame)
  .. ..$ survived: Factor w/ 2 levels "Yes","No": 1 1 2 2 2 1 1 2 1 2 ...
  .. ..$ pclass  : Factor w/ 3 levels "1st","2nd","3rd": 1 1 1 1 1 1 1 1 1 1 ...
  .. ..$ sex     : Factor w/ 2 levels "female","male": 1 2 1 2 1 2 1 2 1 2 ...
  .. ..- attr(*, "description")= chr "## Titanic\n\nThis dataset describes the survival status of individual passengers on the Titanic. The titanic d"| __truncated__
  ..- attr(*, "class")= chr "ranger"
 $ extra_args     : list()
 $ probability    : logi TRUE
 $ rv             : Factor w/ 2 levels "Yes","No": 1 1 2 2 2 1 1 2 1 2 ...
 $ not_vary       : chr(0) 
 $ df_name        : chr "titanic"
 $ vars           : chr [1:2] "pclass" "sex"
 $ rvar           : chr "survived"
 $ evar           : chr [1:2] "pclass" "sex"
 $ type           : chr "classification"
 $ lev            : chr "Yes"
 $ mtry           : NULL
 $ num.trees      : num 100
 $ min.node.size  : num 1
 $ sample.fraction: num 1
 $ replace        : logi TRUE
 $ num.threads    : num 12
 $ wts            : NULL
 $ seed           : chr NA
 $ data_filter    : chr ""
 $ arr            : chr ""
 $ rows           : NULL
 - attr(*, "class")= chr [1:3] "rforest" "model" "list"
> rforest(titanic, "survived", c("pclass", "sex"), max.depth = 1)
$check
[1] ""

$model
Ranger result

Call:
 (function (formula = NULL, data = NULL, num.trees = 500, mtry = NULL,      importance = "none", write.forest = TRUE, probability = FALSE,      min.node.size = NULL, min.bucket = NULL, max.depth = NULL,      replace = TRUE, sample.fraction = ifelse(replace, 1, 0.632),      case.weights = NULL, class.weights = NULL, splitrule = NULL,      num.random.splits = 1, alpha = 0.5, minprop = 0.1, split.select.weights = NULL,      always.split.variables = NULL, respect.unordered.factors = NULL,      scale.permutation.importance = FALSE, local.importance = FALSE,      regularization.factor = 1, regularization.usedepth = FALSE,      keep.inbag = FALSE, inbag = NULL, holdout = FALSE, quantreg = FALSE,      oob.error = TRUE, num.threads = NULL, save.memory = FALSE,      verbose = TRUE, seed = NULL, dependent.variable.name = NULL,      status.variable.name = NULL, classification = NULL, x = NULL,      y = NULL, ...)  {     if (length(list(...)) > 0) {         warning(paste("Unused arguments:", paste(names(list(...)),              collapse = ", ")))     }     snp.data <- as.matrix(0)     gwa.mode <- FALSE     if (is.null(data)) {         if (is.null(x) | is.null(y)) {             stop("Error: Either data or x and y is required.")         }     }     else {         if (inherits(data, "gwaa.data")) {             snp.names <- data@gtdata@snpnames             snp.data <- data@gtdata@gtps@.Data             data <- data@phdata             if ("id" %in% names(data)) {                 data$id <- NULL             }             gwa.mode <- TRUE             save.memory <- FALSE         }         if (is.null(formula)) {             if (is.null(dependent.variable.name)) {                 if (is.null(y) | is.null(x)) {                   stop("Error: Please give formula, dependent variable name or x/y.")                 }             }             else {                 if (is.null(status.variable.name)) {                   y <- data[, dependent.variable.name, drop = TRUE]                   x <- data[, !(colnames(data) %in% dependent.variable.name),                      drop = FALSE]                 }                 else {                   y <- survival::Surv(data[, dependent.variable.name],                      data[, status.variable.name])                   x <- data[, !(colnames(data) %in% c(dependent.variable.name,                      status.variable.name)), drop = FALSE]                 }             }         }         else {             formula <- formula(formula)             if (!inherits(formula, "formula")) {                 stop("Error: Invalid formula.")             }             data.selected <- parse.formula(formula, data, env = parent.frame())             y <- data.selected[, 1]             x <- data.selected[, -1, drop = FALSE]         }     }     if (inherits(x, "Matrix")) {         if (!inherits(x, "dgCMatrix")) {             stop("Error: Currently only sparse data of class 'dgCMatrix' supported.")         }         if (!is.null(formula)) {             stop("Error: Sparse matrices only supported with alternative interface. Use dependent.variable.name or x/y instead of formula.")         }     }     if (any(is.na(x))) {         offending_columns <- colnames(x)[colSums(is.na(x)) >              0]         stop("Missing data in columns: ", paste0(offending_columns,              collapse = ", "), ".", call. = FALSE)     }     if (any(is.na(y))) {         stop("Missing data in dependent variable.", call. = FALSE)     }     if (is.factor(y)) {         if (nlevels(y) != nlevels(droplevels(y))) {             dropped_levels <- setdiff(levels(y), levels(droplevels(y)))             warning("Dropped unused factor level(s) in dependent variable: ",                  paste0(dropped_levels, collapse = ", "), ".",                  call. = FALSE)         }     }     if (is.factor(y) || is.logical(y)) {         if (probability) {             treetype <- 9         }         else {             treetype <- 1         }     }     else if (is.numeric(y) && (is.null(ncol(y)) || ncol(y) ==          1)) {         if (!is.null(classification) && classification && !probability) {             treetype <- 1         }         else if (probability) {             treetype <- 9         }         else {             treetype <- 3         }     }     else if (inherits(y, "Surv") || is.data.frame(y) || is.matrix(y)) {         treetype <- 5     }     else {         stop("Error: Unsupported type of dependent variable.")     }     if (quantreg && treetype != 3) {         stop("Error: Quantile prediction implemented only for regression outcomes.")     }     independent.variable.names <- colnames(x)     if (is.null(respect.unordered.factors)) {         if (!is.null(splitrule) && splitrule == "extratrees") {             respect.unordered.factors <- "partition"         }         else {             respect.unordered.factors <- "ignore"         }     }     if (respect.unordered.factors == TRUE) {         respect.unordered.factors <- "order"     }     else if (respect.unordered.factors == FALSE) {         respect.unordered.factors <- "ignore"     }     covariate.levels <- NULL     if (!is.matrix(x) && !inherits(x, "Matrix") && ncol(x) >          0) {         character.idx <- sapply(x, is.character)         if (respect.unordered.factors == "order") {             ordered.idx <- sapply(x, is.ordered)             factor.idx <- sapply(x, is.factor)             recode.idx <- character.idx | (factor.idx & !ordered.idx)             if (any(recode.idx) & (importance == "impurity_corrected" ||                  importance == "impurity_unbiased")) {                 warning("Corrected impurity importance may not be unbiased for re-ordered factor levels. Consider setting respect.unordered.factors to 'ignore' or 'partition' or manually compute corrected importance.")             }             if (is.factor(y)) {                 num.y <- as.numeric(y)             }             else {                 num.y <- y             }             if (quantreg) {                 x_orig <- x             }             x[recode.idx] <- lapply(x[recode.idx], function(xx) {                 if (!is.factor(xx)) {                   xx <- as.factor(xx)                 }                 if (length(levels(xx)) == 1) {                   levels.ordered <- levels(xx)                 }                 else if (inherits(y, "Surv")) {                   levels.ordered <- largest.quantile(y ~ xx)                   levels.missing <- setdiff(levels(xx), levels.ordered)                   levels.ordered <- c(levels.missing, levels.ordered)                 }                 else if (is.factor(y) & nlevels(y) > 2) {                   levels.ordered <- pca.order(y = y, x = xx)                 }                 else {                   means <- sapply(levels(xx), function(y) {                     mean(num.y[xx == y])                   })                   levels.ordered <- as.character(levels(xx)[order(means)])                 }                 factor(xx, levels = levels.ordered, ordered = TRUE,                    exclude = NULL)             })         }         else {             x[character.idx] <- lapply(x[character.idx], factor)         }         if (any(sapply(x, is.factor))) {             covariate.levels <- lapply(x, levels)         }     }     if (gwa.mode) {         all.independent.variable.names <- c(independent.variable.names,              snp.names)     }     else {         all.independent.variable.names <- independent.variable.names     }     if (length(all.independent.variable.names) < 1) {         stop("Error: No covariates found.")     }     if (!is.numeric(num.trees) || num.trees < 1) {         stop("Error: Invalid value for num.trees.")     }     if (is.function(mtry)) {         nv <- length(all.independent.variable.names)         if (length(formals(mtry)) > 1) {             stop("Error: Given mtry function requires single argument (the number of independent variables in the model).")         }         mtry <- try(mtry(nv), silent = TRUE)         if (inherits(mtry, "try-error")) {             message("The mtry function produced the error: ",                  mtry)             stop("Error: mtry function evaluation resulted in an error.")         }         if (!is.numeric(mtry) || length(mtry) != 1) {             stop("Error: Given mtry function should return a single integer or numeric.")         }         else {             mtry <- as.integer(mtry)         }         if (mtry < 1 || mtry > nv) {             stop("Error: Given mtry function should evaluate to a value not less than 1 and not greater than the number of independent variables ( = ",                  nv, " )")         }     }     if (is.null(mtry)) {         mtry <- 0     }     else if (!is.numeric(mtry) || mtry < 0) {         stop("Error: Invalid value for mtry")     }     if (is.null(seed)) {         seed <- runif(1, 0, .Machine$integer.max)     }     if (!is.logical(keep.inbag)) {         stop("Error: Invalid value for keep.inbag")     }     if (is.null(num.threads)) {         num.threads = 0     }     else if (!is.numeric(num.threads) || num.threads < 0) {         stop("Error: Invalid value for num.threads")     }     if (is.null(min.node.size)) {         min.node.size <- 0     }     else if (!is.numeric(min.node.size) || min.node.size < 0) {         stop("Error: Invalid value for min.node.size")     }     if (is.null(min.bucket)) {         min.bucket <- 0     }     else if (!is.numeric(min.bucket) || min.bucket < 0) {         stop("Error: Invalid value for min.bucket")     }     if (is.null(max.depth)) {         max.depth <- 0     }     else if (!is.numeric(max.depth) || max.depth < 0) {         stop("Error: Invalid value for max.depth. Please give a positive integer.")     }     if (!is.numeric(sample.fraction)) {         stop("Error: Invalid value for sample.fraction. Please give a value in (0,1] or a vector of values in [0,1].")     }     if (length(sample.fraction) > 1) {         if (!(treetype %in% c(1, 9))) {             stop("Error: Invalid value for sample.fraction. Vector values only valid for classification forests.")         }         if (any(sample.fraction < 0) || any(sample.fraction >              1)) {             stop("Error: Invalid value for sample.fraction. Please give a value in (0,1] or a vector of values in [0,1].")         }         if (sum(sample.fraction) <= 0) {             stop("Error: Invalid value for sample.fraction. Sum of values must be >0.")         }         if (length(sample.fraction) != nlevels(y)) {             stop("Error: Invalid value for sample.fraction. Expecting ",                  nlevels(y), " values, provided ", length(sample.fraction),                  ".")         }         if (!replace & any(sample.fraction * length(y) > table(y))) {             idx <- which(sample.fraction * length(y) > table(y))[1]             stop("Error: Not enough samples in class ", names(idx),                  "; available: ", table(y)[idx], ", requested: ",                  (sample.fraction * length(y))[idx], ".")         }         if (!is.null(case.weights)) {             stop("Error: Combination of case.weights and class-wise sampling not supported.")         }         sample.fraction <- sample.fraction[as.numeric(unique(y))]     }     else {         if (sample.fraction <= 0 || sample.fraction > 1) {             stop("Error: Invalid value for sample.fraction. Please give a value in (0,1] or a vector of values in [0,1].")         }     }     if (all(regularization.factor == 1)) {         regularization.factor <- c(0, 0)         use.regularization.factor <- FALSE     }     else {         if (num.threads != 1) {             num.threads <- 1             warning("Parallelization deactivated (regularization used).")         }         use.regularization.factor <- TRUE     }     if (use.regularization.factor) {         if (max(regularization.factor) > 1) {             stop("The regularization coefficients cannot be greater than 1.")         }         if (max(regularization.factor) <= 0) {             stop("The regularization coefficients cannot be smaller than 0.")         }         p <- length(all.independent.variable.names)         if (length(regularization.factor) != 1 && length(regularization.factor) !=              p) {             stop("You must use 1 or p (the number of predictor variables)\n      regularization coefficients.")         }         if (length(regularization.factor) == 1) {             regularization.factor = rep(regularization.factor,                  p)         }     }     if (is.null(importance) || importance == "none") {         importance.mode <- 0     }     else if (importance == "impurity") {         importance.mode <- 1     }     else if (importance == "impurity_corrected" || importance ==          "impurity_unbiased") {         importance.mode <- 5     }     else if (importance == "permutation") {         if (local.importance) {             importance.mode <- 6         }         else if (scale.permutation.importance) {             importance.mode <- 2         }         else {             importance.mode <- 3         }     }     else {         stop("Error: Unknown importance mode.")     }     if (is.null(case.weights) || length(unique(case.weights)) ==          1) {         case.weights <- c(0, 0)         use.case.weights <- FALSE         if (holdout) {             stop("Error: Case weights required to use holdout mode.")         }     }     else {         use.case.weights <- TRUE         if (holdout) {             sample.fraction <- sample.fraction * mean(case.weights >                  0)         }         if (!replace && sum(case.weights > 0) < sample.fraction *              nrow(x)) {             stop("Error: Fewer non-zero case weights than observations to sample.")         }     }     if (is.null(inbag)) {         inbag <- list(c(0, 0))         use.inbag <- FALSE     }     else if (is.list(inbag)) {         use.inbag <- TRUE         if (use.case.weights) {             stop("Error: Combination of case.weights and inbag not supported.")         }         if (length(sample.fraction) > 1) {             stop("Error: Combination of class-wise sampling and inbag not supported.")         }         if (length(inbag) != num.trees) {             stop("Error: Size of inbag list not equal to number of trees.")         }     }     else {         stop("Error: Invalid inbag, expects list of vectors of size num.trees.")     }     if (is.null(class.weights)) {         class.weights <- rep(1, nlevels(y))     }     else {         if (!(treetype %in% c(1, 9))) {             stop("Error: Argument class.weights only valid for classification forests.")         }         if (!is.numeric(class.weights) || any(class.weights <              0)) {             stop("Error: Invalid value for class.weights. Please give a vector of non-negative values.")         }         if (length(class.weights) != nlevels(y)) {             stop("Error: Number of class weights not equal to number of classes.")         }         class.weights <- class.weights[unique(as.numeric(y))]     }     if (is.null(split.select.weights)) {         split.select.weights <- list(c(0, 0))         use.split.select.weights <- FALSE     }     else if (is.numeric(split.select.weights)) {         if (length(split.select.weights) != length(all.independent.variable.names)) {             stop("Error: Number of split select weights not equal to number of independent variables.")         }         split.select.weights <- list(split.select.weights)         use.split.select.weights <- TRUE     }     else if (is.list(split.select.weights)) {         if (length(split.select.weights) != num.trees) {             stop("Error: Size of split select weights list not equal to number of trees.")         }         use.split.select.weights <- TRUE     }     else {         stop("Error: Invalid split select weights.")     }     if (is.null(always.split.variables)) {         always.split.variables <- c("0", "0")         use.always.split.variables <- FALSE     }     else {         use.always.split.variables <- TRUE     }     if (is.null(splitrule)) {         if (treetype == 5) {             splitrule <- "logrank"         }         else if (treetype == 3) {             splitrule <- "variance"         }         else if (treetype %in% c(1, 9)) {             splitrule <- "gini"         }         splitrule.num <- 1     }     else if (splitrule == "logrank") {         if (treetype == 5) {             splitrule.num <- 1         }         else {             stop("Error: logrank splitrule applicable to survival data only.")         }     }     else if (splitrule == "gini") {         if (treetype %in% c(1, 9)) {             splitrule.num <- 1         }         else {             stop("Error: Gini splitrule applicable to classification data only.")         }     }     else if (splitrule == "variance") {         if (treetype == 3) {             splitrule.num <- 1         }         else {             stop("Error: variance splitrule applicable to regression data only.")         }     }     else if (splitrule == "auc" || splitrule == "C") {         if (treetype == 5) {             splitrule.num <- 2         }         else {             stop("Error: C index splitrule applicable to survival data only.")         }     }     else if (splitrule == "auc_ignore_ties" || splitrule == "C_ignore_ties") {         if (treetype == 5) {             splitrule.num <- 3         }         else {             stop("Error: C index splitrule applicable to survival data only.")         }     }     else if (splitrule == "maxstat") {         if (treetype == 5 || treetype == 3) {             splitrule.num <- 4         }         else {             stop("Error: maxstat splitrule applicable to regression or survival data only.")         }     }     else if (splitrule == "extratrees") {         splitrule.num <- 5     }     else if (splitrule == "beta") {         if (treetype == 3) {             splitrule.num <- 6         }         else {             stop("Error: beta splitrule applicable to regression data only.")         }         if (min(y) < 0 || max(y) > 1) {             stop("Error: beta splitrule applicable to regression data with outcome between 0 and 1 only.")         }     }     else if (splitrule == "hellinger") {         if (treetype %in% c(1, 9)) {             splitrule.num <- 7         }         else {             stop("Error: Hellinger splitrule only implemented for binary classification.")         }         if ((is.factor(y) && nlevels(y) > 2) || (length(unique(y)) >              2)) {             stop("Error: Hellinger splitrule only implemented for binary classification.")         }     }     else {         stop("Error: Unknown splitrule.")     }     if (alpha < 0 || alpha > 1) {         stop("Error: Invalid value for alpha, please give a value between 0 and 1.")     }     if (minprop < 0 || minprop > 0.5) {         stop("Error: Invalid value for minprop, please give a value between 0 and 0.5.")     }     if (splitrule == "maxstat" & use.regularization.factor) {         stop("Error: Regularization cannot be used with 'maxstat' splitrule.")     }     if (!is.numeric(num.random.splits) || num.random.splits <          1) {         stop("Error: Invalid value for num.random.splits, please give a positive integer.")     }     if (splitrule.num == 5 && save.memory && respect.unordered.factors ==          "partition") {         stop("Error: save.memory option not possible in extraTrees mode with unordered predictors.")     }     if (num.random.splits > 1 && splitrule.num != 5) {         warning("Argument 'num.random.splits' ignored if splitrule is not 'extratrees'.")     }     if (respect.unordered.factors == "partition") {         ordered.idx <- sapply(x, is.ordered)         factor.idx <- sapply(x, is.factor)         unordered.factor.variables <- independent.variable.names[factor.idx &              !ordered.idx]         if (length(unordered.factor.variables) > 0) {             use.unordered.factor.variables <- TRUE             num.levels <- sapply(x[, factor.idx & !ordered.idx,                  drop = FALSE], nlevels)             max.level.count <- .Machine$double.digits             if (max(num.levels) > max.level.count) {                 stop(paste("Too many levels in unordered categorical variable ",                    unordered.factor.variables[which.max(num.levels)],                    ". Only ", max.level.count, " levels allowed on this system. Consider using the 'order' option.",                    sep = ""))             }         }         else {             unordered.factor.variables <- c("0", "0")             use.unordered.factor.variables <- FALSE         }     }     else if (respect.unordered.factors == "ignore" || respect.unordered.factors ==          "order") {         unordered.factor.variables <- c("0", "0")         use.unordered.factor.variables <- FALSE     }     else {         stop("Error: Invalid value for respect.unordered.factors, please use 'order', 'partition' or 'ignore'.")     }     if (use.unordered.factor.variables && !is.null(splitrule)) {         if (splitrule == "maxstat") {             stop("Error: Unordered factor splitting not implemented for 'maxstat' splitting rule.")         }         else if (splitrule %in% c("C", "auc", "C_ignore_ties",              "auc_ignore_ties")) {             stop("Error: Unordered factor splitting not implemented for 'C' splitting rule.")         }         else if (splitrule == "beta") {             stop("Error: Unordered factor splitting not implemented for 'beta' splitting rule.")         }     }     if (respect.unordered.factors == "order") {         if (treetype == 3 && splitrule == "maxstat") {             warning("Warning: The 'order' mode for unordered factor handling with the 'maxstat' splitrule is experimental.")         }         if (gwa.mode & ((treetype %in% c(1, 9) & nlevels(y) >              2) | treetype == 5)) {             stop("Error: Ordering of SNPs currently only implemented for regression and binary outcomes.")         }     }     prediction.mode <- FALSE     predict.all <- FALSE     prediction.type <- 1     loaded.forest <- list()     if (inherits(x, "dgCMatrix")) {         sparse.x <- x         x <- matrix(c(0, 0))         use.sparse.data <- TRUE     }     else {         sparse.x <- Matrix(matrix(c(0, 0)))         use.sparse.data <- FALSE         if (is.data.frame(x)) {             x <- data.matrix(x)         }     }     if (treetype == 5) {         y.mat <- as.matrix(y)     }     else {         y.mat <- as.matrix(as.numeric(y))     }     if (respect.unordered.factors == "order") {         order.snps <- TRUE     }     else {         order.snps <- FALSE     }     if (treetype == 5) {         if (!all(y.mat[, 2] %in% 0:1)) {             stop("Error: Competing risks not supported yet. Use status=1 for events and status=0 for censoring.")         }     }     result <- rangerCpp(treetype, x, y.mat, independent.variable.names,          mtry, num.trees, verbose, seed, num.threads, write.forest,          importance.mode, min.node.size, min.bucket, split.select.weights,          use.split.select.weights, always.split.variables, use.always.split.variables,          prediction.mode, loaded.forest, snp.data, replace, probability,          unordered.factor.variables, use.unordered.factor.variables,          save.memory, splitrule.num, case.weights, use.case.weights,          class.weights, predict.all, keep.inbag, sample.fraction,          alpha, minprop, holdout, prediction.type, num.random.splits,          sparse.x, use.sparse.data, order.snps, oob.error, max.depth,          inbag, use.inbag, regularization.factor, use.regularization.factor,          regularization.usedepth)     if (length(result) == 0) {         stop("User interrupt or internal error.")     }     if (importance.mode != 0) {         names(result$variable.importance) <- all.independent.variable.names         if (importance.mode == 6) {             result$variable.importance.local <- matrix(result$variable.importance.local,                  byrow = FALSE, ncol = length(all.independent.variable.names),                  dimnames = list(rownames(data), all.independent.variable.names))         }     }     if (treetype == 1 && oob.error) {         if (is.factor(y)) {             result$predictions <- integer.to.factor(result$predictions,                  levels(y))         }         result$confusion.matrix <- table(y, result$predictions,              dnn = c("true", "predicted"), useNA = "ifany")     }     else if (treetype == 5 && oob.error) {         if (is.list(result$predictions)) {             result$predictions <- do.call(rbind, result$predictions)         }         if (is.vector(result$predictions)) {             result$predictions <- matrix(result$predictions,                  nrow = 1)         }         result$chf <- result$predictions         result$predictions <- NULL         result$survival <- exp(-result$chf)     }     else if (treetype == 9 && oob.error) {         if (is.list(result$predictions)) {             result$predictions <- do.call(rbind, result$predictions)         }         if (is.vector(result$predictions)) {             result$predictions <- matrix(result$predictions,                  nrow = 1)         }         colnames(result$predictions) <- unique(y)         if (is.factor(y)) {             result$predictions <- result$predictions[, levels(droplevels(y)),                  drop = FALSE]         }     }     result$splitrule <- splitrule     if (splitrule == "extratrees") {         result$num.random.splits <- num.random.splits     }     if (treetype == 1) {         result$treetype <- "Classification"     }     else if (treetype == 3) {         result$treetype <- "Regression"     }     else if (treetype == 5) {         result$treetype <- "Survival"     }     else if (treetype == 9) {         result$treetype <- "Probability estimation"     }     if (treetype == 3) {         result$r.squared <- 1 - result$prediction.error/var(y)     }     result$call <- sys.call()     result$importance.mode <- importance     if (use.sparse.data) {         result$num.samples <- nrow(sparse.x)     }     else {         result$num.samples <- nrow(x)     }     result$replace <- replace     if (write.forest) {         if (is.factor(y)) {             result$forest$levels <- levels(y)         }         result$forest$independent.variable.names <- independent.variable.names         result$forest$treetype <- result$treetype         class(result$forest) <- "ranger.forest"         if (!is.null(covariate.levels)) {             result$forest$covariate.levels <- covariate.levels         }     }     class(result) <- "ranger"     if (quantreg) {         if (respect.unordered.factors == "order" && !is.null(x_orig)) {             terminal.nodes <- predict(result, x_orig, type = "terminalNodes")$predictions +                  1         }         else {             terminal.nodes <- predict(result, x, type = "terminalNodes")$predictions +                  1         }         n <- result$num.samples         result$random.node.values <- matrix(nrow = max(terminal.nodes),              ncol = num.trees)         for (tree in 1:num.trees) {             idx <- sample(1:n, n)             result$random.node.values[terminal.nodes[idx, tree],                  tree] <- y[idx]         }         if (!is.null(result$inbag.counts)) {             inbag.counts <- simplify2array(result$inbag.counts)             random.node.values.oob <- randomObsNode(terminal.nodes,                  y, inbag.counts)             minoob <- min(rowSums(inbag.counts == 0))             if (minoob < 10) {                 stop("Error: Too few trees for out-of-bag quantile regression.")             }             result$random.node.values.oob <- t(apply(random.node.values.oob,                  1, function(x) {                   sample(x[!is.na(x)], minoob)                 }))         }     }     return(result) })(formula = survived ~ ., mtry = NULL, num.trees = 100, min.node.size = 1,      probability = TRUE, importance = "permutation", sample.fraction = 1,      replace = TRUE, num.threads = 12, case.weights = NULL, data = structure(list(         survived = structure(c(1L, 1L, 2L, 2L, 2L, 1L, 1L, 2L,          1L, 2L, 2L, 1L, 1L, 1L, 1L, 2L, 1L, 1L, 2L, 1L, 1L, 1L,          1L, 1L, 2L, 1L, 1L, 1L, 1L, 2L, 1L, 1L, 1L, 2L, 1L, 1L,          2L, 2L, 1L, 1L, 1L, 1L, 2L, 1L, 1L, 1L, 1L, 2L, 2L, 2L,          1L, 1L, 1L, 1L, 2L, 2L, 1L, 2L, 1L, 1L, 1L, 1L, 1L, 1L,          2L, 1L, 1L, 2L, 1L, 2L, 1L, 1L, 2L, 1L, 1L, 2L, 1L, 1L,          1L, 1L, 2L, 1L, 1L, 1L, 1L, 1L, 1L, 2L, 1L, 1L, 1L, 1L,          2L, 1L, 1L, 1L, 2L, 1L, 2L, 1L, 1L, 1L, 2L, 2L, 1L, 1L,          1L, 1L, 1L, 1L, 1L, 2L, 1L, 2L, 1L, 1L, 1L, 2L, 1L, 2L,          1L, 1L, 2L, 1L, 1L, 1L, 2L, 1L, 1L, 1L, 1L, 2L, 1L, 2L,          1L, 1L, 2L, 1L, 2L, 2L, 1L, 1L, 1L, 2L, 1L, 1L, 1L, 1L,          2L, 1L, 2L, 2L, 2L, 2L, 2L, 1L, 1L, 1L, 1L, 1L, 1L, 2L,          1L, 1L, 1L, 2L, 1L, 2L, 1L, 1L, 2L, 1L, 2L, 1L, 1L, 2L,          2L, 1L, 2L, 2L, 2L, 1L, 1L, 1L, 2L, 2L, 2L, 1L, 1L, 2L,          1L, 2L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, 1L, 2L, 1L, 1L, 1L,          2L, 1L, 2L, 2L, 1L, 2L, 1L, 1L, 2L, 2L, 1L, 2L, 1L, 2L,          1L, 1L, 1L, 2L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 2L, 1L, 1L,          1L, 2L, 2L, 2L, 1L, 1L, 1L, 1L, 1L, 1L, 2L, 1L, 2L, 1L,          1L, 1L, 2L, 2L, 2L, 1L, 1L, 2L, 1L, 1L, 2L, 1L, 1L, 1L,          2L, 2L, 2L, 1L, 2L, 1L, 2L, 2L, 2L, 1L, 1L, 2L, 1L, 2L,          2L, 1L, 1L, 2L, 1L, 1L, 2L, 1L, 2L, 1L, 2L, 2L, 2L, 2L,          1L, 2L, 2L, 2L, 1L, 2L, 2L, 1L, 1L, 2L, 1L, 1L, 1L, 1L,          1L, 1L, 2L, 2L, 2L, 2L, 1L, 1L, 2L, 1L, 1L, 2L, 1L, 2L,          2L, 1L, 1L, 1L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 1L, 1L,          2L, 1L, 2L, 2L, 1L, 1L, 2L, 1L, 1L, 2L, 2L, 1L, 2L, 1L,          1L, 2L, 2L, 2L, 1L, 2L, 2L, 1L, 1L, 2L, 1L, 2L, 1L, 1L,          1L, 2L, 2L, 2L, 2L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 1L, 2L,          2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 1L, 1L, 2L, 1L,          2L, 1L, 2L, 1L, 2L, 1L, 1L, 1L, 2L, 1L, 1L, 2L, 2L, 2L,          2L, 1L, 2L, 2L, 1L, 2L, 2L, 1L, 2L, 1L, 2L, 2L, 2L, 1L,          2L, 1L, 2L, 2L, 2L, 2L, 1L, 2L, 1L, 2L, 2L, 1L, 2L, 2L,          2L, 2L, 1L, 1L, 2L, 1L, 1L, 1L, 2L, 2L, 2L, 2L, 1L, 2L,          1L, 2L, 1L, 2L, 2L, 2L, 2L, 2L, 1L, 1L, 1L, 2L, 2L, 2L,          2L, 2L, 2L, 2L, 2L, 2L, 1L, 1L, 1L, 2L, 2L, 2L, 2L, 1L,          1L, 2L, 1L, 2L, 1L, 2L, 1L, 2L, 2L, 1L, 1L, 2L, 1L, 2L,          1L, 2L, 1L, 1L, 1L, 2L, 2L, 1L, 1L, 2L, 1L, 1L, 1L, 1L,          2L, 1L, 2L, 2L, 2L, 1L, 1L, 1L, 1L, 2L, 1L, 2L, 1L, 2L,          2L, 2L, 2L, 2L, 1L, 2L, 1L, 1L, 2L, 2L, 2L, 1L, 2L, 2L,          1L, 1L, 1L, 1L, 2L, 1L, 1L, 1L, 1L, 1L, 1L, 2L, 1L, 2L,          1L, 1L, 2L, 2L, 2L, 2L, 1L, 1L, 1L, 1L, 1L, 2L, 2L, 2L,          1L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 1L, 2L, 2L, 2L,          1L, 2L, 2L, 2L, 2L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,          2L, 2L, 1L, 2L, 1L, 2L, 1L, 1L, 1L, 2L, 2L, 2L, 2L, 2L,          1L, 2L, 2L, 1L, 1L, 1L, 1L, 1L, 1L, 2L, 2L, 1L, 2L, 2L,          2L, 2L, 2L, 2L, 2L, 2L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,          2L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 1L, 2L, 2L, 2L, 2L, 2L,          2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 1L, 2L, 2L, 2L, 2L, 1L,          2L, 2L, 2L, 2L, 1L, 2L, 2L, 2L, 2L, 2L, 1L, 2L, 2L, 2L,          2L, 2L, 2L, 2L, 1L, 1L, 1L, 2L, 2L, 1L, 2L, 2L, 2L, 1L,          2L, 2L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 1L,          1L, 1L, 2L, 1L, 1L, 2L, 1L, 2L, 2L, 2L, 1L, 2L, 2L, 2L,          2L, 1L, 1L, 2L, 1L, 2L, 2L, 1L, 2L, 1L, 2L, 2L, 2L, 2L,          2L, 2L, 2L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,          2L, 1L, 1L, 2L, 2L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,          2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 1L, 2L, 2L, 2L,          2L, 2L, 1L, 2L, 2L, 1L, 2L, 1L, 2L, 1L, 2L, 2L, 1L, 1L,          2L, 2L, 1L, 2L, 2L, 2L, 2L, 1L, 1L, 2L, 2L, 2L, 2L, 1L,          2L, 2L, 2L, 2L, 1L, 1L, 2L, 2L, 2L, 1L, 2L, 1L, 2L, 2L,          2L, 1L, 2L, 2L, 2L, 1L, 2L, 2L, 1L, 1L, 2L, 2L, 2L, 2L,          2L, 1L, 1L, 1L, 2L, 2L, 2L, 1L, 2L, 1L, 1L, 2L, 2L, 2L,          1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 1L, 2L, 2L, 2L,          2L, 1L, 2L, 1L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, 1L, 2L, 2L,          2L, 2L, 1L, 2L, 2L, 2L, 2L, 1L, 1L, 2L, 1L, 2L, 2L, 1L,          1L, 1L, 1L, 2L, 1L, 1L, 2L, 2L, 1L, 1L, 2L, 2L, 1L, 2L,          2L, 1L, 2L, 2L, 1L, 1L, 2L, 2L, 2L, 2L, 1L, 2L, 2L, 2L,          2L, 2L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,          2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 1L, 2L, 2L, 2L, 2L,          2L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,          2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 1L, 2L, 1L,          1L, 1L, 1L, 2L, 2L, 1L, 2L, 2L, 2L, 1L, 2L, 2L, 2L, 2L,          2L, 2L, 2L, 2L, 2L, 2L, 1L, 2L, 2L, 2L, 1L, 2L, 2L, 2L,          1L, 1L, 2L, 2L, 1L, 2L, 1L, 2L, 1L, 1L, 2L, 2L, 2L, 1L,          1L, 1L, 1L, 2L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,          2L, 2L, 2L, 2L, 2L, 1L, 2L, 2L, 2L, 2L, 2L, 1L, 2L, 2L,          2L, 1L, 2L, 2L, 2L, 2L, 2L, 1L, 2L, 2L, 2L, 2L, 2L), levels = c("Yes",          "No"), class = "factor"), pclass = structure(c(1L, 1L,          1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,          1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,          1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,          1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,          1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,          1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,          1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,          1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,          1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,          1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,          1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,          1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,          1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,          1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,          1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,          1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,          1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,          1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,          1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,          1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,          2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,          2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,          2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,          2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,          2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,          2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,          2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,          2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,          2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,          2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,          2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,          2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,          2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,          2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,          2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,          2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,          2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,          2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,          2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 3L, 3L, 3L, 3L, 3L,          3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L,          3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L,          3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L,          3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L,          3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L,          3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L,          3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L,          3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L,          3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L,          3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L,          3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L,          3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L,          3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L,          3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L,          3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L,          3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L,          3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L,          3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L,          3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L,          3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L,          3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L,          3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L,          3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L,          3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L,          3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L,          3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L,          3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L,          3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L,          3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L,          3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L,          3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L,          3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L,          3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L,          3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L,          3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L,          3L, 3L, 3L, 3L, 3L), levels = c("1st", "2nd", "3rd"), class = "factor"),          sex = structure(c(1L, 2L, 1L, 2L, 1L, 2L, 1L, 2L, 1L,          2L, 2L, 1L, 1L, 1L, 2L, 2L, 1L, 1L, 2L, 2L, 1L, 2L, 1L,          1L, 2L, 2L, 1L, 1L, 2L, 2L, 2L, 1L, 1L, 2L, 1L, 1L, 2L,          2L, 1L, 1L, 1L, 1L, 2L, 2L, 1L, 2L, 1L, 2L, 2L, 2L, 2L,          1L, 2L, 1L, 2L, 2L, 1L, 2L, 1L, 2L, 1L, 1L, 1L, 2L, 2L,          1L, 1L, 2L, 1L, 2L, 1L, 1L, 2L, 1L, 1L, 2L, 1L, 2L, 2L,          1L, 2L, 1L, 2L, 1L, 2L, 2L, 1L, 2L, 1L, 1L, 1L, 2L, 2L,          1L, 1L, 1L, 1L, 2L, 2L, 1L, 1L, 1L, 2L, 2L, 1L, 1L, 2L,          2L, 1L, 2L, 1L, 2L, 1L, 2L, 1L, 1L, 1L, 2L, 2L, 2L, 2L,          1L, 2L, 1L, 2L, 1L, 2L, 2L, 1L, 2L, 1L, 2L, 1L, 2L, 2L,          1L, 2L, 1L, 2L, 2L, 1L, 1L, 1L, 2L, 1L, 2L, 2L, 1L, 1L,          2L, 2L, 2L, 2L, 2L, 2L, 2L, 1L, 1L, 1L, 1L, 2L, 2L, 1L,          1L, 1L, 2L, 1L, 2L, 1L, 1L, 2L, 1L, 2L, 1L, 1L, 2L, 2L,          2L, 2L, 2L, 2L, 1L, 1L, 2L, 2L, 2L, 2L, 1L, 1L, 2L, 1L,          2L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, 1L, 2L, 1L, 1L, 2L, 2L,          1L, 2L, 2L, 1L, 2L, 2L, 1L, 2L, 2L, 1L, 2L, 1L, 2L, 2L,          1L, 1L, 2L, 1L, 1L, 1L, 1L, 2L, 1L, 2L, 2L, 1L, 2L, 2L,          2L, 2L, 2L, 1L, 2L, 1L, 2L, 2L, 1L, 2L, 2L, 2L, 2L, 1L,          1L, 2L, 1L, 2L, 1L, 1L, 2L, 1L, 2L, 2L, 2L, 1L, 2L, 2L,          2L, 2L, 1L, 2L, 1L, 2L, 2L, 2L, 1L, 1L, 2L, 1L, 2L, 2L,          1L, 1L, 2L, 2L, 1L, 2L, 1L, 2L, 1L, 2L, 2L, 2L, 2L, 1L,          2L, 2L, 2L, 1L, 2L, 2L, 2L, 1L, 2L, 2L, 1L, 1L, 1L, 2L,          1L, 2L, 2L, 2L, 2L, 1L, 1L, 2L, 1L, 1L, 2L, 1L, 2L, 2L,          1L, 2L, 2L, 1L, 1L, 2L, 1L, 2L, 2L, 2L, 1L, 1L, 1L, 2L,          1L, 2L, 2L, 2L, 1L, 2L, 1L, 1L, 1L, 2L, 2L, 2L, 1L, 1L,          2L, 2L, 2L, 1L, 2L, 2L, 1L, 1L, 2L, 2L, 2L, 1L, 1L, 1L,          2L, 2L, 2L, 2L, 1L, 2L, 2L, 1L, 2L, 2L, 2L, 1L, 2L, 2L,          2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 1L, 2L, 1L, 2L,          2L, 2L, 1L, 2L, 1L, 1L, 1L, 2L, 1L, 1L, 2L, 2L, 2L, 1L,          1L, 2L, 2L, 1L, 2L, 2L, 1L, 2L, 2L, 2L, 1L, 2L, 1L, 2L,          1L, 2L, 2L, 2L, 2L, 1L, 2L, 1L, 1L, 2L, 1L, 2L, 2L, 1L,          2L, 1L, 1L, 2L, 1L, 1L, 1L, 2L, 2L, 2L, 2L, 1L, 1L, 2L,          2L, 1L, 2L, 2L, 2L, 2L, 2L, 1L, 1L, 2L, 2L, 2L, 2L, 2L,          2L, 2L, 2L, 2L, 2L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 1L,          2L, 2L, 2L, 2L, 2L, 1L, 2L, 2L, 1L, 1L, 2L, 1L, 2L, 2L,          2L, 1L, 1L, 1L, 2L, 2L, 1L, 1L, 2L, 2L, 2L, 1L, 1L, 2L,          1L, 2L, 2L, 2L, 1L, 1L, 1L, 1L, 2L, 1L, 2L, 1L, 2L, 2L,          2L, 2L, 2L, 1L, 2L, 1L, 1L, 2L, 1L, 2L, 1L, 2L, 2L, 1L,          1L, 1L, 1L, 2L, 1L, 2L, 1L, 1L, 1L, 1L, 2L, 1L, 2L, 2L,          1L, 1L, 2L, 2L, 2L, 1L, 1L, 2L, 2L, 1L, 2L, 2L, 1L, 2L,          1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 1L, 2L, 1L, 1L, 1L,          1L, 1L, 1L, 2L, 2L, 2L, 1L, 2L, 2L, 2L, 1L, 2L, 2L, 2L,          2L, 2L, 2L, 1L, 2L, 2L, 1L, 1L, 2L, 2L, 1L, 2L, 2L, 1L,          2L, 2L, 1L, 1L, 1L, 1L, 1L, 1L, 2L, 2L, 2L, 1L, 1L, 1L,          2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 1L, 2L, 1L, 2L,          1L, 1L, 2L, 2L, 2L, 2L, 1L, 2L, 2L, 1L, 1L, 1L, 2L, 2L,          2L, 2L, 1L, 2L, 2L, 2L, 2L, 1L, 1L, 2L, 2L, 2L, 2L, 2L,          2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 1L, 1L, 2L, 2L, 2L,          2L, 2L, 2L, 2L, 2L, 1L, 2L, 2L, 1L, 2L, 2L, 2L, 2L, 1L,          2L, 1L, 2L, 2L, 2L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 1L,          2L, 2L, 2L, 1L, 2L, 1L, 2L, 2L, 2L, 1L, 2L, 2L, 2L, 2L,          2L, 1L, 1L, 1L, 2L, 2L, 2L, 2L, 1L, 2L, 2L, 2L, 2L, 2L,          2L, 2L, 1L, 2L, 2L, 2L, 2L, 1L, 1L, 2L, 2L, 1L, 2L, 2L,          1L, 2L, 2L, 2L, 1L, 2L, 2L, 2L, 2L, 1L, 1L, 2L, 2L, 1L,          2L, 2L, 2L, 2L, 2L, 2L, 1L, 1L, 2L, 1L, 2L, 2L, 2L, 2L,          2L, 1L, 2L, 2L, 2L, 1L, 1L, 1L, 1L, 2L, 1L, 1L, 1L, 2L,          2L, 1L, 2L, 2L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,          2L, 2L, 2L, 2L, 1L, 2L, 2L, 2L, 1L, 2L, 2L, 2L, 1L, 1L,          2L, 2L, 2L, 2L, 2L, 2L, 2L, 1L, 2L, 2L, 2L, 2L, 1L, 2L,          1L, 2L, 1L, 1L, 2L, 1L, 2L, 1L, 1L, 2L, 2L, 2L, 2L, 2L,          2L, 2L, 2L, 2L, 2L, 1L, 1L, 2L, 1L, 2L, 2L, 2L, 1L, 2L,          2L, 2L, 1L, 2L, 2L, 2L, 2L, 1L, 2L, 2L, 1L, 1L, 2L, 1L,          2L, 2L, 2L, 2L, 2L, 2L, 2L, 1L, 2L, 1L, 2L, 2L, 1L, 1L,          2L, 1L, 2L, 2L, 1L, 1L, 2L, 1L, 1L, 2L, 2L, 2L, 2L, 2L,          1L, 2L, 2L, 1L, 2L, 2L, 2L, 1L, 2L, 2L, 2L, 1L, 1L, 2L,          2L, 1L, 2L, 2L, 1L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, 1L, 2L,          2L, 2L, 2L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, 1L, 2L, 2L, 1L,          2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 1L, 1L, 2L, 2L,          1L, 1L, 2L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 1L, 2L, 1L, 1L,          1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 1L, 2L, 2L, 1L, 1L, 2L,          1L, 2L, 2L, 2L, 2L, 1L, 2L, 2L, 1L, 2L, 2L, 1L, 1L, 2L,          2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 1L, 2L, 2L, 2L, 2L, 2L,          1L, 1L, 2L, 1L, 1L, 2L, 2L, 1L, 2L, 1L, 2L, 2L, 2L, 2L,          1L, 2L, 2L, 1L, 2L, 2L, 1L, 2L, 2L, 2L, 1L, 2L, 2L, 2L,          1L, 2L, 2L, 2L, 2L, 2L, 1L, 2L, 1L, 2L, 2L, 2L), levels = c("female",          "male"), class = "factor")), row.names = c(NA, -1043L     ), description = "## Titanic\n\nThis dataset describes the survival status of individual passengers on the Titanic. The titanic data frame does not contain information from the crew, but it does contain actual ages of (some of) the passengers. The principal source for data about Titanic passengers is the Encyclopedia Titanica. One of the original sources is Eaton & Haas (1994) Titanic: Triumph and Tragedy, Patrick Stephens Ltd, which includes a passenger list created by many researchers and edited by Michael A. Findlay.\n\n## Variables\n\n* survival - Survival (Yes, No)\n* pclass - Passenger Class (1st, 2nd, 3rd)\n* sex - Sex (female, male)\n* age - Age in years\n* sibsp - Number of Siblings/Spouses Aboard\n* parch - Number of Parents/Children Aboard\n* fare - Passenger Fare\n* name - Name\n* cabin - Cabin\n* embarked - Port of Embarkation (Cherbourg, Queenstown, Southampton)\n\n##  Notes\n\n`pclass` is a proxy for socio-economic status (SES) 1st ~ Upper; 2nd ~ Middle; 3rd ~ Lower\n\nAge is in Years; Fractional if Age less than One (1). If the Age is Estimated, it is in the form xx.5\n\nWith respect to the family relation variables (i.e. sibsp and parch) some relations were ignored.  The following are the definitions used for sibsp and parch.\n\nSibling:  Brother, Sister, Stepbrother, or Stepsister of Passenger Aboard Titanic\nSpouse:   Husband or Wife of Passenger Aboard Titanic (Mistresses and Fiances Ignored)\nParent:   Mother or Father of Passenger Aboard Titanic\nChild:    Son, Daughter, Stepson, or Stepdaughter of Passenger Aboard Titanic\n\nOther family relatives excluded from this study include cousins, nephews/nieces, aunts/uncles, and in-laws. Some children travelled only with a nanny, therefore parch=0 for them.  As well, some travelled with very close friends or neighbors in a village, however, the definitions do not support such relations.\n\nNote: Missing values and the `ticket` variable were removed from the data\n\n## Related reading\n\n<a href=\"http://phys.org/news/2012-07-shipwrecks-men-survive.html\" target=\"_blank\">In shipwrecks, men more likely to survive</a>", class = c("tbl_df",      "tbl", "data.frame")), max.depth = 1) 

Type:                             Probability estimation 
Number of trees:                  100 
Sample size:                      1043 
Number of independent variables:  2 
Mtry:                             1 
Target node size:                 1 
Variable importance mode:         permutation 
Splitrule:                        gini 
OOB prediction error (Brier s.):  0.1786078 

$extra_args
$extra_args$max.depth
[1] 1


$probability
[1] TRUE

$rv
   [1] Yes Yes No  No  No  Yes Yes No  Yes No  No  Yes Yes Yes Yes No  Yes Yes
  [19] No  Yes Yes Yes Yes Yes No  Yes Yes Yes Yes No  Yes Yes Yes No  Yes Yes
  [37] No  No  Yes Yes Yes Yes No  Yes Yes Yes Yes No  No  No  Yes Yes Yes Yes
  [55] No  No  Yes No  Yes Yes Yes Yes Yes Yes No  Yes Yes No  Yes No  Yes Yes
  [73] No  Yes Yes No  Yes Yes Yes Yes No  Yes Yes Yes Yes Yes Yes No  Yes Yes
  [91] Yes Yes No  Yes Yes Yes No  Yes No  Yes Yes Yes No  No  Yes Yes Yes Yes
 [109] Yes Yes Yes No  Yes No  Yes Yes Yes No  Yes No  Yes Yes No  Yes Yes Yes
 [127] No  Yes Yes Yes Yes No  Yes No  Yes Yes No  Yes No  No  Yes Yes Yes No 
 [145] Yes Yes Yes Yes No  Yes No  No  No  No  No  Yes Yes Yes Yes Yes Yes No 
 [163] Yes Yes Yes No  Yes No  Yes Yes No  Yes No  Yes Yes No  No  Yes No  No 
 [181] No  Yes Yes Yes No  No  No  Yes Yes No  Yes No  Yes Yes No  No  No  No 
 [199] No  Yes No  Yes Yes Yes No  Yes No  No  Yes No  Yes Yes No  No  Yes No 
 [217] Yes No  Yes Yes Yes No  Yes Yes Yes Yes Yes Yes Yes No  Yes Yes Yes No 
 [235] No  No  Yes Yes Yes Yes Yes Yes No  Yes No  Yes Yes Yes No  No  No  Yes
 [253] Yes No  Yes Yes No  Yes Yes Yes No  No  No  Yes No  Yes No  No  No  Yes
 [271] Yes No  Yes No  No  Yes Yes No  Yes Yes No  Yes No  Yes No  No  No  No 
 [289] Yes No  No  No  Yes No  No  Yes Yes No  Yes Yes Yes Yes Yes Yes No  No 
 [307] No  No  Yes Yes No  Yes Yes No  Yes No  No  Yes Yes Yes Yes Yes No  No 
 [325] No  No  No  No  Yes Yes No  Yes No  No  Yes Yes No  Yes Yes No  No  Yes
 [343] No  Yes Yes No  No  No  Yes No  No  Yes Yes No  Yes No  Yes Yes Yes No 
 [361] No  No  No  Yes No  No  No  No  No  No  Yes No  No  No  No  No  No  No 
 [379] No  No  No  No  Yes Yes No  Yes No  Yes No  Yes No  Yes Yes Yes No  Yes
 [397] Yes No  No  No  No  Yes No  No  Yes No  No  Yes No  Yes No  No  No  Yes
 [415] No  Yes No  No  No  No  Yes No  Yes No  No  Yes No  No  No  No  Yes Yes
 [433] No  Yes Yes Yes No  No  No  No  Yes No  Yes No  Yes No  No  No  No  No 
 [451] Yes Yes Yes No  No  No  No  No  No  No  No  No  Yes Yes Yes No  No  No 
 [469] No  Yes Yes No  Yes No  Yes No  Yes No  No  Yes Yes No  Yes No  Yes No 
 [487] Yes Yes Yes No  No  Yes Yes No  Yes Yes Yes Yes No  Yes No  No  No  Yes
 [505] Yes Yes Yes No  Yes No  Yes No  No  No  No  No  Yes No  Yes Yes No  No 
 [523] No  Yes No  No  Yes Yes Yes Yes No  Yes Yes Yes Yes Yes Yes No  Yes No 
 [541] Yes Yes No  No  No  No  Yes Yes Yes Yes Yes No  No  No  Yes Yes Yes No 
 [559] No  No  No  No  No  No  Yes No  No  No  Yes No  No  No  No  Yes No  No 
 [577] No  No  No  No  No  No  No  No  Yes No  Yes No  Yes Yes Yes No  No  No 
 [595] No  No  Yes No  No  Yes Yes Yes Yes Yes Yes No  No  Yes No  No  No  No 
 [613] No  No  No  No  Yes No  No  No  No  No  No  No  No  Yes No  No  No  No 
 [631] No  No  Yes No  No  No  No  No  No  No  No  No  No  No  No  No  Yes No 
 [649] No  No  No  Yes No  No  No  No  Yes No  No  No  No  No  Yes No  No  No 
 [667] No  No  No  No  Yes Yes Yes No  No  Yes No  No  No  Yes No  No  Yes Yes
 [685] No  No  No  No  No  No  No  No  No  Yes Yes Yes No  Yes Yes No  Yes No 
 [703] No  No  Yes No  No  No  No  Yes Yes No  Yes No  No  Yes No  Yes No  No 
 [721] No  No  No  No  No  Yes No  No  No  No  No  No  No  No  No  No  No  Yes
 [739] Yes No  No  Yes No  No  No  No  No  No  No  No  No  No  No  No  No  No 
 [757] No  No  No  No  Yes No  No  No  No  No  Yes No  No  Yes No  Yes No  Yes
 [775] No  No  Yes Yes No  No  Yes No  No  No  No  Yes Yes No  No  No  No  Yes
 [793] No  No  No  No  Yes Yes No  No  No  Yes No  Yes No  No  No  Yes No  No 
 [811] No  Yes No  No  Yes Yes No  No  No  No  No  Yes Yes Yes No  No  No  Yes
 [829] No  Yes Yes No  No  No  Yes No  No  No  No  No  No  No  No  No  Yes No 
 [847] No  No  No  Yes No  Yes Yes Yes No  No  No  No  No  Yes No  No  No  No 
 [865] Yes No  No  No  No  Yes Yes No  Yes No  No  Yes Yes Yes Yes No  Yes Yes
 [883] No  No  Yes Yes No  No  Yes No  No  Yes No  No  Yes Yes No  No  No  No 
 [901] Yes No  No  No  No  No  Yes No  No  No  No  No  No  No  No  No  No  No 
 [919] No  No  No  No  No  No  No  No  No  Yes No  No  No  No  No  Yes No  No 
 [937] No  No  No  No  No  No  No  No  No  No  No  No  No  No  No  No  No  No 
 [955] No  No  No  Yes No  Yes Yes Yes Yes No  No  Yes No  No  No  Yes No  No 
 [973] No  No  No  No  No  No  No  No  Yes No  No  No  Yes No  No  No  Yes Yes
 [991] No  No  Yes No  Yes No  Yes Yes No  No  No  Yes Yes Yes Yes No  Yes Yes
[1009] No  No  No  No  No  No  No  No  No  No  No  No  No  Yes No  No  No  No 
[1027] No  Yes No  No  No  Yes No  No  No  No  No  Yes No  No  No  No  No 
Levels: Yes No

$not_vary
character(0)

$df_name
[1] "titanic"

$vars
[1] "pclass" "sex"   

$rvar
[1] "survived"

$evar
[1] "pclass" "sex"   

$type
[1] "classification"

$lev
[1] "Yes"

$mtry
NULL

$num.trees
[1] 100

$min.node.size
[1] 1

$sample.fraction
[1] 1

$replace
[1] TRUE

$num.threads
[1] 12

$wts
NULL

$seed
[1] NA

$data_filter
[1] ""

$arr
[1] ""

$rows
NULL

attr(,"class")
[1] "rforest" "model"   "list"   
> rforest(diamonds, "price", c("carat", "clarity"), type = "regression") %>% summary()
Random Forest (Ranger)
Type                 : Regression
Data                 : diamonds
Response variable    : price
Explanatory variables: carat, clarity 
Mtry                 : 
Number of trees      : 100 
Min node size        : 1 
Sample fraction      : 1 
Number of threads    : 12 
Nr obs               : 3,000 
R-squared            : 0.917 
OOB prediction error : 1,141.355 
> 
> 
> 
> 
> cleanEx()
> nameEx("rig")
> ### * rig
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: rig
> ### Title: Relative Information Gain (RIG)
> ### Aliases: rig
> 
> ### ** Examples
> 
> rig(runif(20000), dvd$buy, "yes")
[1] -0.748665
> rig(ifelse(dvd$buy == "yes", 1, 0), dvd$buy, "yes")
[1] 1
> 
> 
> 
> cleanEx()
> nameEx("sensitivity.dtree")
> ### * sensitivity.dtree
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: sensitivity.dtree
> ### Title: Evaluate sensitivity of the decision tree
> ### Aliases: sensitivity.dtree
> 
> ### ** Examples
> 
> dtree(movie_contract, opt = "max") %>%
+   sensitivity(
+     vars = "legal fees 0 100000 10000",
+     decs = c("Sign with Movie Company", "Sign with TV Network"),
+     custom = FALSE
+   )
> 
> 
> 
> 
> cleanEx()
> nameEx("sim_cor")
> ### * sim_cor
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: sim_cor
> ### Title: Simulate correlated normally distributed data
> ### Aliases: sim_cor
> 
> ### ** Examples
> 
> sim <- sim_cor(100, .74, c(0, 10), c(1, 5), exact = TRUE)
> cor(sim)
     V1   V2
V1 1.00 0.74
V2 0.74 1.00
> sim_summary(sim)
Variables:
    n_obs    mean     sd     min     p25  median     p75     max
 V1   100  0.0000 1.0000 -2.2375 -0.6343 -0.0305  0.6994  2.1814
 V2   100 10.0000 5.0000  0.2864  6.7778  9.3976 12.7146 22.1361

> 
> 
> 
> 
> cleanEx()
> nameEx("sim_summary")
> ### * sim_summary
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: sim_summary
> ### Title: Print simulation summary
> ### Aliases: sim_summary
> 
> ### ** Examples
> 
> simulater(
+   const = "cost 3",
+   norm = "demand 2000 1000",
+   discrete = "price 5 8 .3 .7",
+   form = c("profit = demand * (price - cost)", "profit5K = profit > 5000"),
+   seed = 1234
+ ) %>% sim_summary()
Constants:
   cost
 3.0000

Variables:
        n_obs       mean         sd         min        p25     median
 demand 1,000 1,973.4028   997.3377 -1,396.0635 1,326.7463 1,960.2058
 price  1,000     7.0820     1.3832      5.0000     5.0000     8.0000
 profit 1,000 8,073.5651 5,160.5597 -4,321.7342 4,041.1407 7,242.6875
         p75         max
  2,615.8186  5,195.9012
      8.0000      8.0000
 11,596.0978 25,979.5060

Logicals:
         TRUE (nr)   TRUE (prop)
profit5K         656       0.656

> 
> 
> 
> 
> cleanEx()
> nameEx("simulater")
> ### * simulater
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: simulater
> ### Title: Simulate data for decision analysis
> ### Aliases: simulater
> 
> ### ** Examples
> 
> simulater(
+   const = "cost 3",
+   norm = "demand 2000 1000",
+   discrete = "price 5 8 .3 .7",
+   form = "profit = demand * (price - cost)",
+   seed = 1234
+ ) %>% str()
Classes ‘simulater’ and 'data.frame':	1000 obs. of  4 variables:
 $ cost  : num  3 3 3 3 3 3 3 3 3 3 ...
 $ demand: num  793 2277 3084 -346 2429 ...
 $ price : num  8 8 8 8 8 8 5 8 5 8 ...
 $ profit: num  3965 11387 15422 -1728 12146 ...
 - attr(*, "radiant_sim_call")=List of 18
  ..$ const   : chr "cost 3"
  ..$ lnorm   : chr ""
  ..$ norm    : chr "demand 2000 1000"
  ..$ unif    : chr ""
  ..$ discrete: chr "price 5 8 .3 .7"
  ..$ binom   : chr ""
  ..$ pois    : chr ""
  ..$ sequ    : chr ""
  ..$ grid    : chr ""
  ..$ data    : NULL
  ..$ form    : chr "profit = demand * (price - cost)"
  ..$ funcs   :length 0 expression()
  .. ..- attr(*, "srcref")= list()
  .. ..- attr(*, "srcfile")=Classes 'srcfilecopy', 'srcfile' <environment: 0x12eef61f0> 
  .. ..- attr(*, "wholeSrcref")= 'srcref' int [1:8] 1 0 2 0 0 0 1 2
  .. .. ..- attr(*, "srcfile")=Classes 'srcfilecopy', 'srcfile' <environment: 0x12eef61f0> 
  ..$ seed    : num 1234
  ..$ nexact  : logi FALSE
  ..$ name    : chr ""
  ..$ nr      : num 1000
  ..$ dataset : NULL
  ..$ envir   : language parent.frame()
 - attr(*, "description")= chr "\n### Simulated data\n\nFormulas:\n\nprofit = demand \\* (price - cost)\n\nDate: 2023-07-02 15:22:18"
> 
> 
> 
> 
> cleanEx()
> nameEx("store.mnl.predict")
> ### * store.mnl.predict
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: store.mnl.predict
> ### Title: Store predicted values generated in the mnl function
> ### Aliases: store.mnl.predict
> 
> ### ** Examples
> 
> result <- mnl(
+   ketchup,
+   rvar = "choice",
+   evar = c("price.heinz28", "price.heinz32", "price.heinz41", "price.hunts32"),
+   lev = "heinz28"
+ )
> pred <- predict(result, pred_data = ketchup)
> ketchup <- store(ketchup, pred, name = c("heinz28", "heinz32", "heinz41", "hunts32"))
> 
> 
> 
> 
> cleanEx()
> nameEx("store.model")
> ### * store.model
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: store.model
> ### Title: Store residuals from a model
> ### Aliases: store.model
> 
> ### ** Examples
> 
> regress(diamonds, rvar = "price", evar = c("carat", "cut"), data_filter = "price > 1000") %>%
+   store(diamonds, ., name = "resid") %>%
+   head()
# A tibble: 6 × 12
  price carat clarity cut   color depth table     x     y     z date       resid
  <int> <dbl> <fct>   <fct> <fct> <dbl> <dbl> <dbl> <dbl> <dbl> <date>     <dbl>
1   580  0.32 VS1     Ideal H      61    56    4.43  4.45  2.71 2012-02-26   NA 
2   650  0.34 SI1     Very… G      63.4  57    4.45  4.42  2.81 2012-02-26   NA 
3   630  0.3  VS2     Very… G      63.1  58    4.27  4.23  2.68 2012-02-26   NA 
4   706  0.35 VVS2    Ideal H      59.2  56    4.6   4.65  2.74 2012-02-26   NA 
5  1080  0.4  VS2     Prem… F      62.6  58    4.72  4.68  2.94 2012-02-26  877.
6  3082  0.6  VVS1    Ideal E      62.5  53.7  5.35  5.43  3.38 2012-02-26  595.
> 
> 
> 
> 
> cleanEx()
> nameEx("store.model.predict")
> ### * store.model.predict
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: store.model.predict
> ### Title: Store predicted values generated in model functions
> ### Aliases: store.model.predict
> 
> ### ** Examples
> 
> regress(diamonds, rvar = "price", evar = c("carat", "cut")) %>%
+   predict(pred_data = diamonds) %>%
+   store(diamonds, ., name = c("pred", "pred_low", "pred_high")) %>%
+   head()
# A tibble: 6 × 14
  price carat clarity cut       color depth table     x     y     z date      
  <int> <dbl> <fct>   <fct>     <fct> <dbl> <dbl> <dbl> <dbl> <dbl> <date>    
1   580  0.32 VS1     Ideal     H      61    56    4.43  4.45  2.71 2012-02-26
2   650  0.34 SI1     Very Good G      63.4  57    4.45  4.42  2.81 2012-02-26
3   630  0.3  VS2     Very Good G      63.1  58    4.27  4.23  2.68 2012-02-26
4   706  0.35 VVS2    Ideal     H      59.2  56    4.6   4.65  2.74 2012-02-26
5  1080  0.4  VS2     Premium   F      62.6  58    4.72  4.68  2.94 2012-02-26
6  3082  0.6  VVS1    Ideal     E      62.5  53.7  5.35  5.43  3.38 2012-02-26
# ℹ 3 more variables: pred <dbl>, pred_low <dbl>, pred_high <dbl>
> 
> 
> 
> 
> cleanEx()
> nameEx("store.nb.predict")
> ### * store.nb.predict
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: store.nb.predict
> ### Title: Store predicted values generated in the nb function
> ### Aliases: store.nb.predict
> 
> ### ** Examples
> 
> result <- nb(titanic, rvar = "survived", evar = c("pclass", "sex", "age"))
> pred <- predict(result, pred_data = titanic)
> titanic <- store(titanic, pred, name = c("Yes", "No"))
> 
> 
> 
> 
> cleanEx()
> nameEx("store.rforest.predict")
> ### * store.rforest.predict
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: store.rforest.predict
> ### Title: Store predicted values generated in the rforest function
> ### Aliases: store.rforest.predict
> 
> ### ** Examples
> 
> result <- rforest(
+   ketchup,
+   rvar = "choice",
+   evar = c("price.heinz28", "price.heinz32", "price.heinz41", "price.hunts32"),
+   lev = "heinz28"
+ )
> pred <- predict(result, pred_data = ketchup)
Using OOB predictions after comparing the training and prediction data
Using OOB predictions
> ketchup <- store(ketchup, pred, name = c("heinz28", "heinz32", "heinz41", "hunts32"))
> 
> 
> 
> 
> cleanEx()
> nameEx("summary.confusion")
> ### * summary.confusion
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: summary.confusion
> ### Title: Summary method for the confusion matrix
> ### Aliases: summary.confusion
> 
> ### ** Examples
> 
> data.frame(buy = dvd$buy, pred1 = runif(20000), pred2 = ifelse(dvd$buy == "yes", 1, 0)) %>%
+   confusion(c("pred1", "pred2"), "buy") %>%
+   summary()
Confusion matrix
Data       : . 
Results for: All 
Predictors : pred1, pred2 
Response   : buy 
Level      : yes in buy 
Cost:Margin: 1 : 2 

 Type Predictor    TP    FP     TN    FN  total   TPR   TNR precision Fscore
  All     pred1 2,590 7,369  7,385 2,656 20,000 0.494 0.501     0.260  0.341
  All     pred2 5,246     0 14,754     0 20,000 1.000 1.000     1.000  1.000

 Type Predictor accuracy  kappa     profit  index   ROME contact   AUC
  All     pred1    0.499 -0.004 -4,779.000 -0.911 -0.480   0.498 0.502
  All     pred2    1.000  1.000  5,246.000  1.000  1.000   0.262 1.000
> 
> 
> 
> cleanEx()
> nameEx("summary.crs")
> ### * summary.crs
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: summary.crs
> ### Title: Summary method for Collaborative Filter
> ### Aliases: summary.crs
> 
> ### ** Examples
> 
> crs(ratings,
+   id = "Users", prod = "Movies", pred = c("M6", "M7", "M8", "M9", "M10"),
+   rate = "Ratings", data_filter = "training == 1"
+ ) %>% summary()
Collaborative filtering
Data       : ratings
Filter     : training == 1
User id    : Users
Product id : Movies
Predict for: M6, M7, M8, M9, M10 

Recommendations:

 Users product rating average   cf ranking avg_rank cf_rank
   U11      M6           3.30 4.10                3       1
   U11      M7           2.70 2.08                5       4
   U11      M8           3.50 1.70                2       5
   U11      M9           2.90 2.13                4       3
   U11     M10           4.10 2.71                1       2
> 
> 
> 
> cleanEx()
> nameEx("summary.crtree")
> ### * summary.crtree
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: summary.crtree
> ### Title: Summary method for the crtree function
> ### Aliases: summary.crtree
> 
> ### ** Examples
> 
> result <- crtree(titanic, "survived", c("pclass", "sex"), lev = "Yes")
> summary(result)
Classification tree
Data                 : titanic
Response variable    : survived
Level                : Yes in survived
Explanatory variables: pclass, sex 
Complexity parameter : 0.001 
Minimum observations : 2 
Nr obs               : 1,043 

node), split, n, loss, yval, (yprob)
      * denotes terminal node

1) root 1043 425 No (0.40747843 0.59252157)  
  2) sex=female 386  96 Yes (0.75129534 0.24870466)  
    4) pclass=1st,2nd 234  16 Yes (0.93162393 0.06837607) *
    5) pclass=3rd 152  72 No (0.47368421 0.52631579) *
  3) sex=male 657 135 No (0.20547945 0.79452055) *> result <- crtree(diamonds, "price", c("carat", "color"), type = "regression")
> summary(result)
Regression tree
Data                 : diamonds
Response variable    : price
Explanatory variables: carat, color 
Complexity parameter : 0.001 
Minimum observations : 2 
Nr obs               : 3,000 

node), split, n, deviance, yval
      * denotes terminal node

 1) root 3000 46955880000  3907.1860  
   2) carat< 0.985 1935  2372900000  1618.3500  
     4) carat< 0.645 1392   380268800  1050.3640  
       8) carat< 0.445 976    53669310   779.0041 *
       9) carat>=0.445 416    86114590  1687.0170 *
     5) carat>=0.645 543   392351300  3074.4030  
      10) carat< 0.835 385   141250000  2743.9140 *
      11) carat>=0.835 158   106584800  3879.7090 *
   3) carat>=0.985 1065 16028000000  8065.7750  
     6) carat< 1.405 715  3304451000  6073.9550  
      12) color=H,I,J 255   319335200  5102.8240  
        24) carat< 1.195 168   103927100  4650.5890 *
        25) carat>=1.195 87   114701700  5976.1030 *
      13) color=D,E,F,G 460  2611311000  6612.3000  
        26) carat< 1.125 345  1332882000  6036.3540  
          52) carat< 1.035 230   591161500  5622.1260 *
          53) carat>=1.035 115   623327300  6864.8090 *
        27) carat>=1.125 115   820663700  8340.1390  
          54) carat< 1.255 85   496929600  7741.9880 *
          55) carat>=1.255 30   207156000 10034.9000 *
     7) carat>=1.405 350  4092028000 12134.7800  
      14) carat< 1.92 222  1737666000 10614.4000  
        28) color=H,I,J 116   500025900  9294.2070  
          56) carat< 1.535 68   227988500  8493.4850 *
          57) carat>=1.535 48   166674200 10428.5600 *
        29) color=D,E,F,G 106   814213500 12059.1300 *
      15) carat>=1.92 128   951174900 14771.6900  
        30) carat< 2.125 80   658765300 14061.3100  
          60) color=G,I,J 45   316470200 13070.9800 *
          61) color=D,E,F,H 35   241416700 15334.6000 *
        31) carat>=2.125 48   184754600 15955.6500 *> 
> 
> 
> cleanEx()
> nameEx("summary.dtree")
> ### * summary.dtree
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: summary.dtree
> ### Title: Summary method for the dtree function
> ### Aliases: summary.dtree
> 
> ### ** Examples
> 
> dtree(movie_contract, opt = "max") %>% summary(input = TRUE)
Decision tree input:
name: Sign contract
variables:
    legal fees: 5000
type: decision
Sign with Movie Company:
    cost: legal fees
    type: chance
    Small Box Office:
        p: 0.3
        payoff: 200000
    Medium Box Office:
        p: 0.6
        payoff: 1000000
    Large Box Office:
        p: 0.1
        payoff: 3000000
Sign with TV Network:
    payoff: 900000

> dtree(movie_contract, opt = "max") %>% summary(input = FALSE, output = TRUE)
Variable input values:
               
legal fees 5000

Initial decision tree:
                             Probability       Payoff     Cost     Type
 Sign contract                                                         
  ¦--Sign with Movie Company                          5,000.00 decision
  ¦   ¦--Small Box Office        30.00 %   200,000.00            chance
  ¦   ¦--Medium Box Office       60.00 % 1,000,000.00            chance
  ¦   °--Large Box Office        10.00 % 3,000,000.00            chance
  °--Sign with TV Network                  900,000.00          decision

Final decision tree:
                             Probability       Payoff     Cost     Type
 Sign contract                             955,000.00                  
  ¦--Sign with Movie Company               955,000.00 5,000.00 decision
  ¦   ¦--Small Box Office        30.00 %   200,000.00            chance
  ¦   ¦--Medium Box Office       60.00 % 1,000,000.00            chance
  ¦   °--Large Box Office        10.00 % 3,000,000.00            chance
  °--Sign with TV Network                  900,000.00          decision
> 
> 
> 
> 
> cleanEx()
> nameEx("summary.evalbin")
> ### * summary.evalbin
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: summary.evalbin
> ### Title: Summary method for the evalbin function
> ### Aliases: summary.evalbin
> 
> ### ** Examples
> 
> data.frame(buy = dvd$buy, pred1 = runif(20000), pred2 = ifelse(dvd$buy == "yes", 1, 0)) %>%
+   evalbin(c("pred1", "pred2"), "buy") %>%
+   summary()
Evaluate predictions for binary response models
Data        : . 
Results for : All 
Predictors  : pred1, pred2 
Response    : buy 
Level       : yes in buy 
Bins        : 10 
Cost:Margin : 1 : 2 

  pred bins nr_obs nr_resp resp_rate gains     profit   ROME cum_prop cum_resp
 pred1   10  2,000     513     0.257 0.098   -974.000 -0.487    0.100      513
 pred1    9  2,000     555     0.278 0.106 -1,864.000 -0.466    0.200    1,068
 pred1    8  2,000     530     0.265 0.101 -2,804.000 -0.467    0.300    1,598
 pred1    7  2,000     502     0.251 0.096 -3,800.000 -0.475    0.400    2,100
 pred1    6  2,000     548     0.274 0.104 -4,704.000 -0.470    0.500    2,648
 pred1    5  2,000     515     0.258 0.098 -5,674.000 -0.473    0.600    3,163
 pred1    4  2,000     485     0.242 0.092 -6,704.000 -0.479    0.700    3,648
 pred1    3  2,000     550     0.275 0.105 -7,604.000 -0.475    0.800    4,198
 pred1    2  2,000     554     0.277 0.106 -8,496.000 -0.472    0.900    4,752
 pred1    1  2,000     494     0.247 0.094 -9,508.000 -0.475    1.000    5,246
 pred2    3  5,246   5,246     1.000 1.000  5,246.000  1.000    0.262    5,246
 pred2   10 14,754       0     0.000 0.000 -9,508.000 -0.475    1.000    5,246
 cum_resp_rate cum_lift cum_gains
         0.257    0.978     0.098
         0.267    1.018     0.204
         0.266    1.015     0.305
         0.263    1.001     0.400
         0.265    1.010     0.505
         0.264    1.005     0.603
         0.261    0.993     0.695
         0.262    1.000     0.800
         0.264    1.006     0.906
         0.262    1.000     1.000
         1.000    3.812     1.000
         0.262    1.000     1.000
> 
> 
> 
> cleanEx()
> nameEx("summary.evalreg")
> ### * summary.evalreg
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: summary.evalreg
> ### Title: Summary method for the evalreg function
> ### Aliases: summary.evalreg
> 
> ### ** Examples
> 
> data.frame(price = diamonds$price, pred1 = rnorm(3000), pred2 = diamonds$price) %>%
+   evalreg(pred = c("pred1", "pred2"), "price") %>%
+   summary()
Evaluate predictions for regression models
Data        : . 
Results for : All 
Predictors  : pred1, pred2 
Response    : price 

 Type Predictor     n   Rsq      RMSE       MAE
  All     pred1 3,000 0.000 5,560.411 3,907.190
  All     pred2 3,000 1.000     0.000     0.000
> 
> 
> 
> 
> cleanEx()
> nameEx("summary.gbt")
> ### * summary.gbt
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: summary.gbt
> ### Title: Summary method for the gbt function
> ### Aliases: summary.gbt
> 
> ### ** Examples
> 
> result <- gbt(titanic, "survived", c("pclass", "sex"), early_stopping_rounds = 0) %>% str()
List of 28
 $ check                : chr ""
 $ model                :List of 15
  ..$ handle         :Class 'xgb.Booster.handle' <externalptr> 
  ..$ raw            : raw [1:6230] 7b 4c 00 00 ...
  ..$ best_iteration : num 1
  ..$ best_ntreelimit: int 1
  ..$ best_score     : num 0.825
  ..$ best_msg       : chr "[1]\ttrain-auc:0.824504"
  ..$ niter          : int 2
  ..$ evaluation_log :Classes ‘data.table’ and 'data.frame':	2 obs. of  2 variables:
  .. ..$ iter     : num [1:2] 1 2
  .. ..$ train_auc: num [1:2] 0.825 0.825
  .. ..- attr(*, ".internal.selfref")=<externalptr> 
  ..$ call           : language xgb.train(params = params, data = dtrain, nrounds = nrounds, watchlist = watchlist,      verbose = verbose, print| __truncated__ ...
  ..$ params         :List of 9
  .. ..$ max_depth          : num 6
  .. ..$ learning_rate      : num 0.3
  .. ..$ min_split_loss     : num 0
  .. ..$ min_child_weight   : num 1
  .. ..$ subsample          : num 1
  .. ..$ nthread            : num 12
  .. ..$ objective          : chr "binary:logistic"
  .. ..$ eval_metric        : chr "auc"
  .. ..$ validate_parameters: logi TRUE
  ..$ callbacks      :List of 3
  .. ..$ cb.print.evaluation:function (env = parent.frame())  
  .. .. ..- attr(*, "call")= language cb.print.evaluation(period = print_every_n)
  .. .. ..- attr(*, "name")= chr "cb.print.evaluation"
  .. ..$ cb.evaluation.log  :function (env = parent.frame(), finalize = FALSE)  
  .. .. ..- attr(*, "call")= language cb.evaluation.log()
  .. .. ..- attr(*, "name")= chr "cb.evaluation.log"
  .. ..$ cb.early.stop      :function (env = parent.frame(), finalize = FALSE)  
  .. .. ..- attr(*, "call")= language cb.early.stop(stopping_rounds = early_stopping_rounds, maximize = maximize,      verbose = verbose)
  .. .. ..- attr(*, "name")= chr "cb.early.stop"
  ..$ feature_names  : chr [1:3] "pclass2nd" "pclass3rd" "sexmale"
  ..$ nfeatures      : int 3
  ..$ importance     :Classes ‘data.table’ and 'data.frame':	3 obs. of  4 variables:
  .. ..$ Feature  : chr [1:3] "sexmale" "pclass3rd" "pclass2nd"
  .. ..$ Gain     : num [1:3] 0.758 0.21 0.032
  .. ..$ Cover    : num [1:3] 0.435 0.435 0.13
  .. ..$ Frequency: num [1:3] 0.25 0.5 0.25
  .. ..- attr(*, ".internal.selfref")=<externalptr> 
  ..$ model          : tibble [1,043 × 3] (S3: tbl_df/tbl/data.frame)
  .. ..$ survived: Factor w/ 2 levels "Yes","No": 1 1 2 2 2 1 1 2 1 2 ...
  .. ..$ pclass  : Factor w/ 3 levels "1st","2nd","3rd": 1 1 1 1 1 1 1 1 1 1 ...
  .. ..$ sex     : Factor w/ 2 levels "female","male": 1 2 1 2 1 2 1 2 1 2 ...
  .. ..- attr(*, "description")= chr "## Titanic\n\nThis dataset describes the survival status of individual passengers on the Titanic. The titanic d"| __truncated__
  ..- attr(*, "class")= chr "xgb.Booster"
 $ output               : chr [1:7] "[1]\ttrain-auc:0.824504 " "Will train until train_auc hasn't improved in 0 rounds." "" "[2]\ttrain-auc:0.824504 " ...
 $ check_args           :function (arg, default, inp = gbt_input)  
 $ extra_args_names     : NULL
 $ extra_args           : list()
 $ gbt_input            :List of 10
  ..$ max_depth            : num 6
  ..$ learning_rate        : num 0.3
  ..$ min_split_loss       : num 0
  ..$ nrounds              : num 100
  ..$ min_child_weight     : num 1
  ..$ subsample            : num 1
  ..$ early_stopping_rounds: num 0
  ..$ nthread              : num 12
  ..$ objective            : chr "binary:logistic"
  ..$ eval_metric          : chr "auc"
 $ not_vary             : chr(0) 
 $ nr_obs               : int 1043
 $ df_name              : chr "titanic"
 $ vars                 : chr [1:2] "pclass" "sex"
 $ rvar                 : chr "survived"
 $ evar                 : chr [1:2] "pclass" "sex"
 $ type                 : chr "classification"
 $ lev                  : chr "Yes"
 $ max_depth            : num 6
 $ learning_rate        : num 0.3
 $ min_split_loss       : num 0
 $ min_child_weight     : num 1
 $ subsample            : num 1
 $ nrounds              : num 100
 $ early_stopping_rounds: num 0
 $ nthread              : num 12
 $ wts                  : NULL
 $ seed                 : chr NA
 $ data_filter          : chr ""
 $ arr                  : chr ""
 $ rows                 : NULL
 - attr(*, "class")= chr [1:3] "gbt" "model" "list"
> summary(result)
Length  Class   Mode 
     0   NULL   NULL 
> 
> 
> 
> cleanEx()
> nameEx("summary.logistic")
> ### * summary.logistic
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: summary.logistic
> ### Title: Summary method for the logistic function
> ### Aliases: summary.logistic
> 
> ### ** Examples
> 
> 
> result <- logistic(titanic, "survived", "pclass", lev = "Yes")
> result <- logistic(titanic, "survived", "pclass", lev = "Yes")
> summary(result, test_var = "pclass")
Logistic regression (GLM)
Data                 : titanic
Response variable    : survived
Level                : Yes in survived
Explanatory variables: pclass 
Null hyp.: there is no effect of pclass on survived
Alt. hyp.: there is an effect of pclass on survived

                OR    OR% coefficient std.error z.value p.value    
 (Intercept)                    0.553     0.124   4.469  < .001 ***
 pclass|2nd  0.453 -54.7%      -0.791     0.176  -4.506  < .001 ***
 pclass|3rd  0.204 -79.6%      -1.588     0.160  -9.919  < .001 ***

Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Pseudo R-squared:0.076, Adjusted Pseudo R-squared:0.073
AUC: 0.672, Log-likelihood: -651.734, AIC: 1309.468, BIC: 1324.317
Chi-squared: 106.517 df(2), p.value < .001 
Nr obs: 1,043 

Model 1: survived ~ 1
Model 2: survived ~ pclass
Pseudo R-squared, Model 1 vs 2: 0 0.076
Chi-squared: 106.517 df(2), p.value < .001> res <- logistic(titanic, "survived", c("pclass", "sex"), int = "pclass:sex", lev = "Yes")
> summary(res, sum_check = c("vif", "confint", "odds"))
Logistic regression (GLM)
Data                 : titanic
Response variable    : survived
Level                : Yes in survived
Explanatory variables: pclass, sex 
Null hyp.: there is no effect of x on survived
Alt. hyp.: there is an effect of x on survived

                         OR    OR% coefficient std.error z.value p.value    
 (Intercept)                             3.227     0.456   7.079  < .001 ***
 pclass|2nd           0.332 -66.8%      -1.103     0.556  -1.982   0.047 *  
 pclass|3rd           0.036 -96.4%      -3.332     0.484  -6.886  < .001 ***
 sex|male             0.021 -97.9%      -3.842     0.487  -7.893  < .001 ***
 pclass|2nd:sex|male  0.949  -5.1%      -0.052     0.624  -0.084   0.933    
 pclass|3rd:sex|male 10.570 957.0%       2.358     0.533   4.427  < .001 ***

Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Pseudo R-squared:0.315, Adjusted Pseudo R-squared:0.308
AUC: 0.826, Log-likelihood: -483.189, AIC: 978.377, BIC: 1008.076
Chi-squared: 443.608 df(5), p.value < .001 
Nr obs: 1,043 

Variance Inflation Factors
there are higher-order terms (interactions) in this model
consider setting type = 'predictor'; see ?vif
    pclass:sex pclass   sex
VIF     29.262 18.925 8.241
Rsq      0.966  0.947 0.879

Waiting for profiling to be done...
                    coefficient   2.5%  97.5%   +/-
(Intercept)               3.227  2.439  4.265 1.038
pclass|2nd               -1.103 -2.286 -0.056 1.047
pclass|3rd               -3.332 -4.412 -2.476 0.856
sex|male                 -3.842 -4.926 -2.980 0.862
pclass|2nd:sex|male      -0.052 -1.243  1.243 1.295
pclass|3rd:sex|male       2.358  1.389  3.513 1.155

                    odds ratio  2.5%  97.5%
pclass|2nd               0.332 0.102  0.946
pclass|3rd               0.036 0.012  0.084
sex|male                 0.021 0.007  0.051
pclass|2nd:sex|male      0.949 0.289  3.465
pclass|3rd:sex|male     10.570 4.010 33.553

> titanic %>%
+   logistic("survived", c("pclass", "sex", "age"), lev = "Yes") %>%
+   summary("vif")
Logistic regression (GLM)
Data                 : .
Response variable    : survived
Level                : Yes in survived
Explanatory variables: pclass, sex, age 
Null hyp.: there is no effect of x on survived
Alt. hyp.: there is an effect of x on survived

                OR    OR% coefficient std.error z.value p.value    
 (Intercept)                    3.515     0.327  10.753  < .001 ***
 pclass|2nd  0.280 -72.0%      -1.274     0.226  -5.649  < .001 ***
 pclass|3rd  0.102 -89.8%      -2.283     0.226 -10.104  < .001 ***
 sex|male    0.083 -91.7%      -2.491     0.166 -14.997  < .001 ***
 age         0.966  -3.4%      -0.034     0.006  -5.438  < .001 ***

Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Pseudo R-squared:0.304, Adjusted Pseudo R-squared:0.298
AUC: 0.839, Log-likelihood: -490.864, AIC: 991.728, BIC: 1016.477
Chi-squared: 428.257 df(4), p.value < .001 
Nr obs: 1,043 

Variance Inflation Factors
    pclass   age   sex
VIF  1.413 1.352 1.053
Rsq  0.292 0.260 0.050

> 
> 
> 
> cleanEx()
> nameEx("summary.mnl")
> ### * summary.mnl
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: summary.mnl
> ### Title: Summary method for the mnl function
> ### Aliases: summary.mnl
> 
> ### ** Examples
> 
> result <- mnl(
+   ketchup,
+   rvar = "choice",
+   evar = c("price.heinz28", "price.heinz32", "price.heinz41", "price.hunts32"),
+   lev = "heinz28"
+ )
> summary(result)
Multinomial logistic regression (MNL)
Data                 : ketchup
Response variable    : choice
Base level           : heinz28 in choice
Explanatory variables: price.heinz28, price.heinz32, price.heinz41, price.hunts32 
Null hyp.: there is no effect of x on choice
Alt. hyp.: there is an effect of x on choice

                         RRR coefficient std.error z.value p.value    
 heinz32 (Intercept)               4.776     0.740   6.452  < .001 ***
 heinz32 price.heinz28 3.000       1.099     0.076  14.439  < .001 ***
 heinz32 price.heinz32 0.101      -2.296     0.135 -17.033  < .001 ***
 heinz32 price.heinz41 0.686      -0.377     0.118  -3.182   0.001 ** 
 heinz32 price.hunts32 1.055       0.053     0.103   0.516   0.606    
 heinz41 (Intercept)               4.757     1.092   4.358  < .001 ***
 heinz41 price.heinz28 2.106       0.745     0.136   5.483  < .001 ***
 heinz41 price.heinz32 0.699      -0.357     0.232  -1.542   0.123    
 heinz41 price.heinz41 0.212      -1.549     0.175  -8.857  < .001 ***
 heinz41 price.hunts32 0.702      -0.353     0.209  -1.688   0.091 .  
 hunts32 (Intercept)               0.526     1.309   0.402   0.688    
 hunts32 price.heinz28 3.602       1.282     0.126  10.200  < .001 ***
 hunts32 price.heinz32 1.710       0.536     0.246   2.185   0.029 *  
 hunts32 price.heinz41 0.911      -0.093     0.188  -0.495   0.620    
 hunts32 price.hunts32 0.070      -2.655     0.208 -12.789  < .001 ***

Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Pseudo R-squared: 0.2
Log-likelihood: -2511.212, AIC: 5052.423, BIC: 5141.473
Chi-squared: 1255.653 df(14), p.value < .001 
Nr obs: 2,798 

> 
> 
> 
> 
> cleanEx()
> nameEx("summary.nb")
> ### * summary.nb
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: summary.nb
> ### Title: Summary method for the nb function
> ### Aliases: summary.nb
> 
> ### ** Examples
> 
> result <- nb(titanic, "survived", c("pclass", "sex", "age"))
> summary(result)
Naive Bayes Classifier
Data                 : titanic
Response variable    : survived
Levels               : Yes, No in survived
Explanatory variables: pclass, sex, age
Laplace              : 0
Nr obs               : 1,043 

A-priori probabilities:
survived
  Yes    No 
0.407 0.593 

Conditional probabilities (categorical) or means & st.dev (numeric):
        pclass
survived   1st   2nd   3rd
     Yes 0.421 0.271 0.308
     No  0.167 0.236 0.597

        sex
survived female  male
     Yes  0.682 0.318
     No   0.155 0.845

        age
survived   mean st.dev
     Yes 28.819 15.004
     No  30.497 13.881

> 
> 
> 
> 
> cleanEx()
> nameEx("summary.nn")
> ### * summary.nn
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: summary.nn
> ### Title: Summary method for the nn function
> ### Aliases: summary.nn
> 
> ### ** Examples
> 
> result <- nn(titanic, "survived", "pclass", lev = "Yes")
> summary(result)
Neural Network
Activation function  : Logistic (classification)
Data                 : titanic
Response variable    : survived
Level                : Yes in survived
Explanatory variables: pclass 
Network size         : 1 
Parameter decay      : 0.5 
Network              : 2-1-1 with 5 weights
Nr obs               : 1,043 
Weights              :
   b->h1 i1->h1 i2->h1 
   -0.96   1.02   2.32 
   b->o h1->o 
   1.19 -2.73  
> 
> 
> 
> cleanEx()
> nameEx("summary.regress")
> ### * summary.regress
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: summary.regress
> ### Title: Summary method for the regress function
> ### Aliases: summary.regress
> 
> ### ** Examples
> 
> result <- regress(diamonds, "price", c("carat", "clarity"))
> summary(result, sum_check = c("rmse", "sumsquares", "vif", "confint"), test_var = "clarity")
Linear regression (OLS)
Data     : diamonds 
Response variable    : price 
Explanatory variables: carat, clarity 
Null hyp.: the effect of x on price is zero
Alt. hyp.: the effect of x on price is not zero

              coefficient std.error t.value p.value    
 (Intercept)    -6780.993   204.952 -33.086  < .001 ***
 carat           8438.030    51.101 165.125  < .001 ***
 clarity|SI2     2790.760   201.395  13.857  < .001 ***
 clarity|SI1     3608.531   200.508  17.997  < .001 ***
 clarity|VS2     4249.906   201.607  21.080  < .001 ***
 clarity|VS1     4461.956   204.592  21.809  < .001 ***
 clarity|VVS2    5109.476   210.207  24.307  < .001 ***
 clarity|VVS1    5027.669   214.251  23.466  < .001 ***
 clarity|IF      5265.170   233.658  22.534  < .001 ***

Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

R-squared: 0.904,  Adjusted R-squared: 0.904 
F-statistic: 3530.024 df(8,2991), p.value < .001
Nr obs: 3,000 

Prediction error (RMSE):  1224.329 
Residual st.dev   (RSD):  1226.17 

Sum of squares:
              df                 SS
Regression     8 42,458,933,806.968
Error      2,991  4,496,947,463.244
Total      2,999 46,955,881,270.212

Variance Inflation Factors
    carat clarity
VIF 1.169   1.169
Rsq 0.145   0.145

             coefficient      2.5%     97.5%     +/-
(Intercept)    -6780.993 -7182.855 -6379.131 401.862
carat           8438.030  8337.834  8538.227 100.196
clarity|SI2     2790.760  2395.873  3185.646 394.886
clarity|SI1     3608.531  3215.384  4001.679 393.148
clarity|VS2     4249.906  3854.604  4645.208 395.302
clarity|VS1     4461.956  4060.801  4863.111 401.155
clarity|VVS2    5109.476  4697.311  5521.640 412.165
clarity|VVS1    5027.669  4607.574  5447.764 420.095
clarity|IF      5265.170  4807.024  5723.317 458.147

Model 1: price ~ carat
Model 2: price ~ carat + clarity
R-squared, Model 1 vs 2: 0.86 0.904
F-statistic: 197.585 df(7,2991), p.value < .001> result <- regress(ideal, "y", c("x1", "x2"))
> summary(result, test_var = "x2")
Linear regression (OLS)
Data     : ideal 
Response variable    : y 
Explanatory variables: x1, x2 
Null hyp.: the effect of x on y is zero
Alt. hyp.: the effect of x on y is not zero

             coefficient std.error t.value p.value    
 (Intercept)       3.108     0.221  14.079  < .001 ***
 x1                7.778     0.230  33.795  < .001 ***
 x2               -1.621     0.035 -46.870  < .001 ***

Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

R-squared: 0.771,  Adjusted R-squared: 0.771 
F-statistic: 1681.071 df(2,997), p.value < .001
Nr obs: 1,000 

Model 1: y ~ x1
Model 2: y ~ x1 + x2
R-squared, Model 1 vs 2: 0.267 0.771
F-statistic: 2196.762 df(1,997), p.value < .001> ideal %>%
+   regress("y", "x1:x3") %>%
+   summary()
Linear regression (OLS)
Data     : . 
Response variable    : y 
Explanatory variables: x1, x2, x3 
Null hyp.: the effect of x on y is zero
Alt. hyp.: the effect of x on y is not zero

             coefficient std.error t.value p.value    
 (Intercept)       0.669     0.279   2.398   0.017 *  
 x1                7.713     0.213  36.157  < .001 ***
 x2               -1.987     0.043 -46.369  < .001 ***
 x3                0.902     0.070  12.861  < .001 ***

Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

R-squared: 0.804,  Adjusted R-squared: 0.803 
F-statistic: 1360.646 df(3,996), p.value < .001
Nr obs: 1,000 

> 
> 
> 
> 
> cleanEx()
> nameEx("summary.rforest")
> ### * summary.rforest
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: summary.rforest
> ### Title: Summary method for the rforest function
> ### Aliases: summary.rforest
> 
> ### ** Examples
> 
> result <- rforest(titanic, "survived", "pclass", lev = "Yes")
> summary(result)
Random Forest (Ranger)
Type                 : Classification
Data                 : titanic
Response variable    : survived
Level                : Yes in survived
Explanatory variables: pclass 
Mtry                 : 
Number of trees      : 100 
Min node size        : 1 
Sample fraction      : 1 
Number of threads    : 12 
Nr obs               : 1,043 
OOB prediction error : 0.218 
> 
> 
> 
> 
> cleanEx()
> nameEx("summary.simulater")
> ### * summary.simulater
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: summary.simulater
> ### Title: Summary method for the simulater function
> ### Aliases: summary.simulater
> 
> ### ** Examples
> 
> simdat <- simulater(norm = "demand 2000 1000", seed = 1234)
> summary(simdat)
Simulation
Simulations: 1,000 
Random seed: 1234 
Sim data   : simdat 
Normal     : demand 2000 1000

Variables:
        n_obs       mean       sd         min        p25     median        p75
 demand 1,000 1,973.4028 997.3377 -1,396.0635 1,326.7463 1,960.2058 2,615.8186
        max
 5,195.9012

> 
> 
> 
> 
> cleanEx()
> nameEx("test_specs")
> ### * test_specs
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: test_specs
> ### Title: Add interaction terms to list of test variables if needed
> ### Aliases: test_specs
> 
> ### ** Examples
> 
> test_specs("a", "a:b")
Interaction terms contain variables specified for testing.
Relevant interaction terms are included in the requested test.

[1] "a:b"
> test_specs("a", c("a:b", "b:c"))
Interaction terms contain variables specified for testing.
Relevant interaction terms are included in the requested test.

[1] "a:b"
> test_specs("a", c("a:b", "b:c", "I(c^2)"))
Interaction terms contain variables specified for testing.
Relevant interaction terms are included in the requested test.

[1] "a:b"
> test_specs(c("a", "b", "c"), c("a:b", "b:c", "I(c^2)"))
Interaction terms contain variables specified for testing.
Relevant interaction terms are included in the requested test.

[1] "a:b" "b:c"
> 
> 
> 
> 
> cleanEx()
> nameEx("var_check")
> ### * var_check
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: var_check
> ### Title: Check if main effects for all interaction effects are included
> ###   in the model
> ### Aliases: var_check
> 
> ### ** Examples
> 
> var_check("a:d", c("a", "b", "c", "d"))
$vars
[1] "a" "b" "c" "d"

$ev
[1] "a" "b" "c" "d"

$intv
NULL

> var_check(c("a", "b"), c("a", "b"), "a:c")
Interaction terms contain variables not selected as main effects.
Removing interactions from the estimation
$vars
[1] "a" "b"

$ev
[1] "a" "b"

$intv
character(0)

> var_check(c("a", "b"), c("a", "b"), "a:c")
Interaction terms contain variables not selected as main effects.
Removing interactions from the estimation
$vars
[1] "a" "b"

$ev
[1] "a" "b"

$intv
character(0)

> var_check(c("a", "b"), c("a", "b"), c("a:c", "I(b^2)"))
Interaction terms contain variables not selected as main effects.
Removing interactions from the estimation
$vars
[1] "a" "b"

$ev
[1] "a" "b"

$intv
[1] "I(b^2)"

> 
> 
> 
> 
> cleanEx()
> nameEx("write.coeff")
> ### * write.coeff
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: write.coeff
> ### Title: Write coefficient table for linear and logistic regression
> ### Aliases: write.coeff
> 
> ### ** Examples
> 
> 
> regress(
+   diamonds,
+   rvar = "price", evar = c("carat", "clarity", "color", "x"),
+   int = c("carat:clarity", "clarity:color", "I(x^2)"), check = "standardize"
+ ) %>%
+   write.coeff(sort = TRUE) %>%
+   format_df(dec = 3)
Standardized coefficients shown

                  label coefficient std.error t.value p.value sig_star dummy
1           (Intercept)      -0.162     0.059  -2.738   0.006      **      0
2      carat:clarity|IF       1.089     0.061  17.729   0.000      ***     0
3    carat:clarity|VVS1       1.020     0.052  19.677   0.000      ***     0
4            clarity|IF       1.015     0.098  10.349   0.000      ***     1
5    carat:clarity|VVS2       1.001     0.049  20.587   0.000      ***     0
6    clarity|IF:color|J      -0.946     0.147  -6.425   0.000      ***     1
7     carat:clarity|VS1       0.743     0.046  16.014   0.000      ***     0
8     carat:clarity|VS2       0.705     0.046  15.430   0.000      ***     0
9    clarity|IF:color|H      -0.610     0.108  -5.654   0.000      ***     1
10   clarity|IF:color|I      -0.606     0.114  -5.298   0.000      ***     1
11    carat:clarity|SI1       0.603     0.045  13.277   0.000      ***     0
12   clarity|IF:color|F      -0.572     0.105  -5.456   0.000      ***     1
13   clarity|IF:color|E      -0.534     0.114  -4.679   0.000      ***     1
14    carat:clarity|SI2       0.481     0.045  10.682   0.000      ***     0
15   clarity|IF:color|G      -0.441     0.115  -3.816   0.000      ***     1
16         clarity|VVS2       0.349     0.062   5.639   0.000      ***     1
17               I(x^2)       0.345     0.020  17.227   0.000      ***     0
18         clarity|VVS1       0.339     0.066   5.159   0.000      ***     1
19                carat       0.286     0.069   4.151   0.000      ***     0
20 clarity|VVS2:color|J      -0.284     0.107  -2.646   0.008      **      1
21          clarity|VS1       0.252     0.061   4.114   0.000      ***     1
22  clarity|VS2:color|J      -0.232     0.099  -2.338   0.019      *       1
23              color|G      -0.231     0.079  -2.912   0.004      **      1
24  clarity|VS2:color|G       0.201     0.081   2.494   0.013      *       1
25 clarity|VVS1:color|J      -0.192     0.120  -1.604   0.109              1
26  clarity|VS1:color|J      -0.192     0.101  -1.908   0.057      .       1
27 clarity|VVS1:color|G       0.190     0.085   2.239   0.025      *       1
28          clarity|VS2       0.177     0.060   2.929   0.003      **      1
29  clarity|SI2:color|G       0.176     0.081   2.169   0.030      *       1
30  clarity|SI1:color|G       0.175     0.081   2.162   0.031      *       1
31  clarity|VS1:color|G       0.155     0.081   1.902   0.057      .       1
32              color|I      -0.144     0.075  -1.926   0.054      .       1
33 clarity|VVS2:color|G       0.144     0.082   1.754   0.080      .       1
34  clarity|SI1:color|J      -0.133     0.099  -1.352   0.176              1
35              color|J      -0.121     0.097  -1.255   0.210              1
36  clarity|SI2:color|J      -0.116     0.100  -1.161   0.246              1
37 clarity|VVS2:color|I      -0.108     0.081  -1.330   0.184              1
38          clarity|SI1       0.103     0.060   1.719   0.086      .       1
39              color|H      -0.101     0.068  -1.487   0.137              1
40  clarity|SI2:color|E       0.100     0.077   1.300   0.194              1
41 clarity|VVS1:color|E       0.096     0.082   1.170   0.242              1
42  clarity|VS1:color|I      -0.095     0.078  -1.214   0.225              1
43              color|E      -0.087     0.075  -1.161   0.246              1
44  clarity|VS2:color|E       0.079     0.077   1.037   0.300              1
45  clarity|SI1:color|E       0.073     0.076   0.961   0.337              1
46  clarity|VS1:color|E       0.066     0.078   0.846   0.397              1
47 clarity|VVS2:color|E       0.063     0.079   0.799   0.425              1
48  clarity|VS1:color|H      -0.059     0.071  -0.823   0.410              1
49 clarity|VVS1:color|F       0.047     0.072   0.647   0.518              1
50 clarity|VVS1:color|I      -0.046     0.084  -0.553   0.580              1
51  clarity|VS2:color|I      -0.038     0.077  -0.486   0.627              1
52  clarity|VS1:color|F      -0.033     0.068  -0.489   0.625              1
53              color|F      -0.032     0.065  -0.497   0.619              1
54  clarity|SI1:color|H       0.029     0.070   0.420   0.675              1
55 clarity|VVS1:color|H       0.026     0.076   0.348   0.728              1
56          clarity|SI2       0.025     0.061   0.406   0.685              1
57  clarity|VS2:color|F       0.024     0.067   0.362   0.718              1
58  clarity|VS2:color|H      -0.020     0.070  -0.281   0.779              1
59  clarity|SI1:color|I      -0.020     0.077  -0.257   0.797              1
60  clarity|SI2:color|F       0.018     0.067   0.270   0.787              1
61 clarity|VVS2:color|H      -0.016     0.073  -0.222   0.824              1
62  clarity|SI2:color|H       0.014     0.071   0.196   0.844              1
63 clarity|VVS2:color|F       0.012     0.069   0.175   0.861              1
64  clarity|SI2:color|I      -0.012     0.078  -0.151   0.880              1
65  clarity|SI1:color|F       0.010     0.067   0.157   0.875              1
66                    x       0.009     0.047   0.197   0.844              0
     mean     sd    min    max importance
1   1.000  0.000  1.000  1.000      0.000
2   0.015  0.097  0.000  1.530      1.089
3   0.039  0.158  0.000  1.580      1.020
4   0.033  0.179  0.000  1.000      1.015
5   0.056  0.207  0.000  1.710      1.001
6   0.001  0.026  0.000  1.000      0.946
7   0.107  0.303  0.000  2.380      0.743
8   0.166  0.375  0.000  2.500      0.705
9   0.007  0.081  0.000  1.000      0.610
10  0.004  0.060  0.000  1.000      0.606
11  0.204  0.425  0.000  2.400      0.603
12  0.010  0.101  0.000  1.000      0.572
13  0.004  0.063  0.000  1.000      0.534
14  0.190  0.467  0.000  2.660      0.481
15  0.007  0.083  0.000  1.000      0.441
16  0.095  0.293  0.000  1.000      0.349
17 34.002 13.444 13.913 88.736      0.345
18  0.075  0.263  0.000  1.000      0.339
19  0.794  0.474  0.200  3.000      0.286
20  0.002  0.048  0.000  1.000      0.284
21  0.147  0.354  0.000  1.000      0.252
22  0.014  0.118  0.000  1.000      0.232
23  0.199  0.399  0.000  1.000      0.231
24  0.040  0.195  0.000  1.000      0.201
25  0.001  0.032  0.000  1.000      0.192
26  0.008  0.091  0.000  1.000      0.192
27  0.023  0.151  0.000  1.000      0.190
28  0.220  0.415  0.000  1.000      0.177
29  0.032  0.177  0.000  1.000      0.176
30  0.032  0.175  0.000  1.000      0.175
31  0.037  0.190  0.000  1.000      0.155
32  0.095  0.293  0.000  1.000      0.144
33  0.026  0.159  0.000  1.000      0.144
34  0.017  0.128  0.000  1.000      0.133
35  0.055  0.227  0.000  1.000      0.121
36  0.011  0.104  0.000  1.000      0.116
37  0.007  0.081  0.000  1.000      0.108
38  0.240  0.427  0.000  1.000      0.103
39  0.151  0.358  0.000  1.000      0.101
40  0.035  0.185  0.000  1.000      0.100
41  0.012  0.110  0.000  1.000      0.096
42  0.017  0.128  0.000  1.000      0.095
43  0.185  0.388  0.000  1.000      0.087
44  0.046  0.210  0.000  1.000      0.079
45  0.049  0.216  0.000  1.000      0.073
46  0.017  0.131  0.000  1.000      0.066
47  0.019  0.135  0.000  1.000      0.063
48  0.023  0.151  0.000  1.000      0.059
49  0.015  0.122  0.000  1.000      0.047
50  0.007  0.081  0.000  1.000      0.046
51  0.019  0.135  0.000  1.000      0.038
52  0.028  0.164  0.000  1.000      0.033
53  0.188  0.391  0.000  1.000      0.032
54  0.039  0.194  0.000  1.000      0.029
55  0.011  0.103  0.000  1.000      0.026
56  0.176  0.381  0.000  1.000      0.025
57  0.043  0.202  0.000  1.000      0.024
58  0.029  0.169  0.000  1.000      0.020
59  0.025  0.157  0.000  1.000      0.020
60  0.031  0.173  0.000  1.000      0.018
61  0.011  0.104  0.000  1.000      0.016
62  0.029  0.167  0.000  1.000      0.014
63  0.016  0.127  0.000  1.000      0.012
64  0.015  0.123  0.000  1.000      0.012
65  0.042  0.200  0.000  1.000      0.010
66  5.722  1.124  3.730  9.420      0.009
> 
> logistic(titanic, "survived", c("pclass", "sex"), lev = "Yes") %>%
+   write.coeff(intercept = FALSE, sort = TRUE) %>%
+   format_df(dec = 2)
Non-standardized coefficients shown

       label   OR   OR% coefficient std.error z.value p.value sig_star dummy
1   sex|male 0.08 -0.92       -2.52      0.16  -15.45    0.00      ***     1
2 pclass|3rd 0.18 -0.82       -1.71      0.19   -8.95    0.00      ***     1
3 pclass|2nd 0.41 -0.59       -0.89      0.21   -4.29    0.00      ***     1
  mean   sd  min  max importance
1 0.63 0.48 0.00 1.00      12.46
2 0.48 0.50 0.00 1.00       5.54
3 0.25 0.43 0.00 1.00       2.44
> 
> 
> 
> ### * <FOOTER>
> ###
> cleanEx()
> options(digits = 7L)
> base::cat("Time elapsed: ", proc.time() - base::get("ptime", pos = 'CheckExEnv'),"\n")
Time elapsed:  5.881 0.274 6.024 0.001 0.002 
> grDevices::dev.off()
null device 
          1 
> ###
> ### Local variables: ***
> ### mode: outline-minor ***
> ### outline-regexp: "\\(> \\)?### [*]+" ***
> ### End: ***
> quit('no')
