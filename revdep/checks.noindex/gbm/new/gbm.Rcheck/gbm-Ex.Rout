
R version 4.2.3 (2023-03-15) -- "Shortstop Beagle"
Copyright (C) 2023 The R Foundation for Statistical Computing
Platform: aarch64-apple-darwin20 (64-bit)

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type 'license()' or 'licence()' for distribution details.

  Natural language support but running in an English locale

R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R or R packages in publications.

Type 'demo()' for some demos, 'help()' for on-line help, or
'help.start()' for an HTML browser interface to help.
Type 'q()' to quit R.

> pkgname <- "gbm"
> source(file.path(R.home("share"), "R", "examples-header.R"))
> options(warn = 1)
> library('gbm')
Loaded gbm 2.1.8.1
> 
> base::assign(".oldSearch", base::search(), pos = 'CheckExEnv')
> base::assign(".old_wd", base::getwd(), pos = 'CheckExEnv')
> cleanEx()
> nameEx("calibrate.plot")
> ### * calibrate.plot
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: calibrate.plot
> ### Title: Calibration plot
> ### Aliases: calibrate.plot
> ### Keywords: hplot
> 
> ### ** Examples
> 
> # Don't want R CMD check to think there is a dependency on rpart
> # so comment out the example
> #library(rpart)
> #data(kyphosis)
> #y <- as.numeric(kyphosis$Kyphosis)-1
> #x <- kyphosis$Age
> #glm1 <- glm(y~poly(x,2),family=binomial)
> #p <- predict(glm1,type="response")
> #calibrate.plot(y, p, xlim=c(0,0.6), ylim=c(0,0.6))
> 
> 
> 
> cleanEx()
> nameEx("gbm")
> ### * gbm
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: gbm
> ### Title: Generalized Boosted Regression Modeling (GBM)
> ### Aliases: gbm
> 
> ### ** Examples
> 
> #
> # A least squares regression example 
> #
> 
> # Simulate data
> set.seed(101)  # for reproducibility
> N <- 1000
> X1 <- runif(N)
> X2 <- 2 * runif(N)
> X3 <- ordered(sample(letters[1:4], N, replace = TRUE), levels = letters[4:1])
> X4 <- factor(sample(letters[1:6], N, replace = TRUE))
> X5 <- factor(sample(letters[1:3], N, replace = TRUE))
> X6 <- 3 * runif(N) 
> mu <- c(-1, 0, 1, 2)[as.numeric(X3)]
> SNR <- 10  # signal-to-noise ratio
> Y <- X1 ^ 1.5 + 2 * (X2 ^ 0.5) + mu
> sigma <- sqrt(var(Y) / SNR)
> Y <- Y + rnorm(N, 0, sigma)
> X1[sample(1:N,size=500)] <- NA  # introduce some missing values
> X4[sample(1:N,size=300)] <- NA  # introduce some missing values
> data <- data.frame(Y, X1, X2, X3, X4, X5, X6)
> 
> # Fit a GBM
> set.seed(102)  # for reproducibility
> gbm1 <- gbm(Y ~ ., data = data, var.monotone = c(0, 0, 0, 0, 0, 0),
+             distribution = "gaussian", n.trees = 100, shrinkage = 0.1,             
+             interaction.depth = 3, bag.fraction = 0.5, train.fraction = 0.5,  
+             n.minobsinnode = 10, cv.folds = 5, keep.data = TRUE, 
+             verbose = FALSE, n.cores = 1)  
CV: 1 
CV: 2 
CV: 3 
CV: 4 
CV: 5 
> 
> # Check performance using the out-of-bag (OOB) error; the OOB error typically
> # underestimates the optimal number of iterations
> best.iter <- gbm.perf(gbm1, method = "OOB")
OOB generally underestimates the optimal number of iterations although predictive performance is reasonably competitive. Using cv_folds>1 when calling gbm usually results in improved predictive performance.
> print(best.iter)
[1] 43
attr(,"smoother")
Call:
loess(formula = object$oobag.improve ~ x, enp.target = min(max(4, 
    length(x)/10), 50))

Number of Observations: 100 
Equivalent Number of Parameters: 8.32 
Residual Standard Error: 0.005885 
> 
> # Check performance using the 50% heldout test set
> best.iter <- gbm.perf(gbm1, method = "test")
> print(best.iter)
[1] 63
> 
> # Check performance using 5-fold cross-validation
> best.iter <- gbm.perf(gbm1, method = "cv")
> print(best.iter)
[1] 70
> 
> # Plot relative influence of each variable
> par(mfrow = c(1, 2))
> summary(gbm1, n.trees = 1)          # using first tree
   var  rel.inf
X3  X3 86.74183
X2  X2 13.25817
X1  X1  0.00000
X4  X4  0.00000
X5  X5  0.00000
X6  X6  0.00000
> summary(gbm1, n.trees = best.iter)  # using estimated best number of trees
   var    rel.inf
X3  X3 68.6902290
X2  X2 25.3361275
X1  X1  3.3863779
X4  X4  1.2681014
X6  X6  1.0914539
X5  X5  0.2277102
> 
> # Compactly print the first and last trees for curiosity
> print(pretty.gbm.tree(gbm1, i.tree = 1))
  SplitVar SplitCodePred LeftNode RightNode MissingNode ErrorReduction Weight
0        2   1.500000000        1         5           9      258.70575    250
1        1   0.643849456        2         3           4       45.93610    122
2       -1  -0.204032200       -1        -1          -1        0.00000     34
3       -1  -0.067172308       -1        -1          -1        0.00000     88
4       -1  -0.105313590       -1        -1          -1        0.00000    122
5        2   2.500000000        6         7           8       41.83219    128
6       -1   0.044497873       -1        -1          -1        0.00000     68
7       -1   0.159057141       -1        -1          -1        0.00000     60
8       -1   0.098197530       -1        -1          -1        0.00000    128
9       -1  -0.001115896       -1        -1          -1        0.00000    250
    Prediction
0 -0.001115896
1 -0.105313590
2 -0.204032200
3 -0.067172308
4 -0.105313590
5  0.098197530
6  0.044497873
7  0.159057141
8  0.098197530
9 -0.001115896
> print(pretty.gbm.tree(gbm1, i.tree = gbm1$n.trees))
  SplitVar SplitCodePred LeftNode RightNode MissingNode ErrorReduction Weight
0        0   0.582418997        1         2           3      0.9193604    250
1       -1   0.006219724       -1        -1          -1      0.0000000     63
2       -1  -0.010214328       -1        -1          -1      0.0000000     57
3        3  51.000000000        4         8           9      0.7852138    130
4        1   1.016134987        5         6           7      0.5866318     62
5       -1   0.003627100       -1        -1          -1      0.0000000     33
6       -1  -0.015867874       -1        -1          -1      0.0000000     29
7       -1  -0.005491517       -1        -1          -1      0.0000000     62
8       -1   0.010784760       -1        -1          -1      0.0000000     28
9       -1   0.009523251       -1        -1          -1      0.0000000     40
     Prediction
0  0.0006082206
1  0.0062197235
2 -0.0102143276
3  0.0026340711
4 -0.0054915172
5  0.0036270997
6 -0.0158678743
7 -0.0054915172
8  0.0107847602
9  0.0095232507
> 
> # Simulate new data
> set.seed(103)  # for reproducibility
> N <- 1000
> X1 <- runif(N)
> X2 <- 2 * runif(N)
> X3 <- ordered(sample(letters[1:4], N, replace = TRUE))
> X4 <- factor(sample(letters[1:6], N, replace = TRUE))
> X5 <- factor(sample(letters[1:3], N, replace = TRUE))
> X6 <- 3 * runif(N) 
> mu <- c(-1, 0, 1, 2)[as.numeric(X3)]
> Y <- X1 ^ 1.5 + 2 * (X2 ^ 0.5) + mu + rnorm(N, 0, sigma)
> data2 <- data.frame(Y, X1, X2, X3, X4, X5, X6)
> 
> # Predict on the new data using the "best" number of trees; by default,
> # predictions will be on the link scale
> Yhat <- predict(gbm1, newdata = data2, n.trees = best.iter, type = "link")
> 
> # least squares error
> print(sum((data2$Y - Yhat)^2))
[1] 5153.285
> 
> # Construct univariate partial dependence plots
> plot(gbm1, i.var = 1, n.trees = best.iter)
> plot(gbm1, i.var = 2, n.trees = best.iter)
> plot(gbm1, i.var = "X3", n.trees = best.iter)  # can use index or name
> 
> # Construct bivariate partial dependence plots
> plot(gbm1, i.var = 1:2, n.trees = best.iter)
> plot(gbm1, i.var = c("X2", "X3"), n.trees = best.iter)
> plot(gbm1, i.var = 3:4, n.trees = best.iter)
> 
> # Construct trivariate partial dependence plots
> plot(gbm1, i.var = c(1, 2, 6), n.trees = best.iter, 
+      continuous.resolution = 20)
> plot(gbm1, i.var = 1:3, n.trees = best.iter)
> plot(gbm1, i.var = 2:4, n.trees = best.iter)
> plot(gbm1, i.var = 3:5, n.trees = best.iter)
> 
> # Add more (i.e., 100) boosting iterations to the ensemble
> gbm2 <- gbm.more(gbm1, n.new.trees = 100, verbose = FALSE)
> 
> 
> 
> graphics::par(get("par.postscript", pos = 'CheckExEnv'))
> cleanEx()
> nameEx("gbm.more")
> ### * gbm.more
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: gbm.more
> ### Title: Generalized Boosted Regression Modeling (GBM)
> ### Aliases: gbm.more
> 
> ### ** Examples
> 
> #
> # A least squares regression example 
> #
> 
> # Simulate data
> set.seed(101)  # for reproducibility
> N <- 1000
> X1 <- runif(N)
> X2 <- 2 * runif(N)
> X3 <- ordered(sample(letters[1:4], N, replace = TRUE), levels = letters[4:1])
> X4 <- factor(sample(letters[1:6], N, replace = TRUE))
> X5 <- factor(sample(letters[1:3], N, replace = TRUE))
> X6 <- 3 * runif(N) 
> mu <- c(-1, 0, 1, 2)[as.numeric(X3)]
> SNR <- 10  # signal-to-noise ratio
> Y <- X1 ^ 1.5 + 2 * (X2 ^ 0.5) + mu
> sigma <- sqrt(var(Y) / SNR)
> Y <- Y + rnorm(N, 0, sigma)
> X1[sample(1:N,size=500)] <- NA  # introduce some missing values
> X4[sample(1:N,size=300)] <- NA  # introduce some missing values
> data <- data.frame(Y, X1, X2, X3, X4, X5, X6)
> 
> # Fit a GBM
> set.seed(102)  # for reproducibility
> gbm1 <- gbm(Y ~ ., data = data, var.monotone = c(0, 0, 0, 0, 0, 0),
+             distribution = "gaussian", n.trees = 100, shrinkage = 0.1,             
+             interaction.depth = 3, bag.fraction = 0.5, train.fraction = 0.5,  
+             n.minobsinnode = 10, cv.folds = 5, keep.data = TRUE, 
+             verbose = FALSE, n.cores = 1)  
CV: 1 
CV: 2 
CV: 3 
CV: 4 
CV: 5 
> 
> # Check performance using the out-of-bag (OOB) error; the OOB error typically
> # underestimates the optimal number of iterations
> best.iter <- gbm.perf(gbm1, method = "OOB")
OOB generally underestimates the optimal number of iterations although predictive performance is reasonably competitive. Using cv_folds>1 when calling gbm usually results in improved predictive performance.
> print(best.iter)
[1] 43
attr(,"smoother")
Call:
loess(formula = object$oobag.improve ~ x, enp.target = min(max(4, 
    length(x)/10), 50))

Number of Observations: 100 
Equivalent Number of Parameters: 8.32 
Residual Standard Error: 0.005885 
> 
> # Check performance using the 50% heldout test set
> best.iter <- gbm.perf(gbm1, method = "test")
> print(best.iter)
[1] 63
> 
> # Check performance using 5-fold cross-validation
> best.iter <- gbm.perf(gbm1, method = "cv")
> print(best.iter)
[1] 70
> 
> # Plot relative influence of each variable
> par(mfrow = c(1, 2))
> summary(gbm1, n.trees = 1)          # using first tree
   var  rel.inf
X3  X3 86.74183
X2  X2 13.25817
X1  X1  0.00000
X4  X4  0.00000
X5  X5  0.00000
X6  X6  0.00000
> summary(gbm1, n.trees = best.iter)  # using estimated best number of trees
   var    rel.inf
X3  X3 68.6902290
X2  X2 25.3361275
X1  X1  3.3863779
X4  X4  1.2681014
X6  X6  1.0914539
X5  X5  0.2277102
> 
> # Compactly print the first and last trees for curiosity
> print(pretty.gbm.tree(gbm1, i.tree = 1))
  SplitVar SplitCodePred LeftNode RightNode MissingNode ErrorReduction Weight
0        2   1.500000000        1         5           9      258.70575    250
1        1   0.643849456        2         3           4       45.93610    122
2       -1  -0.204032200       -1        -1          -1        0.00000     34
3       -1  -0.067172308       -1        -1          -1        0.00000     88
4       -1  -0.105313590       -1        -1          -1        0.00000    122
5        2   2.500000000        6         7           8       41.83219    128
6       -1   0.044497873       -1        -1          -1        0.00000     68
7       -1   0.159057141       -1        -1          -1        0.00000     60
8       -1   0.098197530       -1        -1          -1        0.00000    128
9       -1  -0.001115896       -1        -1          -1        0.00000    250
    Prediction
0 -0.001115896
1 -0.105313590
2 -0.204032200
3 -0.067172308
4 -0.105313590
5  0.098197530
6  0.044497873
7  0.159057141
8  0.098197530
9 -0.001115896
> print(pretty.gbm.tree(gbm1, i.tree = gbm1$n.trees))
  SplitVar SplitCodePred LeftNode RightNode MissingNode ErrorReduction Weight
0        0   0.582418997        1         2           3      0.9193604    250
1       -1   0.006219724       -1        -1          -1      0.0000000     63
2       -1  -0.010214328       -1        -1          -1      0.0000000     57
3        3  51.000000000        4         8           9      0.7852138    130
4        1   1.016134987        5         6           7      0.5866318     62
5       -1   0.003627100       -1        -1          -1      0.0000000     33
6       -1  -0.015867874       -1        -1          -1      0.0000000     29
7       -1  -0.005491517       -1        -1          -1      0.0000000     62
8       -1   0.010784760       -1        -1          -1      0.0000000     28
9       -1   0.009523251       -1        -1          -1      0.0000000     40
     Prediction
0  0.0006082206
1  0.0062197235
2 -0.0102143276
3  0.0026340711
4 -0.0054915172
5  0.0036270997
6 -0.0158678743
7 -0.0054915172
8  0.0107847602
9  0.0095232507
> 
> # Simulate new data
> set.seed(103)  # for reproducibility
> N <- 1000
> X1 <- runif(N)
> X2 <- 2 * runif(N)
> X3 <- ordered(sample(letters[1:4], N, replace = TRUE))
> X4 <- factor(sample(letters[1:6], N, replace = TRUE))
> X5 <- factor(sample(letters[1:3], N, replace = TRUE))
> X6 <- 3 * runif(N) 
> mu <- c(-1, 0, 1, 2)[as.numeric(X3)]
> Y <- X1 ^ 1.5 + 2 * (X2 ^ 0.5) + mu + rnorm(N, 0, sigma)
> data2 <- data.frame(Y, X1, X2, X3, X4, X5, X6)
> 
> # Predict on the new data using the "best" number of trees; by default,
> # predictions will be on the link scale
> Yhat <- predict(gbm1, newdata = data2, n.trees = best.iter, type = "link")
> 
> # least squares error
> print(sum((data2$Y - Yhat)^2))
[1] 5153.285
> 
> # Construct univariate partial dependence plots
> plot(gbm1, i.var = 1, n.trees = best.iter)
> plot(gbm1, i.var = 2, n.trees = best.iter)
> plot(gbm1, i.var = "X3", n.trees = best.iter)  # can use index or name
> 
> # Construct bivariate partial dependence plots
> plot(gbm1, i.var = 1:2, n.trees = best.iter)
> plot(gbm1, i.var = c("X2", "X3"), n.trees = best.iter)
> plot(gbm1, i.var = 3:4, n.trees = best.iter)
> 
> # Construct trivariate partial dependence plots
> plot(gbm1, i.var = c(1, 2, 6), n.trees = best.iter, 
+      continuous.resolution = 20)
> plot(gbm1, i.var = 1:3, n.trees = best.iter)
> plot(gbm1, i.var = 2:4, n.trees = best.iter)
> plot(gbm1, i.var = 3:5, n.trees = best.iter)
> 
> # Add more (i.e., 100) boosting iterations to the ensemble
> gbm2 <- gbm.more(gbm1, n.new.trees = 100, verbose = FALSE)
> 
> 
> 
> graphics::par(get("par.postscript", pos = 'CheckExEnv'))
> cleanEx()
> nameEx("print.gbm")
> ### * print.gbm
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: print.gbm
> ### Title: Print model summary
> ### Aliases: print.gbm show.gbm
> ### Keywords: models nonlinear nonparametric survival
> 
> ### ** Examples
> 
> 
> data(iris)
> iris.mod <- gbm(Species ~ ., distribution="multinomial", data=iris,
+                  n.trees=2000, shrinkage=0.01, cv.folds=5,
+                  verbose=FALSE, n.cores=1)
Warning: Setting `distribution = "multinomial"` is ill-advised as it is currently broken. It exists only for backwards compatibility. Use at your own risk.
CV: 1 
CV: 2 
CV: 3 
CV: 4 
CV: 5 
> iris.mod
gbm(formula = Species ~ ., distribution = "multinomial", data = iris, 
    n.trees = 2000, shrinkage = 0.01, cv.folds = 5, verbose = FALSE, 
    n.cores = 1)
A gradient boosted model with multinomial loss function.
2000 iterations were performed.
The best cross-validation iteration was 352.
There were 4 predictors of which 4 had non-zero influence.
> #data(lung)
> #lung.mod <- gbm(Surv(time, status) ~ ., distribution="coxph", data=lung,
> #                 n.trees=2000, shrinkage=0.01, cv.folds=5,verbose =FALSE)
> #lung.mod
> 
> 
> 
> cleanEx()
> nameEx("quantile.rug")
> ### * quantile.rug
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: quantile.rug
> ### Title: Quantile rug plot
> ### Aliases: quantile.rug
> ### Keywords: aplot
> 
> ### ** Examples
> 
> x <- rnorm(100)
> y <- rnorm(100)
> plot(x, y)
> quantile.rug(x)
> 
> 
> 
> cleanEx()
> nameEx("test.gbm")
> ### * test.gbm
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: test.gbm
> ### Title: Test the 'gbm' package.
> ### Aliases: test.gbm validate.gbm test.relative.influence
> ### Keywords: models
> 
> ### ** Examples
> 
> 
> # Uncomment the following lines to run - commented out to make CRAN happy
> #library(RUnit)
> #val <- validate.texmex()
> #printHTMLProtocol(val, "texmexReport.html")
> 
> 
> 
> ### * <FOOTER>
> ###
> cleanEx()
> options(digits = 7L)
> base::cat("Time elapsed: ", proc.time() - base::get("ptime", pos = 'CheckExEnv'),"\n")
Time elapsed:  1.923 0.078 2.006 0 0 
> grDevices::dev.off()
null device 
          1 
> ###
> ### Local variables: ***
> ### mode: outline-minor ***
> ### outline-regexp: "\\(> \\)?### [*]+" ***
> ### End: ***
> quit('no')
